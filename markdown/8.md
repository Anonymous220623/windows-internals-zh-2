# 第8章　系统机制

Windows操作系统提供了多种可供执行体、内核以及设备驱动程序等内核模式组件使用的基本机制。本章将介绍下列系统机制及其用法：

● 处理器执行模型，包括Ring级别、段、任务状态、陷阱调度（包括中断、延迟过程调用（DPC）、异步过程调用（APC）、计时器、系统工作线程、异常调度以及系统服务调度）。

● 预测执行的屏障和其他软件侧信道缓解措施。

● 执行体对象管理器。

● 同步，包括自旋锁、内核调度程序对象、等待调度，以及与用户模式相关的同步基元（synchronization primitive），如基于地址的等待、条件变量以及精简读取器/写入器（Slim Reader-Writer，SRW）锁。

● 高级本地过程调用（Advanced Local Procedure Call，ALPC）子系统。

● Windows通知设施（Windows Notification Facility，WNF）。

● WoW64。

● 用户模式调试框架。

此外，本章还将详细介绍通用Windows平台（Universal Windows Platform，UWP）以及驱动该平台的用户模式和内核模式服务，例如：

● 打包的应用程序（Packaged Application）和AppX部署服务。

● Centennial应用程序和Windows桌面桥（Windows Desktop Bridge）。

● 进程状态管理（Process State Management，PSM）和进程生命周期管理（Process Lifetime Management，PLM）。

● 主机活动管理器（Host Activity Manager，HAM）和后台活动审查器（Background Activity Moderator，BAM）。

## 8.1　处理器执行模型

本节将深入介绍Intel i386处理器架构，以及现代系统中更常用的AMD64架构（i386架构的扩展）的内部机制。虽然这两种架构最初由不同公司设计，但现在，这两家供应商已经实现了对方的设计。因此，尽管我们可能依然会在Windows文件和注册表键中看到这些后缀，但目前普遍用x86（32位）和x64（64位）指代这两种架构。

本节将讨论段（segmentation）、任务、Ring级别等与关键机制相关的概念，以及陷阱（trap）、中断（interrupt）和系统调用（system call）等概念。

### 8.1.1　段

诸如C/C++和Rust等高级编程语言会被编译为机器代码，通常可称之为汇编语言或汇编代码。借助这种低级语言可直接访问处理器寄存器。通常程序可访问以下三种主要类型的寄存器（调试代码时可见）：

● 程序计数器（Program Counter，PC），在x86/x64架构中可将其称为指令指针（Instruction Pointer，IP），由EIP（x86）和RIP（x64）寄存器所代表。该寄存器始终指向正在执行的汇编代码行（某些32位ARM架构存在例外情况）。

● 栈指针（Stack Pointer，SP），由ESP（x86）和RSP（x64）寄存器所代表。该寄存器会指向保存了当前栈位置的内存位置。

● 其他通用寄存器（General Purpose Register，GPR），包括但不限于EAX/RAX、ECX/RCX、EDX/RDX、ESI/RSI及R8、R14等寄存器。

虽然这些寄存器可包含指向内存的地址值，但在访问内存位置时还需要其他寄存器的介入，这是一种称为受保护模式段（protected mode segmentation）的机制。为此需要检查各种段寄存器，此类寄存器亦可称为选择器（selector）：

● 所有针对程序计数器的访问首先需要检查代码段（Code Segment，CS）寄存器。

● 所有针对栈指针的访问首先需要检查栈段（Stack Segment，SS）寄存器。

● 对其他寄存器的访问由段重写（Override）决定，段重写所用的编码方式可强制针对特定寄存器进行检查，如数据段（Data Segment，DS）、扩展段（Extended Segment，ES）或F段（F Segment，FS）。

这些选择器位于16位段寄存器中，可在一种名为全局描述符表（Global Descriptor Table，GDT）的数据结构中进行查找。为了定位GDT，处理器还会用到另一个CPU寄存器：GDT寄存器，也就是GDTR。这些选择器的格式如图8-1所示。

![](../assets/tx418.png)

图8-1　x86段选择器的格式

段选择器中的偏移量可以在GDT中查看，除非TI位设置为使用另一种名为本地描述符表（Local Descriptor Table，LDT）的数据结构，该数据结构由LDTR所确定，但现代Windows操作系统中已不再使用该数据结构了。因为这种工作方式会造成这样一种结果：在被发现的段项（或者无效段项）中产生一般性保护错误（#GP）或段错误（#SF）异常。

这个段项在现代操作系统中通常被称为段描述符，主要提供两种关键用途：

● 对于代码段，它给出运行这个段选择器所加载的代码即将执行的Ring级别，也叫代码特权级别（Code Privilege Level，CPL）。Ring级别的范围介于0到3之间，会被缓存至实际选择器的最低两位，如图8-1所示。Windows操作系统会使用Ring 0来运行内核模式组件和驱动程序，并使用Ring 3运行应用程序和服务。此外在x64系统中，代码段还可体现出这是一个长模式还是兼容模式的段。前者允许x64代码以原生方式执行，后者可激活与x86的遗留兼容模式。x86系统中也存在类似机制，据此可将段标记为16位段或32位段。

● 对于其他段，它给出访问这些段所需的Ring级别，也叫描述符特权级别（Descriptor Privilege Level，DPL）。虽然在当今现代操作系统中已经算是一项过时的检查，但处理器（以及应用程序）依然会强制要求正确设置该段。

最后，在x86系统中，段项也可以使用32位基址，该值会被添加到已载入（使用重写引用该段的）寄存器的其他任意值中。随后会使用相应的段限制来检查底层寄存器的值是否超过某个固定上限。因为在大部分操作系统中，该基址会被设置为0（且限制为0xFFFFFFFF），所以x64架构代码摒弃了这个概念，但FS和GS选择器除外，它们的工作方式略有差异，如下：

● 如果代码段为长模式，那么会从FS_BASE这个特殊模块寄存器（Model Specific Register，MSR）中的0C0000100h处获得FS段的基址。对于GS段，则查看当前的Swap状态，该状态可通过swapgs指令修改，随后则会载入GS_BASE MSR（0C0000101h）或GS_SWAP MSR（0C0000102h）。  
如果FS或GS段选择器寄存器中设置了TI位，则会从LDT项相应的偏移量处获得对应的值，该值只能采用32位基址。这样做是为了保证与某些忽略32位基址限制操作系统的兼容性。

● 如果代码段为兼容模式，那么会照常从相应的GDT项（如果TI位已设置，则会从LDT项）读取基址。该限制会强制实施，并且会通过段重写后寄存器中的偏移量进行验证。

FS和GS段这种有趣的行为可被Windows等操作系统用于实现某种类型的线程本地寄存器效果，借此，段基址可指向某种特定的数据结构，进而以简单的方式访问其中的特定偏移量/字段。

例如，Windows会将线程环境块（Thread Environment Block，TEB）的地址存储在x86系统的FS段或x64系统的GS（已交换）段中（TEB已在卷1第3章中进行了详细介绍）。随后，当在x86系统中执行内核模式代码时，该FS段会被手动修改为一个不同的段项，该段项包含内核处理器控制区（Kernel Processor Control Region，KPCR）的地址，而在x64系统中则是由GS（未交换）段存储该地址。

因此，段可在Windows上实现这两种效果：在处理器级别下编码并强制实施可供代码片段执行的特权级别，并分别为用户模式和内核模式代码提供对TEB和KPCR数据结构的直接访问。请注意，由于GDT是由CPU寄存器（GDTR）指向的，因此每个CPU都可以有自己的GDT。实际上，Windows正是借此保证了每个GDT都加载相应的每个处理器KPCR，并且在当前处理器上，当前执行线程的TEB同样会保存在自己的段中。

实验：在x64系统中查看GDT

在进行远程调试或分析崩溃转储文件（都需要用到LiveKD）时，我们可以使用dg这个调试器命令查看GDT的内容，包括所有段的状态及其基址（如果相关）。该命令可接收起始段和终止段，也就是下文范例中的10和50：

```
0: kd> dg 10 50 
                                                    P Si Gr Pr Lo 
Sel         Base             Limit           Type   l ze an es ng flags 
---- ----------------- ----------------- ---------- - -- -- -- -- --------
0010 00000000`00000000 00000000`00000000 Code RE Ac 0 Nb By P Lo 0000029b 
0018 00000000`00000000 00000000`00000000 Data RW Ac 0 Bg By P Nl 00000493 
0020 00000000`00000000 00000000`ffffffff Code RE Ac 3 Bg Pg P Nl 00000cfb 
0028 00000000`00000000 00000000`ffffffff Data RW Ac 3 Bg Pg P Nl 00000cf3 
0030 00000000`00000000 00000000`00000000 Code RE Ac 3 Nb By P Lo 000002fb 
0050 00000000`00000000 00000000`00003c00 Data RW Ac 3 Bg By P Nl 000004f3 
```

此处的关键段为10h、18h、20h、28h、30h和50h（上述输出结果有省略，删除了与本话题无关的项）。

在10h（KGDT64_R0_CODE）中可以看到一个处于Ring 0的长模式代码段，该代码段在PI列下显示数字“0”，在Long列下显示字母“Lo”，其类型为Code RE。类似地，在20h（KGDT64_R3_CMCODE）中可以看到一个处于Ring 3的Nl段（Nl代表Not Long，也就是兼容模式），该段可用于在WoW64子系统中执行x86代码。而在30h（KGDT64_R3_CODE）中可以看到一个等价的长模式段。随后请注意18h（KGDT64_ R0_DATA）和28h（KGDT64_R3_DATA）段，它们对应栈、数据和扩展段。

还有最后一个段50h（KGDT_R3_CMTEB），除非我们在转储GDT时在WoW64下运行某些x86代码，否则该段的基址通常为零。根据上文的介绍，在兼容模式下运行时，该段通常会存储TEB的基址。

要查看64位TEB和KPCR段，我们需要转储相应的MSR。在进行本地或远程内核调试时，可通过下列命令进行转储（这些命令无法用于崩溃转储）：

```
lkd> rdmsr c0000101 
msr[c0000101] = ffffb401`a3b80000 
　
lkd> rdmsr c0000102 
msr[c0000102] = 000000e5`6dbe9000 
```

我们可以将这些值与@$pcr和@$teb的值进行对比，随后应该能看到相同的值，例如：

```
lkd> dx -r0 @$pcr 
@$pcr              : 0xffffb401a3b80000 [Type: _KPCR *] 
　
lkd> dx -r0 @$teb 
@$teb              : 0xe56dbe9000 [Type: _TEB *] 
```

实验：在x86系统中查看GDT

在x86系统中，虽然GDT包含类似的段，但分别位于不同的选择器中。此外，由于使用了双FS段来替代swapgs功能，并且缺乏长模式，因此选择器的数量也会有所差异，如下所示：

```
kd> dg 8 38 
                                  P Si Gr Pr Lo 
Sel    Base     Limit     Type    l ze an es ng flags 
---- -------- -------- ---------- - -- -- -- -- --------
0008 00000000 ffffffff Code RE Ac 0 Bg Pg P Nl 00000c9b 
0010 00000000 ffffffff Data RW Ac 0 Bg Pg P Nl 00000c93 
0018 00000000 ffffffff Code RE    3 Bg Pg P Nl 00000cfa 
0020 00000000 ffffffff Data RW Ac 3 Bg Pg P Nl 00000cf3 
0030 80a9e000 00006020 Data RW Ac 0 Bg By P Nl 00000493 
0038 00000000 00000fff Data RW    3 Bg By P Nl 000004f2 
```

此处的关键段为08h、10h、18h、20h、30h和38h。在08h（KGDT_R0_CODE）

中可以看到一个处于Ring 0的代码段。类似地，在18h（KGDT_R3_CODE）中会看到一个Ring 3的段。随后请注意10h（KGDT_R0_DATA）和20h（KGDT_R3_DATA）段，它们对应栈、数据和扩展段。

在x86系统中，可以在30h（KGDT_R0_PCR）段中看到KPCR的基址，并在38h（KGDT_R3_TEB）段中看到当前线程TEB的基址。此类系统的段不使用MSR。

#### 延迟段加载

根据上文有关段的描述和相关值的介绍，在x86或x64系统中调查DS和ES段的值可能会有“惊喜”：它们的值未必会与相应Ring级别所定义的值相匹配。例如，一个x86用户模式线程可能包含下列段：

```
CS = 1Bh (18h | 3) 
ES, DS = 23 (20h | 3) 
FS = 3Bh (38h | 3) 
```

然而，在Ring 0的系统调用中，可能会看到如下段：

```
CS = 08h (08h | 0) 
ES, DS = 23 (20h | 3) 
FS = 30h (30h | 0) 
```

类似地，内核模式执行的x64线程也可以将自己的ES和DS段设置为2Bh（28h | 3）。造成这种差异的原因在于一项名为延迟段加载（lazy segment loading）的功能。此外，这种差异体现在平面内存模型下运作的系统中，如果当前代码特权级别（CPL）为0，那么数据段的描述符特权级别（DPL）将毫无意义。由于更高位的CPL始终可以访问更低位DPL的数据（但无法反向访问），因此在进入内核时将DS和ES段设置为各自“适当”的值后，还需要在返回用户模式时将这些值还原。

虽然10h处的MOV DS指令看似无关紧要，但在遇到该指令后，处理器的微码需要执行一系列选择器正确性检查，这会为系统调用和中断处理增加大量处理成本。因此，为避免增加这些成本，Windows始终会使用Ring 3数据段值。

### 8.1.2　任务状态段

除了代码和数据段寄存器，x86和x64架构中还有另一种特殊寄存器：任务寄存器（Task Register，TR），这也是GDT中充当偏移量的另一个16位选择器。然而，此时的段项并不与代码或数据相关联，而是与任务相关联。这意味着，对于处理器的内部状态而言，当前执行的代码片段会调用任务状态（task state），在Windows中所调用的为当前线程。现代x86操作系统会使用这些由段代表的任务状态（即任务状态段，Task State Segment，TSS）构建各种可关联至关键处理器陷阱（下文将详细介绍）的任务。在最基本的情况下，TSS可代表一个页目录（借助CR3寄存器），如x64系统中的PML4（有关分页的详细信息请参阅卷1第5章），也可代表代码段、堆栈段、指令指针，甚至最多可代表四个栈指针（每个Ring级别一个指针）。此类TSS主要用于如下场景：

● 在未出现特定陷阱时，可代表当前执行状态。如果处理器当前正运行在Ring 3级别下，那么随后处理器可从该TSS加载Ring 0栈，以便正确地处理中断和异常。

● 解决处理调试错误（#DB）时的架构竞争条件，这需要有包含自定义调试错误处理程序和内核栈的专用TSS。

● 代表在出现双重错误（#DF）陷阱时需要加载的执行状态。借此可在安全（备份）内核栈而非当前线程的内核栈上切换至双重错误处理程序，而后者可能也是出现错误的原因。

● 代表在出现不可屏蔽的中断（#NMI）时需要加载的执行状态。类似地，该TSS可用于在安全内核栈上加载NMI处理程序。

● 对于会在计算机检查异常（#MCE）中使用的其他类似任务，出于相同原因，它们也可以在专用的安全内核栈中运行。

在x86系统中，可以在GDT的028h选择器中找到主要的（当前）TSS，这也解释了在Windows的正常执行过程中TR会位于028h的原因。此外，#DF TSS位于58h，NMI TSS位于50h，#MCE TSS位于0A0h，#DB TSS位于0A8h。

在x64系统上，由于TSS功能已被降级为主要执行在专用内核栈上运行的陷阱处理程序，因此删除了系统具有多个TSS的功能。目前只使用一个TSS（在Windows中位于040h），它使用了一个由八个可能的栈指针组成的数组，该数组名为中断栈表（Interrupt Stack Table，IST）。先前遇到的每个陷阱都会关联至IST索引，而不再关联至自定义TSS。在下一节内容中，随着我们转储几个IDT项，你就会直观感受到x86和x64系统以及它们处理这些陷阱的方法上的差异。

实验：在x86系统中查看TSS

在x86系统中，我们可以使用上一个实验中用过的dg命令在28h处查看系统范围内的TSS：

```
kd> dg 28 28 
                                  P Si Gr Pr Lo 
Sel    Base     Limit     Type    l ze an es ng flags 
---- -------- -------- ---------- - -- -- -- -- --------
0028 8116e400 000020ab TSS32 Busy 0 Nb By P  Nl 0000008b 
```

上述命令将返回KTSS数据结构的虚拟地址，随后可使用dx或dt命令对其创建转储：

```
kd> dx (nt!_KTSS*)0x8116e400 
(nt!_KTSS*)0x8116e400              : 0x8116e400 [Type: _KTSS *] 
    [+0x000] Backlink        : 0x0 [Type: unsigned short] 
    [+0x002] Reserved0       : 0x0 [Type: unsigned short] 
    [+0x004] Esp0            : 0x81174000 [Type: unsigned long] 
    [+0x008] Ss0             : 0x10 [Type: unsigned short] 
```

请注意，上述指令只设置了Esp0和Ss0字段，因为Windows绝不会在上文介绍的陷阱之外的其他情况下使用基于硬件的任务切换。因此这个TSS的唯一用途是在硬件中断期间加载相应的内核栈。

正如在“陷阱调度”一节中所述，对于不会受到“Meltdown”处理器架构漏洞影响的系统，这个栈指针也是当前线程的内核栈指针（基于卷1第5章介绍过的KTHREAD

结构）；但对于受此漏洞影响的系统，这个栈指针会指向处理器描述符区域内部的过渡栈。同时，栈段将始终设置为10h，即KGDT_R0_DATA。

如上文所述，计算机检查异常（#MC）使用了另一个TSS。我们同样可以通过dg命令查看：

```
kd> dg a0 a0 
                                  P Si Gr Pr Lo 
Sel    Base     Limit     Type    l ze an es ng flags 
---- -------- -------- ---------- - -- -- -- -- --------
00A0 81170590 00000067 TSS32 Avl  0 Nb By P  Nl 00000089 
```

不过这一次我们会使用.tss命令而非dx命令，该命令可格式化KTSS结构中的不同字段，并以类似于在当前执行线程中那样的方式显示任务。本例中的输入参数为栈选择器（A0h）。

```
kd> .tss a0 
　
eax=00000000 ebx=00000000 ecx=00000000 edx=00000000 esi=00000000 edi=00000000 
eip=81e1a718 esp=820f5470 ebp=00000000 iopl=0         nv up di pl nz na po nc 
cs=0008 ss=0010 ds=0023 es=0023 fs=0030 gs=0000                  efl=00000000 
　
hal!HalpMcaExceptionHandlerWrapper: 
81e1a718 fa              cli 
```

请留意段寄存器的设置方式与上文“延迟段加载”中所提到的方式是一致的，并且程序计数器（EIP）指向了#MC的处理程序。此外，为了不受内存错误影响，该栈被配置为指向内核二进制库中的一个安全栈。最后，尽管并未显示在.tss的输出结果中，但CR3实际上被配置为系统页目录。在“陷阱调度”一节，我们还将使用!idt命令重新查看这个TSS。

实验：在x64系统中查看TSS和IST

很不幸，x64系统中的dg命令存在Bug，无法正确显示64位基址，因此，为了获取TSS段（40h）的基址，我们需要对两个段创建转储，并将高位、中位和低位基址的数据结合在一起：

```
0: kd> dg 40 48 
                                                    P Si Gr Pr Lo 
Sel         Base             Limit          Type    l ze an es ng flags 
---- ----------------- ----------------- ---------- - -- -- -- -- --------
0040 00000000`7074d000 00000000`00000067 TSS32 Busy 0 Nb By P  Nl 0000008b 
0048 00000000`0000ffff 00000000`0000f802 <Reserved> 0 Nb By Np Nl 00000000 
```

因此在本例中，KTSS64位于0xFFFFF8027074D000。作为获取该地址的另一种方式，请注意每个处理器的KPCR都包含一个名为TssBase的字段，其中也包含一个指向KTSS64的指针：

```
0: kd> dx @$pcr->TssBase 
@$pcr->TssBase              : 0xfffff8027074d000 [Type: _KTSS64 *] 
    [+0x000] Reserved0      : 0x0 [Type: unsigned long] 
    [+0x004] Rsp0           : 0xfffff80270757c90 [Type: unsigned __int64] 
```

请留意，此处看到的虚拟地址与GDT中看到的地址是相同的。此外我们还会发现，除RSP0之外，其他所有字段都是零，与x86架构类似，RSP0包含（在不受“Meltdown”硬件漏洞影响的计算机上）当前线程内核栈的地址，或包含处理器描述符区域过渡栈的地址。

执行该实验所用的系统配备了一个第10代Intel处理器，因此RSP0等于当前内核栈：

```
0: kd> dx @$thread->Tcb.InitialStack 
@$thread->Tcb.InitialStack : 0xfffff80270757c90 [Type: void *] 
```

最后，查看中断栈表会看到关联至#DF、#MC、#DB和NMI陷阱的各种栈，在“陷阱调度”一节我们还将进一步查看中断调度表（Interrupt Dispatch Table，IDT）是如何引用这些栈的：

```
0: kd> dx @$pcr->TssBase->Ist 
@$pcr->TssBase->Ist     [Type: unsigned __int64 [8]] 
    [0] :            0x0 [Type: unsigned __int64] 
    [1] :            0xfffff80270768000 [Type: unsigned __int64] 
    [2] :            0xfffff8027076c000 [Type: unsigned __int64] 
    [3] :            0xfffff8027076a000 [Type: unsigned __int64] 
    [4] :            0xfffff8027076e000 [Type: unsigned __int64] 
```

在讨论了GDT中Ring级别、代码执行以及某些关键段之间的关系后，我们将通过下文的“陷阱调度”一节一起看看不同代码段（及其Ring级别）之间实际的过渡过程。但在讨论陷阱调度前，我们先分析在易受熔断（Meltdown）硬件旁路攻击影响的系统中TSS配置是如何变化的。

## 8.2　硬件侧信道漏洞

现代CPU可以在内部寄存器之间以非常快的速度（皮秒级别）计算并移动数据。处理器的寄存器是一种稀缺资源，因此，操作系统和应用程序代码总是通过指令让CPU将数据从CPU寄存器移动至主存，反之亦然。CPU可以访问不同类型的内存。位于CPU封装内部以及可由CPU执行引擎直接访问的内存称为缓存（Cache），缓存具有高速和昂贵的特点。CPU通过外部总线访问的内存通常可称为RAM（随机访问内存，Random Access Memory），RAM速度更慢，价格更低，但容量更大。内存与CPU之间的位置关系定义了一种所谓的“基于内存层次结构”的内存，这些内存有着不同的速度和容量（位置越接近CPU的内存，速度就越快，但容量就越小）。如图8-2所示，现代计算机的CPU通常包含L1、L2和L3这三级高速缓存内存，每个物理内核均可直接访问这些高速缓存。L1和L2缓存距离CPU的内核最近，并且是每个内核专用的。L3缓存距离最远，并且始终被所有CPU内核共享（不过嵌入式处理器一般不具备L3缓存）。

访问时间是缓存的一个重要特征，其访问时间几乎等同于CPU的寄存器（其实缓存比寄存器略慢一些）。主存的访问时间则会慢数百倍。这意味着，如果CPU按顺序执行所有指令，由于需要通过指令访问位于主存中的数据，整体速度会慢很多倍。为了解决这个问题，现代CPU采取了不同的策略。在历史上，这些策略曾引发了侧信道攻击（也叫预测式攻击），事实证明，这会极大地影响终端用户系统的整体安全性。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx505.png)

图8-2　现代CPU的缓存和存储内存及其平均容量与访问时间

为了准确描述侧信道硬件攻击以及Windows所采取的缓解措施，我们首先需要通过一些基本概念了解CPU内部的工作原理。

### 8.2.1　乱序执行

现代微处理器通过自己的流水线执行计算机指令。流水线包含很多阶段，如指令获取、解码、寄存器分配和更名、指令重排序、执行，以及退出。CPU应对内存访问速度不够快的一种常用策略是：让执行引擎忽略指令顺序，优先执行所需资源已可用的指令。这意味着CPU并不会按照某种严格一致的顺序执行指令，借此能够通过让所有内核尽可能满载的方式将所有执行单元的利用率提升至最大限度。在确定某些指令很快将会被用到并被提交（退出）之前，现代处理器能够以预测性的方式执行数百条此类指令。

上述乱序执行方法最大的问题之一在于分支指令。一条带有附带条件的分支指令会在机器代码中定义两个可能的路径，而最终要执行的“正确”路径取决于之前执行过的指令。在计算具体情况时，因为所依赖的“之前执行过的指令”需要访问速度缓慢的RAM，因此整体速度也会被拖慢。此时，执行引擎需要等待定义条件的指令退出（意味着需要等待内存总线完成内存访问操作），随后才能以乱序执行的方式执行正确路径下所包含的后续指令。间接分支也会遇到类似情况。在间接分支中，CPU的执行引擎并不知道分支（通常为Jump或Call）的具体目标，因为必须从主存中获取相关地址。在这个语境中，“推测执行”（speculative execution）这个术语意味着CPU的流水线需要以并行或乱序的方式解码并执行多条指令，但其结果并不会退出至永久性寄存器中，在分支指令最终执行完毕之前，内存写入操作依然会处于挂起状态。

### 8.2.2　CPU分支预测器

在彻底评估分支条件前，CPU如何得知哪个分支（路径）需要执行？（由于目标地址未知，间接分支同样存在类似问题。）答案位于CPU封装所包含的两个组件中：分支预测器（branch predictor）和分支目标预测器（branch target predictor）。

分支预测器是CPU中一种复杂的数字电路，在最终得以确认前，它会尽可能猜测每个分支最终的行进路径。借助类似方式，CPU中所包含的分支目标预测器会在最终确定前，尽可能预测间接分支的目标。虽然实际的硬件实现主要取决于CPU制造商，但这两个组件都用到了一种名为分支目标缓冲（Branch Target Buffer，BTB）的内部缓存，BTB可以使用由索引函数生成的地址标签记录分支的目标地址（或有关条件分支过去曾经做过什么的相关信息），该地址标签与缓存生成标签的方法类似，下一节会详细介绍。当分支指令首次执行时，会将目标地址存储在BTB中。通常，当执行流水线首次停机时，会迫使CPU等待从主存中成功获取条件或目标地址。当同一个分支第二次执行时，会使用BTB中的目标地址来获取预测的目标并将其置于流水线中。图8-3展示了CPU分支目标预测器简化后的架构范例。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx514.png)

图8-3　CPU分支目标预测器简化后的架构范例

如果预测出错，并且已经以预测的方式执行了错误的路径，那么指令流水线会被刷新，之前预测执行的结果会被丢弃。随后会向CPU流水线中送入其他路径，并从正确的分支开始重新执行。这个过程也叫分支预测错误。在这种情况下，浪费掉的CPU周期总数并不会多于顺序执行并等待分支条件的结果或评估间接地址所使用的CPU周期数。然而，CPU依然会在预测执行的过程中产生各种副作用，例如CPU缓存行污染。不幸的是，一些副作用可能会被攻击者发现并利用，进而危及系统的整体安全性。

### 8.2.3　CPU缓存

正如上一节所述，CPU缓存（Cache）是一种高速内存，可大幅缩短获取和存储数据与指令所需的时间。数据会以固定大小的块（通常为64或128字节）在内存和缓存之间传输，这种数据块也叫缓存行或缓存块。当一个缓存行从内存复制到缓存时，会创建一个缓存项。该缓存项中包含数据副本以及用于分辨所请求内存位置的标签。与分支目标预测器不同，缓存始终会通过物理地址创建索引（否则多个地址空间之间的映射和变更过程将变得极为复杂）。从缓存的角度来看，一个物理地址可以拆分为不同的成分，其中较高的位通常代表标签，较低的位代表缓存行以及行本身的偏移量。标签具备唯一性，可用于区分每个缓存块所属的内存地址，如图8-4所示。

当CPU读/写内存位置时，首先会检查缓存中是否存在对应的项（会在可能包含来自该地址数据的任何缓存行中检查。但某些缓存可能存在不同的“向”，下文很快将会提到）。如果处理器发现来自该位置的内存数据已经位于缓存中，此时就出现了“缓存命中”的情况，处理器会立即通过该缓存行读/写数据；如果数据不在缓存中，此谓之“缓存未命中”，此时CPU会在缓存中分配一个新项，并将数据从主存中复制进去，随后进行访问。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx522.png)

图8-4　48位单向CPU缓存范例

图8-4展示了一个单向CPU缓存，该缓存最大可寻址48位虚拟地址空间。在本例中，CPU正在从虚拟地址0x19F566030中读取48字节数据。内存内容最开始已从主存读取到缓存块0x60，该块已经被完全装满，但所请求的数据位于偏移量0x30处。范例缓存只有256块，每块256字节，因此多个物理地址可以装入编号为0x60的块中。标签（0x19F56）能够唯一地区分数据在主存中所在的物理地址。

通过类似的方式，当CPU接到指令向一个内存地址写入新内容时，它首先会更新该内存地址所属的一个或多个缓存行。某些时候，CPU还会将数据写回至物理RAM，这主要取决于内存页面所应用的缓存类型（write-back、write-through、uncached等）。请注意，在多处理器系统中这具有重要的意义：必须设计某种缓存一致协议，以避免出现主CPU更新某个缓存块后，其他CPU针对陈旧数据执行操作的情况（多CPU缓存一致算法是存在的，但超出了本书的讨论范畴）。

当出现缓存未命中情况时，为了给新的缓存项腾出空间，CPU有时会清除某个现有的缓存块。选择要清除的缓存项（意味着选择用哪个缓存块来存储新数据）时所用的算法叫作放置策略（placement policy）。如果放置策略只能替换特定虚拟地址的一个块，这种情况可以叫作直接映射（如图8-4所示缓存只有一个方向，且属于直接映射）。相反，如果缓存可以自由选择（具备相同块编号的）任意项来保存新数据，这样的缓存也叫全相联（fully associative）缓存。很多缓存机制在实现方面进行了妥协，使得主存中的每个项可保存到缓存中*N*个位置中的任何一个位置内，这种机制也叫N向组相联（N-ways set associative）。因此一个“向”可以看作缓存的一个组成部分，缓存中每个向的容量相等，并按照相同的方式进行索引。图8-5展示了一个四向组相联缓存。图中所示的缓存可以存储分属于四个不同物理地址的数据，并通过不同的四个缓存组（使用不同标记）对相同的缓存块创建索引。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx719.png)

图8-5　一个四向组相联缓存

### 8.2.4　侧信道攻击

如上节内容所述，现代CPU的执行引擎只有在指令真正退出后才会写入计算结果。这意味着，就算有多条指令已经乱序执行完毕，并且对CPU寄存器和内存架构不会产生任何可见的影响，但这样做依然会对微架构（microarchitecture）产生一定的副作用，尤其是会影响到CPU缓存。2017年年底出现了一种针对CPU乱序引擎和分支预测器发起的新颖攻击，这种攻击所依赖的前提条件是，微架构所产生的副作用是可衡量的，尽管这些影响无法通过任何软件代码直接访问。

围绕这种方式产生的最具破坏性且最有效的硬件侧信道攻击分别名为Meltdown和Spectre。

#### Meltdown

Meltdown，又被称为恶意数据缓存负载（Rogue Data Cache Load，RDCL），可供恶意的用户模式进程读取所有内存，而该进程完全不需要具备相关授权。该攻击利用了处理器的乱序执行引擎，以及内存访问指令处理过程中内存访问和特权检查两个环节之间存在的内部争用条件。

在Meltdown攻击中，恶意的用户模式进程首先会刷新整个缓存（从用户模式调用可执行该操作的指令），随后该进程会执行一个非法的内核内存访问，并执行指令以可控的方式（使用一个probe数组）填满缓存。因为该进程无法访问内核内存，所以此时处理器会产生异常，该异常会被应用程序捕获，进而导致进程被终止。然而由于乱序执行的缘故，CPU已经执行了（但未退出，这意味着在任何CPU寄存器或RAM中均无法检测到对架构产生的影响）非法内存访问之后发出的指令，因此已经使用非法请求的内核内存内容填满了缓存。

随后恶意应用程序会衡量访问数组（该数组已被用于填充CPU的缓存块）中每个页面所需的时间，借此探测整个缓存。如果访问时间落后于某个阈值，则意味着数据位于缓存行中，攻击者进而就可以通过从内核内存读取的数据推断出准确的内容。图8-6取自最早有关Meltdown的研究论文（详见https://meltdownattack.com/ ），其中展示了1 MB probe数组（由256个4KB的页组成）的访问时间。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx727.png)

图8-6　访问一个1 MB probe数组所需的CPU时间

如图8-6所示，每个页面的访问时间都是类似的，只有一个页面的时间有较大差异。假设一次可读取1字节的机密数据，而1字节只能有256个值，那么只要准确得知数组中的哪个页面导致了缓存命中，攻击者就可以知道内核内存中到底存储了哪一字节的数据。

#### Spectre

Spectre攻击与Meltdown攻击类似，意味着它也依赖上文介绍的乱序执行漏洞，但Spectre所利用的CPU组件主要为分支预测器和分支目标预测器。起初，Spectre攻击曾出现过两种变体，这两种变体都可以总结为如下三个阶段：

1）在设置阶段，攻击者会通过低特权（且由攻击者控制的）进程反复执行多次操作，误导CPU分支预测器，此举意在通过训练让CPU执行（合法的）条件分支或精心定义好的间接分支目标。

2）在第二阶段，攻击者会迫使作为受害者的高特权应用程序（或上一阶段所使用的进程）以预测执行的方式执行错误预测分支中所包含的指令。这些指令通常会将机密信息从受害者应用程序的上下文中转移至微架构信道（通常为CPU缓存）。

3）在最终阶段，攻击者会通过低特权进程恢复存储在CPU缓存（微架构信道）中的敏感信息，为此攻击者会探测整个缓存（与Meltdown攻击的做法相同），借此即可获得本应在受害者高特权地址空间中受到保护的机密信息。

Spectre攻击的第一个变体可通过迫使CPU分支预测器以预测执行的方式执行条件分支中错误的分支，进而获取存储在受害者进程地址空间（该地址空间可以是攻击者所控制的地址空间，或不受攻击者控制的地址空间）中的机密信息。该分支通常是一个函数的一部分，这个函数会在访问内存缓冲区中所包含的某些非机密数据之前执行边界检查。如果该缓冲区与某些机密数据相邻，并且攻击者控制了提供给分支条件的偏移量，攻击者即可反复训练分支预测器并提供合法的偏移量值，借此顺利通过边界检查并让CPU执行正确的路径。

随后，攻击者会准备一个精心定义的CPU缓存（通过精心调整内存缓冲区大小，使得边界检查无法位于缓存中）并为实现边界检查分支的函数提供一个非法的偏移量。通过训练，CPU分支预测器会始终沿用最初的合法路径，然而这一次的路径是错误的（此时本应选择其他路径）。因此访问内存缓冲区的指令会以预测执行的方式来执行，进而导致在边界之外执行以机密数据为目标的读取操作。通过这种方式，攻击者即可探测整个缓存并读取机密数据（与Meltdown攻击的做法类似）。

Spectre攻击的第二个变体利用了CPU分支目标预测器，并会对间接分支投毒。通过这种方式，即可在攻击者控制的上下文中，借助间接分支错误预测的路径读取受害者进程（或操作系统内核）的任意内存数据。如图 8-7 所示，对于变体2，攻击者会通过恶意目标对分支预测器进行误导性训练，使得CPU能在BTB中构建出足够的信息，进而以乱序执行的方式执行位于攻击者所选择的地址中的指令。在受害者的地址空间内，该地址本应指向Gadget。Gadget是一组可以访问机密数据，并将其存储在缓冲区（该缓冲区会以受控的方式进行缓存）中的指令（攻击者需要间接控制受害者一个或多个CPU寄存器的内容，如果API接受不可信的输入数据，那么这种目的很好实现）。

在攻击者完成对分支目标预测器的训练后，即可刷新CPU缓存并调用由目标高特权实体（进程或操作系统内核）提供的服务。实现该服务的代码必须同时实现与攻击者控制的进程类似的间接分支。随后，CPU分支目标预测器会以预测执行的方式执行位于错误目标地址中的Gadget。这与变体1和Meltdown攻击一样，会在CPU缓存中产生微架构副作用，进而使其可以从低特权上下文中读取。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx736.png)

图8-7　Spectre攻击变体2的结构

#### 其他侧信道攻击

Spectre和Meltdown攻击一经曝光，就催生了多种类似的侧信道硬件攻击。与Meltdown和Spectre相比，虽然其他攻击方式的破坏性和影响范围并没有那么大，但我们依然有必要了解这类全新侧信道攻击所采用的整体方法。

CPU性能优化措施所催生的预测式存储旁路（Speculative Store Bypass，SSB），可以让CPU评估过的加载指令不再依赖之前所用的存储，而是能够在存储的结果退出前以预测执行的方式执行。如果预测错误，则可能导致加载操作读取陈旧数据，其中很可能包含机密信息。读取到的数据可以转发给预测过程中执行的其他操作。这些操作可以访问内存并生成微架构副作用（通常位于CPU缓存中）。借此攻击者即可衡量副作用并从中恢复机密信息。

Foreshadow（又名L1TF）是一种更严重的攻击，在设计上，这种攻击最初是为了从硬件隔区（SGX）中窃取机密数据，随后广泛应用于在非特权上下文中执行的普通用户模式软件。Foreshadow利用了现代CPU预测执行引擎中的两个硬件漏洞，分别如下：

● 在不可访问的虚拟内存中进行预测。在本场景中，当CPU访问由页表项（Page Table Entry，PTE）所描述的虚拟地址中存储的某些数据时，如果未包含“存在”位（意味着该地址非有效地址），则将以正确的方式生成一个异常。然而，如果该项包含有效地址转换，CPU就可以根据读取的数据预测执行指令。与其他所有侧信道攻击方式类似，处理器并不会重试这些指令，但会产生可衡量的副作用。在这种情况下，用户模式应用程序即可读取内核内存中保存的机密数据。更严重的是，该应用程序在某些情况下还能读取其他虚拟机中的数据：当CPU转换客户物理地址（Guest Physical Address，GPA）时，如果在二级地址转换（Second Level Address Translation，SLAT）表中遇到了不存在的项，就会产生相同的副作用（有关SLAT、GPA以及转换机制的详细信息，请参阅本书卷1第5章，以及卷2第9章）。

● 在CPU内核的逻辑（超线程）处理器上进行预测。现代CPU的每个物理核心可以具备多条执行流水线，借此即可通过共享的执行引擎以乱序的方式同时执行多个指令（这是一种对称多线程（Symmetric Multi-Threading，SMT）架构，详见第9章）。在这种处理器中，两个逻辑处理器（Logical Processor，LP）共享同一个缓存。因此，当一个LP在高特权上下文中执行某些代码时，对端的另一个LP即可读取这个LP的高特权代码执行过程中产生的副作用。这会对系统的整体安全性造成极为严重的影响。与Foreshadow的第一个变体类似，在低特权上下文中执行攻击者代码的LP，甚至只需要等待虚拟机代码通过调度由对端LP执行，即可窃取其他高安全性虚拟机中存储的机密信息。Foreshadow的这个变体属于一种Group 4漏洞。

微架构副作用并非总是以CPU缓存为目标。为了更好地访问已缓存和未缓存的内存并对微指令重新排序，Intel的CPU使用了其他中等级别的高速缓冲区（不同缓冲区的介绍已超出本书范畴）。微架构数据采样（Microarchitectural Data Sampling，MDS）攻击可暴露下列微架构结构所包含的机密数据：

● **存储缓冲区（store buffer）**。在执行存储操作时，处理器会将数据写入一个名为存储缓冲区的内部临时微架构结构中，这样CPU就能在数据被真正写入缓存或主存（对于未缓存的内存访问）之前继续执行指令。当加载操作从与之前的存储相同的内存地址读取数据时，处理器可以从该存储缓冲区直接转发数据。

● **填充缓冲区（fill buffer）**。填充缓冲区是一种内部处理器结构，主要用于在一级数据缓存未命中（并且执行了I/O或特殊寄存器操作）时收集（或写入）数据。填充缓冲区在CPU缓存和CPU乱序执行引擎之间充当了中介的作用，其中可能保留了上一个内存请求所涉及的数据，这些数据可能会以推测的方式转发给加载操作。

● **加载端口（load port）**。加载端口是一种临时的内部CPU结构，主要用于从内存或I/O端口执行加载操作。

微架构缓冲区通常属于单一CPU内核，但会被SMT线程共享。这意味着，即使难以通过可靠的方式对这些结构发起攻击，在特定情况下依然有可能跨越SMT线程，通过推测的方式从中提取机密数据。

一般来说，所有硬件侧信道漏洞的后果都是相同的：可以从受害者地址空间中窃取机密数据。为了防范Spectre、Meltdown以及上文提到的其他各种侧信道攻击，Windows实现了多种缓解措施。

## 8.3　Windows中的侧信道缓解措施

本节简要介绍Windows如何通过各种缓解措施防范侧信道攻击。总的来说，某些侧信道缓解措施是由CPU制造商通过微码（microcode）更新实现的。然而，并非所有这类措施都始终可用，有些缓解措施需要由软件（Windows内核）启用。

### 8.3.1　KVA影子

内核虚拟地址影子（kernel virtual address shadowing）也称KVA影子（在Linux的世界中称为KPTI，代表内核页表隔离，kernel page table isolation），可在内核与用户页表之间创建清晰的隔离，借此缓解Meltdown攻击。当处理器未以正确的特权级别访问时，预测执行使得CPU能够获取到内核数据，但这要求在转换目标内核页的页表中存在一个有效的页帧编号。Meltdown攻击针对的内核内存通常会使用系统页表中有效的叶项（leaf entry）进行转换，这意味着需要具备监管特权级别（有关页表和虚拟地址转换的介绍请参阅本书卷1第5章）。在启用KVA影子后，系统会为每个进程分配并使用两个顶级页表：

● 内核页表，用于映射整个进程地址空间（包括内核和用户页）。在Windows中，用户页会以不可执行的方式进行映射，这是为了防止内核代码执行以用户模式分配的内存（这类似于硬件SMEP提供的功能）。

● 用户页表（又名影子页表），只负责映射用户页以及最少量不包含任何机密信息的内核页，可用于为页表切换、内核栈提供最基本的功能，以及中断、系统调用和其他转换、陷阱的处理。这组内核页也叫过渡（transition）地址空间。

在这个过渡地址空间中，NT内核通常会映射一种名为KPROCESSOR_DESCRIPTOR_ AREA的数据结构，该数据结构被包含在处理器的PRCB中，其中包含需要在用户（或影子）和内核页表之间共享的数据，如处理器的TSS、GDT以及内核模式GS基址的副本。此外，该过渡地址空间还包括NT内核映像“.KVASCODE”节下的所有影子陷阱处理程序。

当启用KVA影子的系统运行非特权用户模式线程（如以非管理员特权级别运行）时，处理器并不会映射任何可能包含机密数据的内核页。因此Meltdown攻击将彻底失效，因为内核页不再有效映射至进程的页表，并且任何以这些页为目标的CPU预测操作都无法继续进行。当用户进程使用系统调用，或当CPU在用户模式进程中执行代码的同时遇到中断时，CPU会在过渡栈上构建一个陷阱帧，并按照上文所述的方式将其同时映射至用户和内核页表。随后CPU会执行影子陷阱处理程序的代码，借此处理中断或系统调用。在处理系统调用时通常还需要切换至内核页表，复制内核栈中的陷阱帧，然后跳转至最初的陷阱处理程序（这意味着需要实现一种妥善的算法，以便刷新TLB中陈旧的项。下文将详细介绍TLB刷新算法）。这样即可在映射了整个地址空间的情况下，执行最初的陷阱处理程序。

#### 初始化

在内核初始化第1阶段的早期，当处理器功能位（feature bit）计算完毕后，NT内核会借助内部例程KiDetectKvaLeakage判断CPU是否会受到Meltdown攻击。该例程会获取处理器信息，并将除Atom（一种有序处理器）外其他所有Intel处理器的内部变量KiKvaLeakage都设置为“1”。

内部变量KiKvaLeakage设置完毕后，系统会通过KiEnableKvaShadowing例程启用KVA影子，并开始准备处理器的TSS和过渡栈。处理器TSS的RSP0（内核）和IST栈会设置为指向相应的过渡栈。随后在基栈中写入一种名为KIST_BASE_FRAME的数据结构，借此让过渡栈（其大小为512字节）做好准备。该数据结构使得过渡栈能够链接至自己的非过渡内核栈（只有在页表切换之后才能访问），如图8-8所示。请注意，常规的非IST内核栈并不需要该数据结构。操作系统可以从CPU的PRCB中获取用户与内核模式切换所需的全部数据。每个线程都有对应的内核栈。当新线程被选中执行后，调度器会将其内核栈链接至处理器的PRCB，以此激活该内核栈。这是内核栈与IST栈的一个重要差异，并且每个处理器中只存在一个IST栈。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx743.png)

图8-8　KVA影子被激活后，CPU任务状态段（TSS）的配置情况

KiEnableKvaShadowing例程还承担一个重要职责：确定适合的TLB刷新算法（下面将详细介绍）。而确定后的结果（全局项或PCID）会存储在全局变量KiKvaShadowMode中。最后，对于非引导处理器，该例程会调用KiShadowProcessorAllocation在影子页表中映射每个处理器的共享数据结构。对于BSP处理器，则会在初始化阶段1的后期，当SYSTEM进程及其影子页表均已成功创建（且IRQL已被降至被动级别）之后再进行映射。只有在这种情况下，影子陷阱处理程序（全局的，且并非每个处理器专用的）才会映射至用户页表。

#### 影子页表

当进程的地址空间创建完成后，内存管理器将使用内部例程MiAllocateProcessShadow分配影子（或用户）页表。新进程的影子页表在创建好后内容为空。随后，内存管理器会将SYSTEM进程的所有内核影子顶级页表项复制到新进程的影子页表中。

借此，操作系统可快速将整个过渡地址空间（位于内核中，被所有用户模式进程共享）映射给新进程。对于SYSTEM进程，影子页表依然为空，正如上一节所述，该页表将由KiShadowProcessorAllocation例程填充，这个例程会使用内存管理器服务将特定的内存块映射至影子页表，并重建整个页面层次结构。

内存管理器只会在特定情况下更新影子页表，并且仅有内核可以写入映射或解除映射。当一个请求需要分配或映射新内存到用户进程地址空间时，可能会遇到特定地址的顶级页表项丢失的情况。在这种情况下，内存管理器会分配整个页表层次结构的所有页面，并将新的顶级PTE存储在内核页表中。然而在启用KVA后，仅这样做还不够，内存管理器还必须在影子页表中写入顶级PTE。否则在陷阱处理程序正确切换页表后，返回用户模式之前，该地址将无法出现在用户映射中。

相比内核页表，内核地址会使用不同的方式映射至过渡地址空间。为防止错误地将与映射至过渡地址空间中的内存块距离太过接近的地址共享出来，内存管理器会始终为被共享的一个或多个PTE重建页表层次结构映射。这也意味着当内核需要在进程的过渡地址空间中映射某些新页面时，都必须在所有进程的影子页表中重复进行该映射（该操作完全由内部例程MiCopyTopLevelMappings负责）。

#### TLB刷新算法

在x86架构中，切换页表通常会导致刷新当前处理器的TLB（Translation Look-aside Buffer，转译后备缓冲区）。TLB是一种缓存，处理器会用它来快速转译在执行代码或访问数据时所用的虚拟地址。TLB中的有效项可以让处理器无须查询页表链，因此可加快执行速度。在未启用KVA影子的系统中，TLB中用于转译内核地址的项无须显式刷新。在Windows中，内核地址空间在大部分情况下是唯一的，并会被所有进程共享。Intel和AMD采用不同的技术来避免每次切换页表时刷新内核项，例如全局/非全局位和进程上下文标识符（Process-Context Identifier，PCID）。Intel与AMD的架构手册中详细描述了TLB及其刷新方法，本书不再深入讨论。

通过使用CPU的新功能，操作系统可以只刷新用户项，以此确保性能不受影响。但在启用KVA影子的情况下无疑是无法接受这种做法的，因为线程有义务切换页表，即使是在进入或退出内核时。在启用KVA的系统中，Windows会借助一种算法确保只在必要时才明确刷新内核和用户TLB项，进而实现下列两个目标：

● 在执行线程用户代码时，TLB中不维持任何有效的内核项。否则这些内核项可能被攻击者使用与Meltdown类似的推测技术所利用，进而读取机密的内核数据。

● 在切换页表时，只刷新最少量的TLB项。这样可确保因启用KVA影子而导致的性能损失处于可接受范围内。

TLB刷新算法主要应用于这三个场景：上下文切换、进入陷阱以及退出陷阱。无论是只支持全局/非全局位，还是在此基础上还能支持PCID的系统，都可以运行该算法。对于只支持全局/非全局位的系统，非KVA影子的配置将有所差异，其中所有内核页面都会标记为“非全局”，而过渡页和用户页会标记为“全局”。进行页表切换时，全局页不会被刷新（系统会更改CR3寄存器的值）。对于支持PCID的系统，则会将内核页标记为PCID 2，并将用户页标记为PCID 1。此时会忽略全局位和非全局位。

在当前执行的线程结束其量程（quantum）时，将会初始化上下文切换。当内核被调度去执行隶属于其他进程地址空间的线程时，TLB算法会保证TLB中的所有用户页均已移出（这意味着对于使用全局/非全局位的系统，需要进行一次彻底的TLB刷新，并且用户页会被标记为全局）。在内核退出陷阱（内核执行完代码返回用户模式）时，算法会保证TLB中的所有内核项已被移出（或作废）。这一点很容易实现，在支持全局/非全局位的处理器上，只需重新加载页表即可迫使处理器将所有非全局页作废；在支持PCID的系统中，用户页表会使用User PCID重新加载，进而让所有陈旧的内核TLB项自动作废。

该策略允许内核进入陷阱，即系统正在执行用户代码时产生了中断，或线程使用了系统调用，此时TLB中的一切都不会作废。上述TLB刷新算法的方案如表8-1所示。

表8-1　KVA影子TLB刷新策略

| 配置类型              | 用户页        | 内核页        | 过渡页        |
| ----------------- | ---------- | ---------- | ---------- |
| KVA影子已禁用          | 非全局        | 全局         | N / D      |
| KVA影子已启用，PCID策略   | PCID 1，非全局 | PCID 2，非全局 | PCID 1，非全局 |
| KVA影子已启用，全局/非全局策略 | 全局         | 非全局        | 全局         |

### 8.3.2　硬件间接分支控制（IBRS、IBPB、STIBP、SSBD）

处理器制造商也为不同的侧信道攻击设计了硬件层面的缓解措施。这些缓解措施在设计上能够与软件措施配合生效。有关侧信道攻击的硬件缓解措施主要通过下列间接分支控制机制来实现，具体采用何种机制通常是由CPU特殊模块寄存器（MSR）中的一位决定的。

● **间接分支限制推测（Indirect Branch Restricted Speculation，IBRS）**： 可在切换至不同安全上下文（用户/内核模式，或VM根/非根）时彻底禁用分支预测器（并刷新分支预测器缓冲区）。如果操作系统在过渡到更高特权的模式后设置了IBRS，那么间接分支预测目标将无法继续被低特权模式下执行的软件所控制。此外，在启用IBRS后，间接分支预测目标将无法被其他逻辑处理器所控制。操作系统通常会将IBRS设置为1，并在返回至较低特权安全上下文之前始终保持该设置。  
IBRS的实现取决于CPU制造商：一些CPU会在启用IBRS后彻底禁用分支预测器缓冲区（这是一种禁止行为），而其他CPU可能只会刷新预测器的缓冲区（这是一种刷新行为）。在这些CPU中，IBRS缓解措施的工作方式与IBPB的较为类似，因此这些CPU通常只会实现IBRS。

● **间接分支预测器屏障（Indirect Branch Predictor Barrier，IBPB）**：在设置为“1”后，会刷新分支预测器的内容，以此防止之前执行过的软件控制同一个逻辑处理器上的间接分支预测目标。

● **单线程间接分支预测器（Single Thread Indirect Branch Predictor，STIBP）**：可对同一个物理CPU内核上不同逻辑处理器之间共享的分支预测进行限制。将逻辑处理器的STIBP设置为“1”后，可防止当前正在执行的逻辑处理器的间接分支预测目标被同一个内核中其他逻辑处理器上执行（或曾经执行过）的软件所控制。

● **预测存储旁路禁止（Speculative Store Bypass Disable，SSBD）**：可以让处理器不以预测执行的方式加载，除非所有较旧的存储均处于已知状态。这样即可确保加载操作不会因为同一个逻辑处理器上较旧存储所产生的旁路，而以预测的方式使用陈旧的数据值，从而可防范预测性存储旁路攻击（详见上文“其他侧信道攻击”一节）。

NT内核会使用一种复杂的算法来确定上述间接分支限制机制的值，而这些值也会在上文有关KVA影子介绍中所提到的三个场景中产生相应的变化，这三个场景分别为上下文切换、进入陷阱以及退出陷阱。在兼容的系统中，系统会在始终启用IBRS的情况下运行内核代码（除非启用了Retpoline）。如果没有可用的IBRS（但IBPB和STIBP均可支持），内核将在启用STIBP的情况下运行，并在每次进入陷阱时（使用IBPB）刷新分支预测器缓冲区（这样，分支预测器就不会被用户模式运行的代码或在其他安全上下文中运行的“同胞”线程所影响）。如果CPU支持SSBD，则SSBD会始终在内核模式中启用。

出于性能方面的考虑，用户模式线程在执行时通常并不会启用硬件预测缓解措施，或只启用STIBP（取决于STIBP配对是否启用，详见下一节）。如果需要，则必须通过全局或每个进程的预测执行功能手动启用针对预测性存储旁路攻击的防护。实际上，所有预测缓解措施均可通过全局注册表值HKLM\System\CurrentControlSet\Control\Session Manager\ Memory Management\FeatureSettings加以调整。这是一个32位掩码值，其中的每一位对应一个具体的设置。表8-2总结了不同的功能设置及其含义。

表8-2　功能设置及其对应的值

| 名称                                              | 值       | 含义                                                                   |
| ----------------------------------------------- | ------- | -------------------------------------------------------------------- |
| FEATURE_SETTINGS_DISABLE_IBRS_EXCEPT_<br>HVROOT | 0x1     | 禁用IBRS，但非嵌套根分区除外（Server SKU的默认设置）                                    |
| FEATURE_SETTINGS_DISABLE_KVA_SHADOW             | 0x2     | 强制禁用KVA影子                                                            |
| FEATURE_SETTINGS_DISABLE_IBRS                   | 0x4     | 忽略计算机配置，直接禁用IBRS                                                     |
| FEATURE_SETTINGS_SET_SSBD_ALWAYS                | 0x8     | 始终在内核和用户模式下设置SSBD                                                    |
| FEATURE_SETTINGS_SET_SSBD_IN_KERNEL             | 0x10    | 仅在内核模式下设置SSBD（会导致用户模式代码容易遭受SSB攻击）                                    |
| FEATURE_SETTINGS_USER_STIBP_ALWAYS              | 0x20    | 忽略 STIBP 配对，始终为用户线程启用STIBP                                           |
| FEATURE_SETTINGS_DISABLE_USER_TO_USER           | 0x40    | 禁用默认的预测缓解策略（仅限 AMD 系统），只启用“用户对用户”的缓解措施。设置该标记后，内核模式下运行时将不再使用预测执行控制措施  |
| FEATURE_SETTINGS_DISABLE_STIBP_PAIRING          | 0x80    | 始终禁用STIBP配对                                                          |
| FEATURE_SETTINGS_DISABLE_RETPOLINE              | 0x100   | 始终禁用Retpoline                                                        |
| FEATURE_SETTINGS_FORCE_ENABLE_RETPOLINE         | 0x200   | 无论 CPU 可支持 IBPB 或 IBRS，均启用Retpoline（为防范Spectre v2，Retpoline至少需要IBPB） |
| FEATURE_SETTINGS_DISABLE_IMPORT_LINKING         | 0x20000 | 忽略Retpoline，直接禁用导入优化                                                 |

### 8.3.3　Retpoline和导入优化

硬件缓解措施会对系统性能产生极大影响，因为在启用这些缓解措施后，CPU的分支预测器会受到限制甚至被彻底禁用。对游戏和关键业务应用程序来说，大幅度的性能下降往往是无法接受的。用于防范Spectre的IBRS（或IBPB）可能是对性能产生最大影响的缓解措施。在内存屏障（memory fence）指令的帮助下，可以在不使用任何硬件缓解措施的情况下防范Spectre的第一个变体，例如x86架构中所用的LFENCE。这些指令会迫使处理器在屏障本身建立完成之前不以预测执行的方式执行任何新操作，仅在屏障建立完成（并且在此之前的所有指令均已退出）后，处理器的流水线才会重新开始执行（并预测）新的操作码（Opcode）。不过Spectre的第二个变体依然需要通过硬件缓解措施来预防，进而会因为IBRS和IBPB导致性能退化。

为了解决这个问题，Google的工程师设计了一种新颖的二进制修改技术，名为Retpoline。Retpoline代码序列如图8-9所示，可将间接分支从预测执行中隔离出来。这样无须执行存在漏洞的间接调用，处理器可以跳转至一个安全控制序列，该序列可动态地修改栈，记录最终的预测，并通过“Return”操作抵达新的目标。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx911.png)

图8-9　x86 CPU的Retpoline代码序列

在Windows中，Retpoline是在NT内核里实现的，这样可通过动态值重定位表（Dynamic Value Relocation Table，DVRT），动态地为内核与外部驱动程序映像应用Retpoline代码序列。当内核映像使用Retpoline编译（通过兼容的编译器）时，编译器会在映像的DVRT里为代码中存在的每个间接分支插入一个项，以此描述其地址和类型。执行该间接分支的操作码会照原样保存在最终的代码中，但会被增加一个大小可变的填充（padding）。DVRT中的项包含NT内核动态修改间接分支的操作码所需的全部信息。这种架构确保了使用Retpoline选项编译的外部驱动程序也可以在老版本操作系统中运行，为此只需跳过DVRT表中这些项的解析操作即可。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　DVRT的开发最初是为了支持内核ASLR（Address Space Layout Randomization，地址空间布局随机化，详见卷1第5章）。随后DVRT表通过扩展包含了Retpoline描述符。系统可以识别映像中所包含的DVRT表的版本。 |
| ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |

在初始化的阶段1，内核将检测处理器是否会受到Spectre攻击，如果系统可兼容并具备足够可用的硬件缓解措施，就会启用Retpoline并将其应用到NT内核映像和HAL。RtlPerformRetpolineRelocationsOnImage例程会扫描DVRT，将表中每项所描述的间接分支替换为不容易受到预测攻击，且以Retpoline代码序列为目标的直接分支。间接分支最初的目标地址会保存在一个CPU寄存器（AMD和Intel处理器的R10寄存器）中，并通过一条指令覆盖写入由编译器生成的填充。Retpoline代码序列会存储在 NT 内核映像的RETPOL节中，为该节提供支撑的页面会映射至每个驱动程序映像的结尾处。

启动前，内部例程MiReloadBootLoadedDrivers会将引导驱动程序物理迁移至其他位置，并为每个驱动程序的映像进行必要的修复（包括Retpoline）。所有引导驱动程序、NT内核以及HAL映像都会被Windows加载器（Windows Loader）分配一块连续的虚拟地址空间，该空间不包含相关的控制区域，因此这些空间将不可分页。这意味着为这些映像提供支撑的内存将始终驻留，并且NT内核可以使用同一个RtlPerformRetpolineRelocationsOnImage函数直接在代码中修改每个间接分支。如果启用了HVCI，那么系统必须调用安全内核（Secure Kernel）以应用Retpoline（借助安全调用PERFORM_RETPOLINE_RELOCATIONS）。实际上，在这个场景中，驱动程序的可执行内存会按照第9章介绍的安全内核写入执行限制措施加以保护，不允许任何形式的修改，仅安全内核可以进行修改。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　Retpoline和导入优化修复措施是由内核在PatchGuard（也叫内核补丁保护，Kernel Patch Protection，详见本书卷1第7章）初始化并提供一定程度的保护之前对引导驱动程序应用的。对于驱动程序和NT内核本身，修改受保护驱动程序的代码节是一种非法操作。 |
| ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |

运行时驱动程序（详见本书卷1第5章）由NT内存管理器负责加载，可创建出由驱动程序的映像文件支撑的节对象（section object）。这意味着为了跟踪内存节中的页面，需要创建一个控制区（包括原型PTE数组）。对于驱动程序节，一些物理页面最初被放入内存中只是为了验证代码的完整性，随后就会被转移至备用表（standby list）中。当这样的节随后被映射并且驱动程序的页面被首次访问时，来自备用表（或来自备份文件）的物理页面会被页面错误处理程序按需进行具体化。Windows会对原型PTE所指向的共享页面应用Retpoline。如果同一节同时也被用户模式的应用程序所映射，内存管理器就会新建一个私有页，并将共享页面中的内容复制到私有页，借此重新恢复Retpoline（以及导入优化）的修复措施。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　一些较新的Intel处理器还会对“Return”指令进行预测。此类CPU将无法启用Retpoline，因为无法借此防范Spectre v2。在这种情况下，只能使用硬件缓解措施。增强型IBRS（一种新的硬件缓解措施）解决了IBRS的性能退化问题。 |
| ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |

#### Retpoline位图

在Windows中实现Retpoline的最初设计目标（局限）之一在于需要为混合环境（同时包含兼容和不兼容Retpoline的驱动程序）提供支持，并针对Spectre v2提供整体性系统保护。这意味着不支持Retpoline的驱动程序应在启用IBRS（或在启用STIBP的情况下同时为内核项启用IBPB，详见“硬件间接分支控制”一节）的情况下运行，其他驱动程序则可在不启用任何硬件预测缓解措施的情况下运行（此时可由Retpoline代码序列和内存屏障提供保护）。

为了动态实现与老旧驱动程序的兼容性，在初始化的阶段0过程中，NT内核会分配并初始化一个动态位图，以此跟踪组成整个内核地址空间的每个64 KB内存块。在这种模型中，设置为“1”的位代表64 KB的地址空间块包含可兼容Retpoline的代码，反之则会设置为“0”。随后，NT内核会将代表HAL和NT映像（始终兼容Retpoline）的地址空间对应的位设置为“1”。每次加载新的内核映像后，系统都会尝试为其应用Retpoline。如果能成功应用，那么Retpoline位图中对应的位也会被设置为“1”。

Retpoline代码序列还可进一步加入位图检查功能：每次执行间接分支时，系统会检查最初的调用目标是否位于可兼容Retpoline的模块中。如果检查通过（且相关位被设置为“1”），则系统会执行Retpoline代码序列（见图8-9）并以安全的方式进入目标地址。否则（当Retpoline位图中的位被设置为“0”时）将会初始化Retpoline退出序列。随后，当前CPU的PRCB会设置RUNNING_NON_RETPOLINE_CODE标记（用于上下文切换），IBRS会被启用（或启用STIBP，取决于硬件配置），需要时会发出IBPB和LFENCE，并生成内核事件SPEC_CONTROL。最后，处理器依然能以安全的方式进入目标地址（由硬件缓解措施提供所需的保护能力）。

当线程量程终止且调度器选择新线程后，调度器会将当前处理器的Retpoline状态（由是否出现RUNNING_NON_RETPOLINE_CODE标记来表示）保存在旧线程的KTHREAD数据结构中。通过这种方式，当旧线程被选中再次执行（或发生了进入内核陷阱事件）时，系统就会知道自己需要重新启用所需的硬件预测缓解措施，进而确保系统能够始终获得保护。

#### 导入优化

DVRT中的Retpoline项还描述了以导入函数为目标的间接分支。DVRT会借助导入的控制传输项，使用指向IAT中正确项的索引来描述此类分支（IAT是指Image Import Address Table，即映像导入地址表，这是一种由加载器编译的导入函数指针数组）。当Windows加载器编译了IAT后，其内容通常就不太可能发生变化了（但存在一些罕见的例外情况）。如图8-10所示，其实并不需要将指向导入函数的间接分支转换为Retpoline分支，因为NT内核可以保证两个映像（调用方和被调用方）的虚拟地址足够接近，可直接调用（不超过2 GB的）目标。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1007.png)

图8-10　ExAllocatePool函数不同的间接分支

导入优化（import optimization，在内部通常称为“导入链接”）这项功能可使用Retpoline动态重定向，将指向导入函数的间接调用转换为直接分支。如果使用直接分支将代码执行过程转向至导入函数，则无须应用Retpoline，因为直接分支不会受到预测攻击。NT内核会在应用Retpoline的同时应用导入优化，虽然这两个功能可以单独配置，但为了正常生效，它们都用到了相同的DVRT项。借助导入优化，甚至在不会受到Spectre v2攻击的系统中，Windows也可以进一步获得性能提升（直接分支不需要任何额外的内存访问）。

### 8.3.4　STIBP配对

在超线程（hyper-thread）系统中，为保护用户模式代码免受Spectre v2攻击，系统至少会在启用了STIBP的情况下运行用户线程。在非超线程系统中则无须这样做：因为先前执行内核模式代码时已经启用了IBRS，此时已经可以防止先前执行的用户模式线程进行预测。如果启用了Retpoline，当跨进程切换线程并且首次从内核陷阱返回时，就已经发出了所需的IBPB。这确保了在执行用户线程代码前，CPU分支预测器缓冲区一定为空。

在超线程系统中启用STIBP会导致性能退化，因此，默认情况下，用户模式线程的STIBP会被禁用，这会导致线程可能受到来自同胞SMT线程的预测攻击。终端用户可以通过USER_STIBP_ALWAYS功能设置，或使用RESTRICT_INDIRECT_BRANCH_PREDICTION这个进程缓解选项为用户线程手动启用STIBP（详见“硬件间接分支控制”一节）。

上述场景并非最理想的。更好的解决方案是通过STIBP配对机制来实现。STIBP配对是由I/O管理器在NT内核初始化的阶段1启用的（使用KeOptimizeSpecCtrlSettings函数），但这需要满足一些条件。系统必须启用超线程，CPU需要支持IBRS和STIBP。此外，只有非嵌套虚拟化环境或禁用Hyper-V的情况下才能支持STIBP配对（详见第9章）。

在STIBP配对场景中，系统会为每个进程分配一个安全域标识符（存储在EPROCESS数据结构中），该标识符由一个64位数字表示。System安全域标识符（等于“0”）只会分配给使用System或完整管理令牌运行的进程。Nonsystem安全域则会在进程创建时（由内部函数PspInitializeProcessSecurit）按照如下规则分配：

● 如果新建的进程未明确分配新的主令牌，那么它会从创建它的父级进程获得相同的安全域。

● 如果新进程明确指定了新的主令牌（例如使用CreateProcessAsUser或CreateProcessWithLogon API），则会从内部符号PsNextSecurityDomain开始为新进程生成新的用户安全域ID。随后每生成一个新的域ID，其值都会增加（保证了在系统运行全过程中不会产生冲突的安全域）。

● 请注意，进程最初创建完毕后，还可以使用NtSetInformationProcess API（以及ProcessAccessToken信息类）分配新的主令牌。为了让该API的操作成功实现，进程需要创建为挂起状态（其中未运行任何线程）。至此，该进程依然具备最初的令牌并处于非冻结状态。新安全域则会按照上文介绍的规则进行分配。

安全域还可以以手动方式分配给属于同一组的不同进程。应用程序可以使用NtSetInformationProcess API以及ProcessCombineSecurityDomainsInformation类，将进程的安全域替换为同一组中其他进程的安全域。该API可接收两个进程句柄，并在两个令牌都被冻结的情况下替换第一个进程的安全域，而这两个进程可以通过PROCESS_VM_ WRITE和PROCESS_VM_OPERATION访问权打开对方。

STIBP配对机制的正常生效离不开安全域。STIBP配对可将逻辑处理器（LP）与其“同胞”链接在一起（两者共享一个物理内核。本节内容中出现的LP和CPU这两个术语可互换）。只有在本地CPU和远程CPU的安全域相同，或者两个LP中有一个闲置时，两个LP才会由STIBP配对算法（实现于内部函数KiUpdateStibpPairing中）进行配对。这些情况下，两个LP都可以在不设置STIBP的情况下运行，并暗地受到预测执行保护（对相同安全上下文中运行的同胞CPU进行此类攻击无法获得任何好处）。

STIBP配对算法实现于KiUpdateStibpPairing函数中，其中包含一个完整的状态机。只有当CPU的PRCB中所存储的配对状态信息变得陈旧时，陷阱退出处理程序才会调用该例程（会在系统退出内核模式开始执行用户模式线程时调用）。LP的配对状态主要会因为如下两个原因变得陈旧：

● NT调度器选择了在当前CPU上执行的新线程。如果新线程的安全域不同于旧线程，CPU的PRCB配对状态就会被标记为陈旧。随后STIBP配对算法会重新评估两者的配对状态。

● 当同胞CPU脱离闲置状态时，它会请求远程CPU重新评估自己的STIBP配对状态。

请注意，当LP在启用STIBP的情况下运行代码时，可防范来自同胞CPU的预测。STIBP配对是基于相反概念开发的：启用STIBP的情况下执行LP时，可保证同胞CPU能够防范来自自己的预测。这意味着当通过上下文切换进入不同的安全域时，完全不需要中断同胞CPU的执行，哪怕对方正在禁用STIBP的情况下运行用户模式代码。

上述场景唯独不适用于这种情况：调度器选择的VP调度线程（在启用根调度器的情况下为虚拟处理器提供支撑，详见第9章）隶属于VMMEM进程。这种情况下，系统会立刻向同胞线程发送IPI以便更新其STIBP配对状态。实际上，运行客户端虚拟机代码的VP调度线程始终可以决定禁用STIBP，导致同胞线程（同样运行于STIBP禁用的情况下）处于不受保护的状态。

实验：查询系统的侧信道缓解状态

Windows会使用原生API NtQuerySystemInformation，通过SystemSpeculationControl- Information和SystemSecureSpeculationControlInformation这两个信息类暴露侧信道缓解信息。很多工具可利用该API向终端用户显示系统的侧信道缓解状态：

● 由Matt Miller开发并由微软官方提供支持的PowerShell脚本SpeculationControl，这是一个开源工具，已发布至如下 GitHub 代码库：https://github.com/microsoft/ SpeculationControl。

● 由亚历克斯·伊奥尼斯库（本书作者之一）开发的SpecuCheck工具，同样已开源并发布至如下GitHub代码库：https://github.com/ionescu007/SpecuCheck。

● 由安德里亚·阿列维（本书作者之一）开发的SkTool，（在撰写本书时）已被纳入较新的Windows Insider版本中。

上述三个工具都能提供大致相同的结果。但只有SkTool能够显示安全内核中实现的侧信道缓解措施（虚拟机监控程序和安全内核详见第9章）。在这个实验中，我们将了解自己系统中启用了哪些缓解措施。请下载SpecuCheck并打开命令提示符窗口（在搜索框中输入cmd）执行该工具，随后应该能看到类似如下的输出结果：

```
SpecuCheck v1.1.1    --   Copyright(c) 2018 Alex Ionescu 
https://ionescu007.github.io/SpecuCheck/  --   @aionescu 
--------------------------------------------------------
　
Mitigations for CVE-2017-5754 [rogue data cache load] 
--------------------------------------------------------
[-] Kernel VA Shadowing Enabled:                       yes 
     > Unnecessary due lack of CPU vulnerability:    no 
     > With User Pages Marked Global:                no 
     > With PCID Support:                           yes 
     > With PCID Flushing Optimization (INVPCID):   yes 
　
Mitigations for CVE-2018-3620 [L1 terminal fault] 
[-] L1TF Mitigation Enabled:                           yes 
     > Unnecessary due lack of CPU vulnerability:    no 
     > CPU Microcode Supports Data Cache Flush:     yes 
     > With KVA Shadow and Invalid PTE Bit:         yes 
```

（为节省版面，上述输出结果已节略。）

此外，也可下载最新的Windows Insider版本并尝试使用SkTool工具。在不添加任何命令行参数的情况下启动该工具后，默认即可显示虚拟机监控程序和安全内核的状态。要查看所有侧信道缓解措施的状态，需要使用/mitigations这个命令行参数来调用该工具：

```
Hypervisor / Secure Kernel / Secure Mitigations Parser Tool 1.0 
Querying Speculation Features... Success! 
   This system supports Secure Speculation Controls. 
　
System Speculation Features. 
   Enabled: 1 
   Hardware support: 1 
   IBRS Present: 1 
   STIBP Present: 1 
   SMEP Enabled: 1 
   Speculative Store Bypass Disable (SSBD) Available: 1 
   Speculative Store Bypass Disable (SSBD) Supported by OS: 1 
   Branch Predictor Buffer (BPB) flushed on Kernel/User transition: 1 
   Retpoline Enabled: 1 
   Import Optimization Enabled: 1 
   SystemGuard (Secure Launch) Enabled: 0 (Capable: 0) 
   SystemGuard SMM Protection (Intel PPAM / AMD SMI monitor) Enabled: 0 
　
Secure system Speculation Features. 
   KVA Shadow supported: 1 
   KVA Shadow enabled: 1 
   KVA Shadow TLB flushing strategy: PCIDs 
   Minimum IBPB Hardware support: 0 
   IBRS Present: 0 (Enhanced IBRS: 0) 
   STIBP Present: 0 
   SSBD Available: 0 (Required: 0) 
   Branch Predictor Buffer (BPB) flushed on Kernel/User transition: 0 
   Branch Predictor Buffer (BPB) flushed on User/Kernel and VTL 1 transition: 0 
   L1TF mitigation: 0 
   Microarchitectural Buffers clearing: 1
```

## 8.4　陷阱调度

中断和异常是一类会导致处理器在常规控制流范围外执行代码的操作系统状况。硬件和软件都可能导致此类状况。陷阱（trap）是指在发生异常或中断时，处理器捕获执行中的线程并将控制权转交到操作系统中固定位置的机制。在Windows中，处理器会将控制权转交给陷阱处理程序（trap handler），这是一种针对特定中断或异常的函数。图8-11展示了一些可能会激活陷阱处理程序的状况。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1110.png)

图8-11　陷阱调度

内核会通过如下方式区分中断和异常。中断（interrupt）是一种异步事件（可能会在任何时间发生），但通常与处理器正在执行的工作无关。中断主要由I/O设备、处理器时钟或计时器生成的，可以启用（开启）或禁用（关闭）。作为对比，异常（exception）是一种同步状况，通常是在执行特定指令时产生的（对计算机检查的中止是一种处理器异常，但这通常与指令的执行无关）。异常和中止有时也被称为错误（fault），如页面错误（page fault）或双重错误（double fault）。在相同条件下用相同数据再次运行程序可以重现异常。异常的常见范例包括内存访问冲突、某些调试器指令及“除以零”错误等。内核也会将系统服务调用视为异常（不过从技术的角度来看，它们其实是系统陷阱）。

无论硬件或软件都可能产生异常和中断。例如，硬件问题可能造成总线错误异常，软件Bug可能导致“除以零”异常。同样，I/O设备也可产生中断，而内核本身也可能产生软件中断（如APC或DPC，下面将介绍它们）。

产生硬件异常或中断后，x86和x64处理器首先会检查当前代码段（Code Segment，CS）是否位于CPL 0或更低级别（即当前线程是在内核模式还是用户模式下运行）。如果线程已经运行在Ring 0级别，则处理器会为当前栈存储（或推送）下列信息，这相当于进行了从内核到内核的过渡。

● 当前处理器的标记（EFLAGS/RFLAGS）。

● 当前的代码段（CS）。

● 当前的程序计数器（EIP/RIP）。

● 可选：某些类型异常的错误代码。

当处理器实际在Ring 3级别下运行用户模式代码时，首先会根据任务寄存器（Task Register，TR）查找当前的TSS，随后在x86系统中切换至SS0/ESP0，或在x64系统中直接切换至RSP0，这一过程已在“任务状态段”中进行了介绍。随着处理器开始在内核栈上执行，它会首先存储之前的SS（用户模式值）和ESP（用户模式栈），随后存储从内核到内核过渡期间的其他相同数据。

存储这些数据可以获得双重收益。首先，可以在内核栈中记录足够的计算机状态信息，以便在当前线程的控制流中返回最初的点位并继续执行，就好像什么事情都没有发生过。其次，由此操作系统可以（根据保存的CS值）得知陷阱的来源，例如可以得知某个异常是来自用户模式代码还是内核系统调用。

由于处理器仅存储还原控制流所必需的信息，计算机的其他状态（包括EAX、EBX、ECX、EDI等寄存器）均保存在陷阱帧中，这是Windows在线程的内核栈中分配的一种数据结构。陷阱帧存储了线程的执行状态，属于线程完整上下文的超集，并包含额外的状态信息。若要查看其定义，可在内核调试器中使用dt nt!_KTRAP_FRAME命令，或下载Windows驱动程序开发包（WDK）并查看NTDDK.H头文件，其中包含相关定义及备注信息（有关线程上下文的详细介绍请参阅本书卷1第5章）。内核会将软件中断作为硬件中断的一部分加以处理，或当线程调用与软件中断有关的内核函数时以同步的方式来处理。

大部分情况下，在将控制权转交给产生陷阱的其他函数之前或之后，内核会安装前端陷阱处理函数，并以此执行与陷阱有关的常规处理任务。举例来说，如果遇到设备中断，内核硬件中断陷阱处理程序会将控制权转交给设备驱动程序为中断设备提供的中断服务例程（Interrupt Service Routine，ISR）。如果相关状况是由系统服务的调用所致，那么常规系统服务陷阱处理程序会将控制权转交给执行体中的特定系统服务函数。

在一些不常见的情形下，内核还会收到本不应看到或处理的陷阱或中断。有时这些情况也叫虚假陷阱或非预期陷阱。陷阱处理程序通常会执行系统函数KeBugCheckEx，当内核检测到有问题或错误的行为时，它会将计算机挂起，如果不检查这样的情况，则可能会导致数据出错。下一节将进一步详细介绍中断、异常以及系统服务调度。

### 8.4.1　中断调度

硬件生成的中断通常源自那些需要通知处理器自己何时需要服务的I/O设备。中断驱动的设备可以用重叠的方式集中处理I/O操作，以此让操作系统最大限度地充分利用处理器。当线程向/从一个设备启动I/O传输后，即可在设备完成传输操作的过程中执行其他工作。当设备传输操作完成后，会向处理器发出中断，以便要求获得服务。指点设备、打印机、键盘、磁盘驱动器以及网卡通常都属于中断驱动的设备。

系统软件也可以产生中断。举例来说，内核产生软件中断以初始化线程调度，并以异步的方式打断线程的执行。内核还可以禁用中断，这样处理器就不会再遇到中断，但这种情况并不常见，通常只发生在一些关键时刻，如对中断控制器进行编程或调度异常时。

为响应设备中断，内核会安装中断陷阱处理程序。中断陷阱处理程序可以将控制权转交给处理该中断的外部例程（ISR），或者转交给响应该中断的内部内核例程。设备驱动程序会为设备中断的相关服务提供ISR，其他类型的中断则由内核提供中断处理例程。

在下面几节我们将介绍硬件向处理器发出设备中断通知的方式、内核可支持的中断类型、设备驱动程序与内核交互的方式（这是中断处理工作的一部分）、内核可识别的软件中断（以及用于实现中断的内核对象）。

#### 硬件中断处理

在Windows可支持的硬件平台上，外部I/O中断将成为中断控制器（例如I/O高级可编程中断控制器，I/O Advanced Programmable Interrupt Controller，IOAPIC）的一种输入。随后控制器将打断一个或多个处理器的本地高级可编程中断控制器（Local Advanced Programmable Interrupt Controller，LAPIC），最终在输入线上中断处理器。

被中断的处理器会向控制器查询全局系统中断向量（Global System Interrupt Vector，GSIV），GSIV有时会表现为一个中断请求（Interrupt Request，IRQ）编号。中断控制器可将GSIV转换为处理器中断向量，随后将该向量作为中断调度表（Interrupt Dispatch Table，IDT）这种数据结构的索引，IDT存储在CPU的IDT寄存器（即IDTR）中，可以为中断向量返回匹配的IDT项。

根据IDT项所包含的信息，处理器可以将控制转交给Ring 0级别下运行的相应中断调度例程（这一进程的具体描述可参阅本节开头处），或者也可以使用一种名为中断门（interrupt gate）的进程，加载新的TSS并更新任务寄存器（TR）。对于Windows，在系统引导过程中，内核会向IDT中填充指针，这些指针指向部分专用内核与HAL例程，它们与每个异常以及内部处理过的中断相对应。此外，还有一些指针会指向一种名为KiIsrThunk的形式转换（Thunk）内核例程，借此处理第三方设备驱动程序可注册的外部中断。在x86和x64架构的处理器中，与中断向量0～31所关联的前32个IDT项是为处理器陷阱保留的，详见表8-3的介绍。

表8-3　处理器陷阱

| 向量（助记缩写） | 含义          |
| -------- | ----------- |
| 0 (#DE)  | 除法错误        |
| 1 (#DB)  | 调试陷阱        |
| 2 (NMI)  | 不可屏蔽的中断     |
| 3 (#BP)  | 断点陷阱        |
| 4 (#OF)  | 溢出错误        |
| 5 (#BR)  | 边界错误        |
| 6 (#UD)  | 未定义的操作码错误   |
| 7 (#NM)  | FPU错误       |
| 8 (#DF)  | 双重错误        |
| 9 (#MF)  | 协处理器错误（已弃用） |
| 10 (#TS) | TSS错误       |
| 11 (#NP) | 段错误         |
| 12 (#SS) | 栈错误         |
| 13 (#GP) | 常规保护错误      |
| 14 (#PF) | 页面错误        |
| 15       | 保留          |
| 16 (#MF) | 浮点错误        |
| 17 (#AC) | 对齐检查错误      |
| 18 (#MC) | 机器检查中止      |
| 19 (#XM) | SIMD错误      |
| 20 (#VE) | 虚拟化异常       |
| 21 (#CP) | 控制保护异常      |
| 22～31    | 保留          |

其余IDT项包含硬编码的值（例如向量30～34始终用于与Hyper-V有关的VMBus中断）以及设备驱动程序、硬件、中断控制器与平台软件（如ACPI）协商获得的值。例如，键盘控制器可能会在一个Windows系统中发出中断向量82，而在另一个系统中可能会发出中断向量67。

实验：查看64位IDT

我们可以使用调试器命令!idt查看IDT的内容，包括与Windows为中断（包括异常和IRQ）分配的陷阱处理程序相关的信息。在不包含任何标记的情况下运行!idt命令，可以显示简化后的输出结果，其中仅包含已注册的硬件中断（在64位计算机上还会包含处理器陷阱处理程序）。

下列范例展示了在x64系统上运行!idt命令后看到的结果：

```
0: kd> !idt 
　
Dumping IDT: fffff8027074c000 
　
00:     fffff8026e1bc700 nt!KiDivideErrorFault 
01:     fffff8026e1bca00 nt!KiDebugTrapOrFault  Stack = 0xFFFFF8027076E000 
02:     fffff8026e1bcec0 nt!KiNmiInterrupt  Stack = 0xFFFFF8027076A000 
03:     fffff8026e1bd380 nt!KiBreakpointTrap 
04:     fffff8026e1bd680 nt!KiOverflowTrap 
05:     fffff8026e1bd980 nt!KiBoundFault 
06:     fffff8026e1bde80 nt!KiInvalidOpcodeFault 
07:     fffff8026e1be340 nt!KiNpxNotAvailableFault 
08:     fffff8026e1be600 nt!KiDoubleFaultAbort  Stack = 0xFFFFF80270768000 
09:     fffff8026e1be8c0 nt!KiNpxSegmentOverrunAbort 
0a:     fffff8026e1beb80 nt!KiInvalidTssFault 
0b:     fffff8026e1bee40 nt!KiSegmentNotPresentFault 
0c:     fffff8026e1bf1c0 nt!KiStackFault 
0d:     fffff8026e1bf500 nt!KiGeneralProtectionFault 
0e:     fffff8026e1bf840 nt!KiPageFault 
10:     fffff8026e1bfe80 nt!KiFloatingErrorFault 
11:     fffff8026e1c0200 nt!KiAlignmentFault 
12:     fffff8026e1c0500 nt!KiMcheckAbort  Stack = 0xFFFFF8027076C000 
13:     fffff8026e1c0fc0 nt!KiXmmException 
14:     fffff8026e1c1380 nt!KiVirtualizationException 
15:     fffff8026e1c1840 nt!KiControlProtectionFault 
1f:     fffff8026e1b5f50 nt!KiApcInterrupt 
20:     fffff8026e1b7b00 nt!KiSwInterrupt 
29:     fffff8026e1c1d00 nt!KiRaiseSecurityCheckFailure 
2c:     fffff8026e1c2040 nt!KiRaiseAssertion 
2d:     fffff8026e1c2380 nt!KiDebugServiceTrap 
2f:     fffff8026e1b80a0 nt!KiDpcInterrupt 
30:     fffff8026e1b64d0 nt!KiHvInterrupt 
31:     fffff8026e1b67b0 nt!KiVmbusInterrupt0 
32:     fffff8026e1b6a90 nt!KiVmbusInterrupt1 
33:     fffff8026e1b6d70 nt!KiVmbusInterrupt2 
34:     fffff8026e1b7050 nt!KiVmbusInterrupt3 
35:     fffff8026e1b48b8 hal!HalpInterruptCmciService (KINTERRUPT fffff8026ea59fe0)
b0:     fffff8026e1b4c90 ACPI!ACPIInterruptServiceRoutine (KINTERRUPT ffffb88062898dc0)
ce:     fffff8026e1b4d80 hal!HalpIommuInterruptRoutine (KINTERRUPT fffff8026ea5a9e0)
d1:     fffff8026e1b4d98 hal!HalpTimerClockInterrupt (KINTERRUPT fffff8026ea5a7e0)
d2:     fffff8026e1b4da0 hal!HalpTimerClockIpiRoutine (KINTERRUPT fffff8026ea5a6e0)
d7:     fffff8026e1b4dc8 hal!HalpInterruptRebootService (KINTERRUPT fffff8026ea5a4e0)
d8:     fffff8026e1b4dd0 hal!HalpInterruptStubService (KINTERRUPT fffff8026ea5a2e0)
df:     fffff8026e1b4e08 hal!HalpInterruptSpuriousService (KINTERRUPT fffff8026ea5a1e0)
e1:     fffff8026e1b8570 nt!KiIpiInterrupt 
e2:     fffff8026e1b4e20 hal!HalpInterruptLocalErrorService (KINTERRUPT fffff8026ea5a3e0)
e3:     fffff8026e1b4e28 hal!HalpInterruptDeferredRecoveryService 
                         (KINTERRUPT fffff8026ea5a0e0) 
fd:     fffff8026e1b4ef8 hal!HalpTimerProfileInterrupt (KINTERRUPT fffff8026ea5a8e0)
fe:     fffff8026e1b4f00 hal!HalpPerfInterrupt (KINTERRUPT fffff8026ea5a5e0) 
```

在执行上述实验的系统中，ACPI SCI ISR位于中断编号B0h。此外，我们还可以看到，中断14 (0Eh)对应了KiPageFault，由上文的介绍可知，这是一种预定义的CPU陷阱。

另外我们还会注意到，有些中断（尤其是1、2、8、12）的旁边有一个栈指针。这些栈指针对应了上文“任务状态段”中所介绍的陷阱，需要由专用的安全内核栈来处理。通过转储IDT项，调试器可以得知这些栈指针的存在，而我们也可以使用dx命令并取消对IDT中某个中断向量的引用来达到相同的目的。虽然我们可以从处理器的IDTR获得IDT，但其实也可以从内核的KPCR结构中获得，该结构在一个名为IdtBase的字段中有一个指向IDT的指针。

```
0: kd> dx @$pcr->IdtBase[2].IstIndex 
@$pcr->IdtBase[2].IstIndex : 0x3 [Type: unsigned short] 
　
0: kd> dx @$pcr->IdtBase[0x12].IstIndex 
@$pcr->IdtBase[0x12].IstIndex : 0x2 [Type: unsigned short] 
```

将上述IDT值与上一个实验中转储的x64 TSS值进行比较，就会看到与该实验有关的可匹配的内核栈指针。

每个处理器都有自己的IDT（由自己的IDTR所指向），因此必要时，不同的处理器可以运行不同的ISR。例如在多处理器系统中，每个处理器都能收到时钟中断，但只有一个处理器可以更新系统时钟以响应此中断。不过所有的处理器都可以使用该中断来衡量线程量程，并在线程量程结束后发起重调度。类似地，有些系统配置可能需要由特定的处理器来处理某些设备中断。

#### 可编程中断控制器架构

传统x86系统依赖i8259A可编程中断控制器（Programmable Interrupt Controller，PIC），这是一项源自早期IBM PC的标准。i8259A PIC仅适用于单处理器系统，且只包含8条中断线（interrupt line）。然而IBM PC体系结构还额外定义了一种名为Secondary的第二个PIC，其中断可通过多路传输（multiplexed）进入主PIC的一条中断线中。这样总共就可以提供15个中断（7个位于主PIC，8个位于辅PIC，通过主PIC的第八条中断线进行多路传输）。由于PIC会通过如此奇特的方式处理8个以上的设备，并且15个中断依然不太够用，以及受各种电气问题（很容易造成虚假的中断）以及单处理器支持本身存在局限的影响，所以现代系统逐渐淘汰了这种类型的中断控制器，转而使用一种名为i82489高级可编程中断控制器（Advanced Programmable Interrupt Controller，APIC）的变体。

由于APIC可适用于多处理器系统，所以Intel与其他公司还定义了多处理器规范（Multiprocessor Specification，MPS），这适用于x86多处理器系统的设计标准且以APIC的使用为中心，并将连接外部硬件设备的I/O APIC（IOAPIC）与连接处理器内核的本地APIC（LAPIC）进行了集成。随着时间的推移，MPS标准被融入高级配置和电源接口（Advanced Configuration and Power Interface，ACPI）中，这两个标准的首字母缩写如此相似纯属巧合。为了兼容单处理器操作系统以及在单处理器模式下启动多处理器系统的引导代码，APIC支持一种PIC兼容模式，该模式可提供15个中断，并且中断只会被传递给主处理器。APIC架构如图8-12所示。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1241.png)

图8-12　APIC架构

如上文所述，APIC包含多个组件：一个负责从设备接收中断的I/O APIC，多个在总线上接收来自I/O APIC的中断并打断所关联处理器的本地APIC，以及一个将APIC信号转换为等价PIC信号且可兼容i8259A的中断控制器。由于系统中可能存在多个I/O APIC，所以主板上通常会在它们以及处理器之间放置一定的核心逻辑。该逻辑负责实现中断路由算法，借此跨越多个处理器对设备中断的负载进行均衡，并充分利用位置的毗邻性，将设备中断发送给刚刚处理过相同类型中断的同一个处理器。软件程序可以通过一种固定的路由算法对I/O APIC重编程，进而绕过这种芯片组逻辑。大部分情况下，Windows会用自己的路由逻辑对I/O APIC重编程以便支持各种功能（如中断路由控制），但设备驱动程序和固件也可以这样做。

因为x64架构可兼容x86操作系统，所以x64系统必须提供与x86相同的中断控制器。不过此时的一个重大差异在于，x64版本的Windows会拒绝在不包含APIC的系统中运行，因为x64版Windows需要使用APIC实现中断控制，而x86版的Windows可同时支持PIC和APIC硬件。这种情况在Windows 8和Windows后续版本中有所变化，无论CPU架构如何，这些系统都只能在APIC硬件上运行。x64系统的另一个差异在于，APIC的任务优先级寄存器（Task Priority Register，TPR）已经直接绑定至处理器的控制寄存器8（Control Register 8，CR8）。包括Windows在内的现代操作系统会使用该寄存器存储当前软件中断优先级（在Windows中这叫IRQL），并在做出路由决策时告知IOAPIC。下文很快将介绍有关IRQL处理的更多信息。

实验：查看PIC和APIC

我们可以分别使用内核调试器命令!pic和!apic查看单处理器系统的PIC配置以及多处理器系统的当前本地APIC。单处理器系统中的!pic命令输出结果如下。请注意，即使在具备APIC的系统中，该命令依然可以生效，因为为了模拟老旧硬件，APIC系统始终包含相关联的等价PIC。

```
lkd> !pic 
----- IRQ Number ----- 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F 
Physically in service:  Y  .  .  .  .  .  .  .  .  Y  Y  Y  .  .  .  . 
Physically masked:      Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y 
Physically requested:   Y  .  .  .  .  .  .  .  .  Y  Y  Y  .  .  .  . 
Level Triggered:        .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
```

在启用Hyper-V的系统中运行!apic命令的输出结果如下，从中可见，由于SINTI项的存在，此处引用了Hyper-V的综合中断控制器（Synthetic Interrupt Controller，SynIC，详见第9章的介绍）。另外还请注意，在本地内核调试过程中，该命令可显示与当前处理器相关联的APIC，换句话说，也就是在运行该命令时恰好用于运行调试器线程的任何一个处理器。如果要查看崩溃转储或远程系统，可以使用~命令，后跟想要查看的本地APIC所对应的处理器编号。无论哪种情况，ID:标记旁边的编号都对应了我们想要查看的处理器。

```
lkd> !apic 
Apic (x2Apic mode)  ID:1 (50014)  LogDesc:00000002  TPR 00 
TimeCnt: 00000000clk  SpurVec:df  FaultVec:e2  error:0 
Ipi Cmd: 00000000`0004001f  Vec:1F  FixedDel    Dest=Self    edg high 
Timer..: 00000000`000300d8  Vec:D8  FixedDel    Dest=Self    edg high    m 
Linti0.: 00000000`000100d8  Vec:D8  FixedDel    Dest=Self    edg high    m 
Linti1.: 00000000`00000400  Vec:00  NMI         Dest=Self    edg high 
Sinti0.: 00000000`00020030  Vec:30  FixedDel    Dest=Self    edg high 
Sinti1.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sinti2.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sinti3.: 00000000`000000d1  Vec:D1  FixedDel    Dest=Self    edg high 
Sinti4.: 00000000`00020030  Vec:30  FixedDel    Dest=Self    edg high 
Sinti5.: 00000000`00020031  Vec:31  FixedDel    Dest=Self    edg high 
Sinti6.: 00000000`00020032  Vec:32  FixedDel    Dest=Self    edg high 
Sinti7.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sinti8.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sinti9.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sintia.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sintib.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sintic.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sintid.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sintie.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
Sintif.: 00000000`00010000  Vec:00  FixedDel    Dest=Self    edg high    m 
TMR: 95, A5, B0 
IRR: 
ISR: 
```

Vec后跟的各种编号代表了特定命令的IDT中所关联的向量。例如，在上述输出结果中，中断编号0x1F关联了中断处理器中断（Interrupt Processor Interrupt，IPI）向量，而中断编号0xE2负责处理APIC错误。再次查看上一个实验中!idt命令的输出结果将会发现，0x1F是内核的APC中断（意味着刚刚使用了IPI从一个处理器向另一个处理器发送了APC），而0xE2当然就是HAL的本地APIC错误处理程序。

下列输出是!ioapic命令的运行结果，其中显示了I/O APIC的配置，以及连接到设备的中断控制器组件。例如，请留意GSIV/IRQ 9（系统控制中断，System Control Interrupt，SCI）是如何关联到向量B0h的，而在上一个实验的!idt命令输出结果中，当时关联的是ACPI.SYS。

```
0: kd> !ioapic 
Controller at 0xfffff7a8c0000898 I/O APIC at VA 0xfffff7a8c0012000 
IoApic @ FEC00000  ID:8 (11)  Arb:0 
Inti00.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti01.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti02.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti03.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti04.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti05.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti06.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti07.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti08.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti09.: ff000000`000089b0  Vec:B0  LowestDl  Lg:ff000000      lvl high 
Inti0A.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m 
Inti0B.: 00000000`000100ff  Vec:FF  FixedDel  Ph:00000000      edg high    m
```

#### 软件中断请求级别（IRQL）

虽然中断控制器会按照一定的优先级顺序来执行中断，但Windows会强制实行自己的中断优先级方案，名为中断请求级别（Interrupt Request Level，IRQL）。在内部，内核会使用数字0～31（x86）或0～15（x64以及ARM/ARM64）代表IRQL，数字越大，中断优先级越高。虽然内核为软件中断定义了一套标准的IRQL，但HAL会将硬件中断编号映射至这些IRQL。图8-13展示了为x86架构和x64（以及ARM/ARM64）架构定义的IRQL。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1248.png)

图8-13　x86和x64的中断请求级别（IRQL）

中断会按照优先级顺序获得服务，较高优先级的中断可以抢占低优先级中断获得服务的机会。当发生高优先级中断后，处理器会保存被中断线程的状态，并调用与该中断关联的陷阱调度程序。陷阱调度程序会提升IRQL并调用中断的服务例程。该服务例程执行完毕后，中断调度程序会将处理器的IRQL降低为该中断发生之前的级别，随后加载保存的计算机状态。被中断的线程可以从之前断掉的地方恢复执行。当内核降低IRQL时，之前被遮蔽的低优先级中断可能会被具体化（materialize）。如果发生这种情况，内核会重复执行该过程来处理新中断。

IRQL优先级与线程调度优先级（详见本书卷1第5章）有着截然不同的含义。调度优先级是线程本身的一种属性，而IRQL是中断来源（如键盘或鼠标）的一种属性。此外，每个处理器都有一个会随操作系统代码执行而改变的IRQL设置。正如上文所述，在x64系统中，IRQL会存储在CR8寄存器中，后者会映射回APIC的TPR上。

每个处理器的IRQL设置决定了处理器可以接收哪些中断。IRQL还可用于对内核模式数据结构进行同步访问（下面将详细介绍同步）。当内核模式线程运行时，会调用KeRaiseIrql和KeLowerIrql直接提升或降低处理器的IRQL，或者更常见的做法是通过调用获取内核同步对象的函数来间接更改IRQL。如图8-14所示，如果中断来源的IRQL高于当前级别，则这种中断会打断处理器的执行；而如果中断来源的IRQL等于或低于当前级别，那么在有执行线程低于该IRQL之前，此类中断会被遮蔽。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1255.png)

图8-14　中断的遮蔽

根据所要执行的操作，内核模式线程可以提高或降低自己运行所在处理器的IRQL。例如，当发生中断后，陷阱处理程序（或者也可能是处理器本身，这取决于具体架构）会将处理器的IRQL提升至中断来源所分配的IRQL。这种提升会使得（仅这一个处理器上）所有等于或低于该IRQL的中断被遮蔽，这样即可确保处理器对高IRQL中断提供的服务不会被同级或更低级别的中断拦截。被遮蔽的中断可以被其他处理器处理，或一直等待，直到IRQL降低。因此系统中的所有组件（包括内核和设备驱动程序）都会尽可能地保持IRQL为被动级别（有时也叫低级别）。这样做是因为，即使IRQL在很长时间里没能保持非必要的提升状态，设备驱动程序也可以及时响应硬件中断。因此，当系统没有执行任何中断工作（或需要与中断同步）或处理诸如DPC或APC等软件中断时，IRQL可以始终为“0”。很明显，这也适用于所有用户模式的处理，因为允许用户模式代码碰触IRQL可能会对系统运行产生极大影响。实际上，以大于0的IRQL返回到用户模式线程会导致系统立即崩溃（BugCheck），对驱动程序来说这是一种非常严重的Bug。

最后请注意，调度程序自身是以IRQL 2级别运行的（例如，因为抢占而从一个线程上下文切换至另一个线程），因此才有了“调度级别”（dispatch level）的概念，意味着处理器在这个级别以及更高级别上将表现为类似单线程合作运行的工作方式。然而此时的一些做法是非法的，例如，等待处于这种IRQL的调度程序对象（有关该话题的详情请参阅下文“同步”一节），因为通过上下文切换进入另一个线程（或Idle线程）的情况永远不会发生。另一个限制在于，仅未分页的内存可以在DPC/Dispatch级别或更高IRQL级别上访问。

这一规则实际上属于第一个限制所产生的副作用，因为试图访问非常驻内存的操作会导致页面错误。当发生页面错误时，内存管理器会发起磁盘I/O操作，随后需要等待文件系统驱动程序从磁盘中读取页面内容。进而，这个等待过程需要调度器执行上下文切换（如果没有别的用户线程等待运行，也许会切换至Idle线程），而这就违反了“调度器无法被调用”这一规则（因为在读取磁盘时，IRQL依然处于DPC/Dispatch级别或更高级别）。进一步还会导致另一个问题：I/O完成操作通常发生在APC_LEVEL，即使有时并不需要等待，I/O也永远无法完成，因为真正需要“完成”的APC根本没有机会运行。

如果违反上述两个限制中的任何一个，系统会崩溃并显示IRQL_NOT_LESS_OR_ EQUAL或DRIVER_IRQL_NOT_LESS_OR_EQUAL崩溃代码（有关系统崩溃的详细讨论请参阅第10章）。违反这些限制是设备驱动程序最常见的Bug之一。Windows驱动程序验证器（Windows driver verifier）提供了一个选项，可以帮助我们查找这种类型的Bug。

反之，这也意味着当运行在IRQL 1（也叫APC级别）时，依然可以进行抢占或上下文切换。这使得IRQL 1的行为结果在本质上更像是一种线程本地IRQL而非处理器本地IRQL，因为在IRQL 1上执行的等待或抢占操作会导致调度器将当前IRQL保存到线程的控制块（位于KTHREAD结构中，详见本书卷1第5章）中，并将处理器的IRQL还原为新执行线程的IRQL。这意味着处于被动级别（IRQL 0）的线程依然可以抢占运行在APC级别（IRQL 1）的线程，因为在IRQL 2以下的级别中，是由调度器来决定由哪个线程控制处理器的。

实验：查看IRQL

我们可以使用调试器命令!irql查看处理器已保存的IRQL。已保存的IRQL代表调试器进入之前那一刻的IRQL，在这之后，IRQL将提升至一个静态且无实际意义的值：

```
kd> !irql 
Debugger saved IRQL for processor 0x0 -- 0 (LOW_LEVEL) 
```

请注意，IRQL值会保存在两个位置。第一个位置是处理器控制区（Processor Control Region，PCR），其中所存储的值代表当前的IRQL；第二个位置是PCR的扩展，即处理器区控制块（Processor Region Control Block，PRCB），其中包含了DebuggerSavedIRQL字段中已保存的IRQL。使用这种保存方式的原因在于，远程内核调试器的使用会将IRQL升高至HIGH_LEVEL，以便在用户调试计算机时阻止所有异步的处理器操作，因为这种操作会导致!irql命令的输出结果变得毫无意义。因此会使用这种“保存”的值代表调试器连接之前那一刻的IRQL。

每个中断级别都有具体的用途。例如，内核会发出处理器间中断（Inter-processor

Interrupt，IPI）来请求另一个处理器执行某操作，如调度要执行的特定线程，或者更新自己的转译后备缓冲区（TLB）缓存。系统时钟会以固定间隔生成中断，内核通过更新时钟并衡量线程执行时间作为对此的响应。HAL为中断驱动的设备提供了中断级别，而具体数字取决于处理器和系统配置。内核会使用软件中断（详见本章下文）来发起线程调度，并以异步方式打断线程的执行。

#### 将中断向量映射至IRQL

在非APIC架构的系统中，GSIV/IRQ与IRQL之间的映射必须非常严格。为避免一些情况下中断控制器可能认为某个中断线的优先级比其他中断线更高，在Windows的世界里，IRQL其实会反映一种相反的情况。好在凭借APIC，Windows可以轻松地通过APIC的TPR暴露这些IRQL，这些IRQL随后可被APIC用于做出更完善的交付决策。此外，在APIC系统中，每个硬件中断的优先级并不会绑定至自己的GSIV/IRQ，而是会绑定至中断向量，具体来说，会将向量中较高的4位重新映射为优先级。由于IDT中最多可包含256个项，因此就可以产生16个可能的优先级（例如向量0x40可以代表优先级4），这与TPR可以保存的16个数字相同，这些数字也可以重新映射至Windows所实现的相同的16个IRQL！

因此，Windows为了判断要为某个中断分配哪个IRQL，首先必须判断该中断对应的中断向量，并对IOAPIC进行编程，以便相关的硬件GSIV使用该向量。或者反过来看，如果硬件设备需要某个特定的IRQL，Windows必须选择一个能重新映射至该优先级的中断向量。这些决策是由即插即用管理器与一种名为“总线驱动程序”的设备驱动程序配合做出的，借此可确定总线上所连接的设备（PCI、USB设备等）以及要为每个设备分配的中断。

总线驱动程序会将这些信息上报至即插即用管理器，后者在权衡过所有其他设备可接受的中断分配情况后，决定具体为每个设备分配哪个中断。随后，即插即用管理器会调用一个即插即用中断仲裁程序（Arbiter），借此将中断映射至IRQL。该仲裁程序由HAL（Hardware Abstraction Layer，硬件抽象层）暴露，同时也需要与ACPI总线驱动程序及PCI总线驱动程序配合，共同决定相应的映射关系。大多数情况下，会通过轮询的方式选择最终的向量编号，因此无法通过计算的方式预先得知该编号。本节稍后的一个实验将展示调试器如何通过中断仲裁程序查询这些信息。

除了与硬件中断相关的仲裁中断向量，Windows还有一系列预定义的中断向量（见表8-4），这些向量在IDT中始终具备相同的索引。

表8-4　预定义的中断向量

| 向量        | 用途                |
| --------- | ----------------- |
| 0x1F      | APC中断             |
| 0x2F      | DPC中断             |
| 0x30      | Hypervisor中断      |
| 0x31～0x34 | VMBus中断           |
| 0x35      | CMCI中断            |
| 0xCD      | Thermal中断         |
| 0xCE      | IOMMU中断           |
| 0xCF      | DMA中断             |
| 0xD1      | 时钟计时器中断           |
| 0xD2      | 时钟IPI中断           |
| 0xD3      | 时钟Always on中断     |
| 0xD7      | Reboot中断          |
| 0xD8      | Stub中断            |
| 0xD9      | Test中断            |
| 0xDF      | Spurious中断        |
| 0xE1      | IPI中断             |
| 0xE2      | LAPIC错误中断         |
| 0xE3      | DRS中断             |
| 0xF0      | Watchdog中断        |
| 0xFB      | Hypervisor HPET中断 |
| 0xFD      | Profile中断         |
| 0xFE      | Performance中断     |

通过表8-4可知，这些向量编号的优先级（上文曾经提到，优先级信息存储在较高的4位或半字节（Nibble）中）通常会与图8-14中所示的IRQL保持匹配，例如APC中断为1，DPC中断为2，IPI中断为14，Profile中断为15。关于这个话题，下面一起看看在现代Windows系统中这些预定义的IRQL分别是什么。

#### 预定义的IRQL

接下来一起详细看看这些预定义的IRQL的使用，首先从图8-13中所示的最高级别开始：

● 通常只有在内核将系统停止于KeBugCheckEx状态并对所有中断进行屏蔽（masking out）或连接了远程内核调试器的情况下，才会使用高级别。在非x86系统中，Profile级别共享了相同的值，在启用该功能的情况下，Profile计时器也是在该级别下运行的。Performance中断（与如Intel Processor Trace，即Intel PT及其他硬件性能监视单元，即PMU功能有关）也运行在该级别下。

● Interprocessor interrupt级别可用于请求另一个处理器执行某个操作，如更新处理器的TLB缓存或修改所有处理器的控制寄存器。Deferred Recovery Service（DRS）级别也共享了相同的值，在x64系统中，Windows Hardware Error Architecture（WHEA，Windows硬件错误架构）会使用该级别从某些机器检查错误（Machine Check Errors，MCE）中恢复。

● Clock级别被系统时钟所使用，内核可借此跟踪时间，并为线程衡量和分配CPU时间。

● Synchronization IRQL供调度程序和调度器代码内部使用，借此保护全局线程调度和等待/同步代码的访问过程。通常，该级别会被定义为Device IRQL之下最高的级别。

● Device IRQL可用于对设备中断划分优先级（有关硬件中断级别映射至IRQL的具体方法请参阅上一节）。

● 当CPU或固件通过机器检查错误（MCE）接口上报了严重但已纠正的硬件状况后，可通过Corrected machine check interrupt级别向操作系统发出信号。

● DPC/Dispatch级别和APC级别的中断是内核与设备驱动程序生成的软件中断（下文将详细介绍DPC和APC）。

● Passive级别是最低的IRQL，严格来说，该级别并非真正的中断级别，而是一种设置，常规线程可在该设置下执行并产生所有其他中断。

#### 中断对象

内核提供了一种可移植机制（一种名为中断对象的内核控制对象，即KINTERRUPT），设备驱动程序可借此为自己的设备注册ISR。中断对象包含了内核将设备ISR关联至特定硬件中断所需的全部信息，如ISR的地址、中断的极性（polarity）和触发器模式、设备中断所处的IRQL、共享状态、GSIV和其他中断控制器数据，以及性能统计信息的主机。

这些中断对象是从一个通用内存池分配的，当设备驱动程序（通过IoConnectInterrupt或IoConnectInterruptEx）注册中断时，其中一个中断对象会被初始化所有的必要信息。基于有资格接收该中断（由设备驱动程序指定的中断相关性决定该资格）的处理器编号，每个有资格的处理器将会分配到一个KINTERRUPT对象，通常来说，这包括计算机上的每个处理器。随后，当选择了中断向量后，每个有资格的处理器的KPRCB中会有一个数组（名为InterruptObject）被更新，借此即可指向专为该处理器分配的KINTERRUPT对象。

KINTERRUPT分配完成后，系统会检查和验证所选中断向量是否为可共享的向量；如果可共享，还会检查是否有现有的KINTERRUPT已经声明了该向量。如果已声明，内核会更新（KINTERRUPT数据结构的）DispatchAddress字段，使其指向KiChainedDispatch函数，并将这个KINTERRUPT添加到第一个已与该向量关联的现有KINTERRUPT所包含的链表（InterruptListEntry）中。但如果是专用向量，则会使用KiInterruptDispatch函数。

中断对象还存储了与中断有关的IRQL，这样KiInterruptDispatch或KiChainedDispatch就可以在调用ISR之前将IRQL提升至正确的级别，并在ISR返回后降低IRQL。这个包含两个步骤的过程是必需的，因为初始调度是通过硬件执行的，因此无法在初始调度上传递指向中断对象或其他参数的指针。

当中断发生时，IDT会指向KiIsrThunk函数的256个副本之一，每个副本都有一个不同的汇编代码行负责推送内核栈上的中断向量（因为该向量并非由处理器提供的），随后调用一个共享的KiIsrLinkage函数执行后续处理工作。此外，按照上文介绍，该函数还会构建相应的陷阱帧，并最终调用存储在KINTERRUPT中的调度地址（上述两个函数之一）。这个函数会读取当前KPRCB的InterruptObject数组以查找KINTERRUPT，并将栈上的中断向量用作索引进而取消对匹配指针的引用。如果KINTERRUPT不存在，那么该中断会被视为非预期中断。根据注册表HKLM\SYSTEM\CurrentControlSet\Control\Session Manager\Kernel键下BugCheckUnexpectedInterrupts的值，系统可能会因KeBugCheckEx而崩溃，或者中断会被悄然忽略，执行过程会恢复至原始控制点。

在x64 Windows系统中，内核会使用特定例程来优化中断调度，这些例程通过忽略不需要的功能进而节省处理器运行周期。例如为没有关联内核管理自旋锁的中断（此类中断通常被希望与ISR保持同步的驱动程序所用）使用KiInterruptDispatchNoLock例程，为不希望使用ETW性能跟踪的中断使用KiInterruptDispatchNoLockNoEtw例程，为激活之后无须发送“中断终止”信号的虚假中断使用KiSpuriousDispatchNoEOI例程。

最后，还可以为将APIC设置为Auto-End-of-Interrupt（Auto-EOI）模式的中断使用KiInterruptDispatchNoEOI例程，因为中断控制器会自动发送EOI信号，内核无须额外的代码来亲自执行EOI。例如，很多HAL中断例程会利用“无锁”调度代码，因为HAL无须内核与自己的ISR保持同步。

另一个内核中断处理程序是KiFloatingDispatch，它可用于需要保存浮点状态的中断。内核模式代码通常不允许使用浮点（MMX、SSE、3DNow!）操作，因为这些寄存器无法跨越上下文切换过程保存，ISR可能需要使用这些寄存器（例如显卡ISR执行快速绘图操作）。连接中断时，驱动程序可将FloatingSave参数设置为TRUE，进而请求内核使用浮点调度例程来保存浮点寄存器（但这会大幅增加中断延迟）。请注意，仅32位系统支持此做法。

无论使用哪个调度例程，最终都需要调用KINTERRUPT中的ServiceRoutine字段，这里存储了驱动程序的ISR。或者对于下文即将介绍的消息信号中断（Message Signaled Interrupt，MSI），作为指向KiInterruptMessageDispatch的指针，随后可由该中断调用KINTERRUPT中的MessageServiceRoutine指针。请注意，在某些情况下，例如处理内核模式驱动程序框架（Kernel Mode Driver Framework，KMDF）驱动程序或处理基于NDIS或StorPort等某些微型端口（Miniport）驱动程序时（有关驱动程序框架的详情请参阅本书卷1第6章），可能需要用到这些框架或端口驱动程序特定的例程，由这些例程在最终调用底层驱动程序之前执行进一步的处理工作。

图8-15展示了与中断对象有关的中断所包含的典型中断控制流。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1363.png)

图8-15　典型的中断控制流

将ISR与特定中断级别进行关联的过程也称“连接中断对象”，而将ISR与IDT分离的过程称为“断开中断对象”。这些操作需要通过调用内核函数IoConnectInterruptEx和IoDisconnectInterruptEx来完成，可供设备驱动程序在载入系统时“开启”ISR，并在卸载驱动程序时“关闭”ISR。

如上文所述，使用中断对象注册ISR可防止设备驱动程序无谓地直接与中断硬件交互（具体方式因不同处理器架构而异），并且无须了解有关IDT的任何细节。内核的这一功能有助于开发可移植的设备驱动程序，因为该功能使得我们无须使用汇编语言开发驱动程序代码，也无须在设备驱动程序代码中考虑不同处理器的差异。中断对象还提供了其他好处。通过使用中断对象，内核可将ISR的执行过程与设备驱动程序中可能需要与ISR共享数据的其他部分保持同步（有关设备驱动程序如何响应中断的详细信息请参阅本书卷1第6章）。

上文还提到了链式调度（chained dispatch）的概念，该功能使得内核能够非常轻松地为任何中断级别调用多个ISR。如果多个设备驱动程序创建了中断对象并将其连接到同一个IDT项，在特定中断线发生中断后，KiChainedDispatch例程可调用每一个ISR。借此内核即可轻松地支持菊花链式（daisy-chain）配置，让多个设备共享同一个中断线。当任何一个ISR向中断调度程序返回一种状态，借此声明中断的所有权后，这种链便会断开。

如果共享同一个中断的多个设备同时需要服务，那么无法通过ISR确认的设备会在中断调度程序降低IRQL后再次中断系统。只有在所有希望使用同一个中断的设备驱动程序告知内核自己可以共享中断（这种情况可由KINTERRUPT对象中的ShareVector字段来代表）的情况下，才允许创建链式配置；如果无法共享中断，即插即用管理器会重新调整它们的中断分配情况，以保证中断的分配符合每个驱动程序有关共享的要求。

实验：查看中断的内部机理

我们可以通过内核调试器查看中断对象的内部细节，包括其IRQL、ISR地址以及自定义中断分发代码。首先请执行调试器命令!idt以检查能否找到一个引用了I8042KeyboardInterruptService的项，这是适用于PS2键盘设备的ISR例程。此外，也可以查看指向Stornvme.sys或Scsiport.sys，或者指向我们可识别的其他任何第三方驱动程序的项。在Hyper-V虚拟机中则可以直接使用Acpi.sys项。具备PS2键盘设备项的系统会显示如下结果：

```
70:    fffff8045675a600 i8042prt!I8042KeyboardInterruptService (KINTERRUPT ffff8e01cbe3b280)
```

运行dt命令后，可以直接点击调试器提供的链接查看该中断所关联的中断对象的内容，或者也可以手动使用**dx**命令查看。本次实验中所用计算机上的KINTERRUPT内容如下所示：

```
6: kd> dt nt!_KINTERRUPT ffff8e01cbe3b280 
   +0x000 Type             : 0n22 
   +0x002 Size             : 0n256 
   +0x008 InterruptListEntry : _LIST_ENTRY [ 0x00000000`00000000 - 0x00000000`00000000 ]
   +0x018 ServiceRoutine   : 0xfffff804`65e56820 
                             unsigned char i8042prt!I8042KeyboardInterruptService
   +0x020 MessageServiceRoutine : (null) 
   +0x028 MessageIndex     : 0 
   +0x030 ServiceContext   : 0xffffe50f`9dfe9040 Void 
   +0x038 SpinLock         : 0 
   +0x040 TickCount        : 0 
   +0x048 ActualLock       : 0xffffe50f`9dfe91a0 -> 0 
   +0x050 DispatchAddress  : 0xfffff804`565ca320 void nt!KiInterruptDispatch+0
   +0x058 Vector           : 0x70 
   +0x05c Irql             : 0x7 '' 
   +0x05d SynchronizeIrql  : 0x7 '' 
   +0x05e FloatingSave     : 0 '' 
   +0x05f Connected        : 0x1 '' 
   +0x060 Number           : 6 
   +0x064 ShareVector      : 0 '' 
   +0x065 EmulateActiveBoth : 0 '' 
   +0x066 ActiveCount      : 0 
   +0x068 InternalState    : 0n4 
   +0x06c Mode             : 1 ( Latched ) 
   +0x070 Polarity         : 0 ( InterruptPolarityUnknown ) 
   +0x074 ServiceCount     : 0 
   +0x078 DispatchCount    : 0 
   +0x080 PassiveEvent     : (null) 
   +0x088 TrapFrame        : (null) 
   +0x090 DisconnectData   : (null) 
   +0x098 ServiceThread    : (null) 
   +0x0a0 ConnectionData   : 0xffffe50f`9db3bd90 _INTERRUPT_CONNECTION_DATA 
   +0x0a8 IntTrackEntry    : 0xffffe50f`9d091d90 Void 
   +0x0b0 IsrDpcStats      : _ISRDPCSTATS 
   +0x0f0 RedirectObject   : (null) 
   +0x0f8 Padding          : [8] "" 
```

本例中，Windows为该中断分配的IRQL为7，这与中断向量0x70是一致的（该向量的高4位为7）。此外，我们可从DispatchAddress字段中看到这是一个常规的KiInterruptDispatch样式中断，不包含额外优化或共享。

如果想查看该中断关联了哪个GSIV（IRQ），此时可通过两种方式实现。首先，新版Windows会将该数据以INTERRUPT_CONNECTION_DATA结构嵌入KINTERRUPT的ConnectionData字段，具体情况可参阅上一个命令的输出结果。此外，我们也可以使用**dt**命令从自己的系统中转储指针，方法如下：

```
6: kd> dt 0xffffe50f`9db3bd90 _INTERRUPT_CONNECTION_DATA Vectors[0].. 
nt!_INTERRUPT_CONNECTION_DATA 
   +0x008 Vectors      : [0] 
      +0x000 Type         : 0 ( InterruptTypeControllerInput ) 
      +0x004 Vector       : 0x70 
      +0x008 Irql         : 0x7 '' 
      +0x00c Polarity     : 1 ( InterruptActiveHigh ) 
      +0x010 Mode         : 1 ( Latched ) 
      +0x018 TargetProcessors : 
      +0x000 Mask         : 0xff 
      +0x008 Group        : 0 
      +0x00a Reserved     : [3] 0 
   +0x028 IntRemapInfo : 
      +0x000 IrtIndex     : 0y000000000000000000000000000000 (0) 
      +0x000 FlagHalInternal : 0y0 
      +0x000 FlagTranslated : 0y0 
      +0x004 u            : <anonymous-tag> 
   +0x038 ControllerInput : 
      +0x000 Gsiv         : 1 
```

上述输出结果中的Type表明，这是一个传统的、基于线/控制器的输入，而Vector和Irql字段确认了前一个实验中我们已经在KINTERRUPT中看到的数据。随后通过查看ControllerInput结构，我们可以看到GSIV为1（即IRQ 1）。如果查看的是不同类型的中断（如消息信号中断，详见下文），则应取消对MessageRequest字段的引用。

我们还可以通过另一种方法将GSIV映射至中断向量：当通过所谓的仲裁程序管理设备资源时，Windows会持续跟踪整个过程。对于每一类资源，可通过仲裁程序维持虚拟资源的使用情况（如中断向量）和物理资源（如中断线）之间的关系。因此我们可以查询ACPI IRQ仲裁程序并获得相关映射关系。为此可使用**!apciirqarb**命令获取有关ACPI IRQ仲裁程序的信息：

```
6: kd> !acpiirqarb 
　
Processor 0 (0, 0): 
Device Object: 0000000000000000 
Current IDT Allocation: 
... 
  000000070 - 00000070 D ffffe50f9959baf0 (i8042prt) A:ffffce0717950280 IRQ(GSIV):1
... 
```

请注意，键盘的GSIV为IRQ 1，这是一个古老的遗留数值，甚至可以从今天一直追溯至IBM PC/AT时代。我们也可以使用**!arbiter 4**（“4”可以让调试器只显示IRQ仲裁程序）查看ACPI IRQ仲裁程序内部包含的项：

```
6: kd> !arbiter 4 
　
DEVNODE ffffe50f97445c70 (ACPI_HAL\PNP0C08\0) 
  Interrupt Arbiter "ACPI_IRQ" at fffff804575415a0 
    Allocated ranges: 
      0000000000000001 - 0000000000000001 ffffe50f9959baf0 (i8042prt) 
```

本例中要注意，上述范围代表了GSIV（IRQ）而非中断向量。此外要注意，上述这些输出结果中我们都可以看到向量的所有信息，这是以设备对象的类型来表示的（本例中为0xFFFFE50F9959BAF0）。随后即可使用**!devobj**命令查看本例中i8042prt设备（对应着PS/2驱动程序）的相关信息：

```
6: kd> !devobj 0xFFFFE50F9959BAF0 
Device object (ffffe50f9959baf0) is for: 
 00000049 \Driver\ACPI DriverObject ffffe50f974356f0 
Current Irp 00000000 RefCount 1 Type 00000032 flags 00001040 
SecurityDescriptor ffffce0711ebf3e0 DevExt ffffe50f995573f0 DevObjExt ffffe50f9959bc40
DevNode ffffe50f9959e670 
Extensionflags (0x00000800) DOE_DEFAULT_SD_PRESENT 
Characteristics (0x00000080) FILE_AUTOGENERATED_DEVICE_NAME 
AttachedDevice (Upper) ffffe50f9dfe9040 \Driver\i8042prt 
Device queue is not busy. 
```

该设备对象关联了一个设备节点，其中存储了该设备的所有物理资源。至此我们已

经可以使用**!devnode**命令转储这些资源，并使用0xF标记同时查看原始数据和转换后的资源信息：

```
6: kd> !devnode ffffe50f9959e670 f 
DevNode 0xffffe50f9959e670 for PDO 0xffffe50f9959baf0 
  InstancePath is "ACPI\LEN0071\4&36899b7b&0" 
  ServiceName is "i8042prt" 
  TargetDeviceNotify List - f 0xffffce0717307b20 b 0xffffce0717307b20 
  State = DeviceNodeStarted (0x308) 
  Previous State = DeviceNodeEnumerateCompletion (0x30d) 
  CmResourceList at 0xffffce0713518330 Version 1.1 Interface 0xf Bus #0 
    Entry 0 - Port (0x1) Device Exclusive (0x1) 
      Flags (PORT_MEMORY PORT_IO 16_BIT_DECODE 
      Range starts at 0x60 for 0x1 bytes 
    Entry 1 - Port (0x1) Device Exclusive (0x1) 
      Flags (PORT_MEMORY PORT_IO 16_BIT_DECODE 
      Range starts at 0x64 for 0x1 bytes 
    Entry 2 - Interrupt (0x2) Device Exclusive (0x1) 
      Flags (LATCHED 
      Level 0x1, Vector 0x1, Group 0, Affinity 0xffffffff 
... 
  TranslatedResourceList at 0xffffce0713517bb0 Version 1.1 Interface 0xf Bus #0
    Entry 0 - Port (0x1) Device Exclusive (0x1) 
      Flags (PORT_MEMORY PORT_IO 16_BIT_DECODE 
      Range starts at 0x60 for 0x1 bytes 
    Entry 1 - Port (0x1) Device Exclusive (0x1) 
      Flags (PORT_MEMORY PORT_IO 16_BIT_DECODE 
      Range starts at 0x64 for 0x1 bytes 
    Entry 2 - Interrupt (0x2) Device Exclusive (0x1) 
      Flags (LATCHED 
      Level 0x7, Vector 0x70, Group 0, Affinity 0xff 
```

通过设备节点可知，该设备有一个包含三项内容的资源列表，其中一项为对应于IRQ 1的中断项（级别和向量编号代表了GSIV而非中断向量）。从后续显示的转换后的资源列表可知IRQL为7（这是级别编号），而中断向量为0x70。

在ACPI系统中，我们可以通过一种更简单的方式获取此类信息，为此可查看上述**!acpiirqarb**命令的扩展输出结果。该输出结果还会显示IRQ与IDT之间的映射表：

```
Interrupt Controller (Inputs: 0x0-0x77): 
    (01)Cur:IDT-70 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
    (02)Cur:IDT-80 Ref-1 Boot-1 edg hi    Pos:IDT-00 Ref-0 Boot-1 lev unk 
    (08)Cur:IDT-90 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk 
    (09)Cur:IDT-b0 Ref-1 Boot-0 lev hi    Pos:IDT-00 Ref-0 Boot-0 lev unk
    (0e)Cur:IDT-a0 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (10)Cur:IDT-b5 Ref-2 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (11)Cur:IDT-a5 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (12)Cur:IDT-95 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (14)Cur:IDT-64 Ref-2 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (17)Cur:IDT-54 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (1f)Cur:IDT-a6 Ref-1 Boot-0 lev low   Pos:IDT-00 Ref-0 Boot-0 lev unk
    (41)Cur:IDT-96 Ref-1 Boot-0 edg hi    Pos:IDT-00 Ref-0 Boot-0 lev unk
```

不出所料，IRQ 1关联给了IDT项0x70。有关设备对象、资源以及相关概念的详细信息，请参阅卷1第6章。

### 8.4.2　基于线的中断和基于消息信号的中断

共享的中断经常会导致较高的中断延迟，甚至可能导致稳定性问题。对于物理中断线路（interrupt line）有限的计算机，这是一种需要尽力避免的副作用。例如，对于能同时支持USB、Compact Flash存储卡、Sony Memory Stick记忆棒、Secure Digital存储卡以及其他介质的多合一读卡器，同一个物理设备中包含的所有控制器通常都会连接到同一个中断线，随后被不同设备驱动程序配置为共享的中断向量。这会导致延迟增加，因为需要按顺序轮流调用每个驱动程序才能确定为该媒体设备发出中断的实际控制器。

更好的解决方案是让每个设备控制器使用自己的中断，并通过同一个驱动程序管理不同的中断，以此得知这些中断来自哪个设备。然而，为一个设备使用四个传统的IRQ线会很快导致IRQ线耗尽。此外无论如何，每个PCI设备都只能连接到一个IRQ线，因此，对于上述那样的多媒体读卡器，即使需要，也无法使用超过一个的IRQ。

通过IRQ线生成中断的另一个问题在于，如果无法正确管理IRQ信号，可能会导致计算机遇到中断风暴或其他类型的死锁，因为在ISR确认信号之前，信号需要处于“高”或“低”的状态（此外，中断控制器通常必须收到EOI信号）。如果由于存在Bug而无法实现上述操作，系统将永久陷入中断状态，后续中断将无法被屏蔽，甚至同时出现这两种情况。最后，基于线的中断在多处理器环境中的可扩展性有限。很多情况下，当即插即用管理器为一个中断选择了一组处理器后，最终将由硬件决定要中断哪个处理器，设备驱动程序在其中起到的作用极为有限。

为解决上述所有问题，PCI 2.2标准中首次引入了一种名为消息信号中断（Message- Signaled Interrupt，MSI）的机制。虽然这是该标准的一种可选组件，并且很少出现在客户端计算机（主要被服务器用于改善网卡和存储控制器性能）中，但随着PCI Express 3.0和后续标准的普及，大部分现代操作系统已经可以全面支持这种模型。在MSI的世界里，设备可以通过PCI总线对一个特定内存地址执行写入操作，以此向自己的驱动程序传递消息。从硬件的角度来看，实际上这可以视为一种直接内存访问（Direct Memory Access，DMA）操作。该操作会产生一个中断，随后Windows即可使用消息内容（值）和消息传递到的地址来调用ISR。设备还可以向内存地址传递多个消息（最多32个），以此根据不同事件传递不同的消息载荷。

对于某些要求更高性能和更低延迟的系统，PCI 3.0标准引入了MSI-X技术，这是对原有MSI模型的扩展，该技术可支持32位（而不再是16位）的消息，最多可支持2048个（不再是仅仅32个）不同的消息，更重要的是，该技术可以为每个MSI载荷使用不同的地址（地址可动态确定）。不同地址的使用使得MSI载荷可以被写入属于不同处理器的不同物理地址范围，或写入不同的目标处理器集，这种方式高效地实现了通过非一致内存访问（Nonuniform Memory Access，NUMA）来感知中断交付，进而可将中断发送给最初发起相关硬件请求的处理器。通过在中断完成过程中监视负载和距离最近的NUMA节点，该技术可以大幅改善延迟与可扩展性。

在上述这些模型中，因为要基于内存值进行通信，并且因为内容是与中断一起交付的，因此可以不再需要IRQ线（进而使得系统对于MSI整体限于中断向量的数量，而非IRQ线的数量），而是需要通过驱动程序ISR向设备查询与中断有关的数据，进而降低了延迟。由于该模型可提供大量设备中断，也使得共享中断的必要性显著降低，进而通过将中断数据直接交付给相关ISR而进一步降低了延迟。

也正因如此，我们可以看到大部分调试器命令会使用GSIV这个术语来替代IRQ，因为GSIV可以概括地描述MSI向量（由不同的“负数”进行区分）、传统的基于IRQ的线，甚至嵌入式设备中的通用输入/输出（General Purpose Input Output，GPIO）引脚。此外，ARM和ARM64系统并未使用上述任何一种模型，而是使用了通用中断控制器（Generic Interrupt Controller，GIC）架构。从图8-16中可以看到两个计算机系统中的设备管理器，其中分别显示了传统的基于IRQ的GSIV，以及以负数形式显示的MSI值的分配情况。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1371.png)

图8-16　基于IRQ和MSI的GSIV分配情况

#### 中断路由控制

在非虚拟化环境中运行且一个处理器组包含2～16个处理器的客户端（即非服务器SKU）系统中，Windows会通过一种名为中断路由控制（Interrupt Steering）的功能满足消费级现代操作系统对能耗和延迟的需求。在该功能的帮助下，可以按需将中断的负载分摊到多个处理器，以避免单一CPU可能造成的瓶颈，而内核休止引擎（Core Parking Engine，详见本书卷1第6章）亦可将中断路由至未休止的内核，以避免大量中断的分配导致太多处理器在同一时间处于被唤醒的状态。

中断路由控制的具体功能取决于中断控制器。例如，在支持GIC的ARM系统中，所有等级敏感的以及边缘（锁存）触发的中断均可进行路由控制；而在APIC系统（除非在Hyper-V中运行）中，仅等级敏感的中断可进行路由控制。然而，由于MSI始终是等级边缘触发（level edge-triggered）的，所以会导致该技术提供的收益大幅降低，为应对这种情况，Windows还实现了另一种中断重定向模型。

在启用路由控制后，中断控制器通过重编程将GSIV交付给不同处理器的LAPIC（在ARM GIC环境中也会实现类似的交付机制）。在必须进行重定向的情况下，所有处理器都会成为GSIV的交付目标，随后实际收到该中断的处理器需要手动向该中断原本应该路由到的目标处理器发送一个IPI。

除了内核休止引擎所使用的中断路由控制，Windows还会通过系统信息类暴露这些功能，该信息类会由KeIntSteerAssignCpuSetForGsiv通过Windows 10的实时音频功能和CPU集（CPU Set）功能进行处理，详见本书卷1第4章。由此特定GSIV即可路由至能够被用户模式应用程序选择的特定处理器组，但前提是应用程序需具备Increase Base Priority权限，通常只有管理员或本地服务账户具备该权限。

#### 中断的相关性和优先级

Windows允许驱动程序开发者和管理员在一定程度上控制处理器相关性（选择接收中断的处理器或处理器组）和相关性策略（决定处理器的选择方式以及要选择处理器组中的哪个处理器）。此外，Windows还能根据IRQL的选择情况实现一种用于为中断划分优先级的基元机制。相关性策略的定义如表8-5所示，这些策略可通过设备实例的注册表键中Interrupt Management\Affinity Policy子键下一个名为InterruptPolicyValue的注册表值加以控制。因此管理员无须配置任何代码，即可将该值添加到特定驱动程序的注册表键中，进而改变其行为。有关中断相关性的详细介绍可参阅微软文档：https://docs.microsoft.com/ windows-hardware/drivers/kernel/interrupt-affinity-and-priority。

表8-5　IRQ相关性策略

| 名称                                         | 值                                                                                                |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------ |
| IrqPolicyMachineDefault                    | 该设备无需特定相关性策略。Windows将使用默认的计算机策略，即选择计算机（逻辑处理器不超过8个的计算机）上任何可用处理器                                   |
| IrqPolicyAllCloseProcessors                | 在NUMA计算机中，即插即用管理器会将中断分配给靠近设备（位于同一个节点中）的所有处理器；在非NUMA计算机中，将使用与IrqPolicyAllProcessorsInMachine相同的行为 |
| IrqPolicyOneCloseProcessor                 | 在NUMA计算机中，即插即用管理器会将中断分配给靠近设备（位于同一个节点中）的一个处理器；在非NUMA计算机中，将选择系统中任何一个可用处理器                          |
| IrqPolicyAllProcessorsInMachine            | 中断将由计算机中任何可用处理器处理                                                                                |
| IrqPolicySpecifiedProcessors               | 中断仅由AssignmentSetOverride注册表值下的相关性掩码指定的处理器处理                                                     |
| IrqPolicySpreadMessagesAcrossAllProcessors | 不同的消息信号中断将分散到有资格的处理器所组成的最佳处理器集中，并在可能的情况下尽量跟踪NUMA拓扑问题。该策略需要设备和平台支持MSI-X                           |
| IrqPolicyAllProcessorsInGroupWhenSteered   | 中断完全由中断路由控制机制进行控制，因此中断会分配给所有处理器IDT，并根据路由控制规则动态选择目标处理器                                            |

除了设置上述相关性策略，我们还可以根据表8-6列出的注册表值设置中断的优先级。

表8-6　IRQ优先级

| 名称                   | 值                                        |
| -------------------- | ---------------------------------------- |
| IrqPriorityUndefined | 该设备无需特定优先级。此时将获得默认优先级（IrqPriorityNormal） |
| IrqPriorityLow       | 该设备可容忍高延迟，因此可获得低于常规的IRQL（3或4）            |
| IrqPriorityNormal    | 该设备可获得平均延迟，因此可获得与其中断向量相关的默认IRQL（5或11）    |
| IrqPriorityHigh      | 该设备需要尽可能降低延迟，因此可获得超出正常情况的高IRQL（12）       |

我们需要意识到Windows并非实时操作系统，因此这些IRQ优先级仅仅是提供给系统的一种“暗示”，只能用于控制与中断有关的IRQL，无法提供Windows IRQL优先级方案机制之外的其他优先级。由于IRQ优先级也存储在注册表中，因此管理员可以自由地为未利用此功能的驱动程序更改相关注册表值，以便有更低的延迟。

#### 软件中断

虽然大部分中断是硬件生成的，但Windows内核也能为很多任务生成软件中断，这些任务包括：

● 初始化线程调度。

● 处理非时间关键型中断。

● 处理计时器过期。

● 在特定线程的上下文中以异步方式执行过程。

● 为异步I/O操作提供支持。

下面将详细介绍这些任务。

#### 调度或延迟过程调用（DPC）中断

DPC通常是一种与中断有关的功能，会在所有设备中断处理完毕后执行某种处理任务。该功能名称中的“延迟”是指相关任务也许不会立即执行。内核会使用DPC处理计时器过期（并释放等待该计时器的线程）并在线程的量程过期后重新调度处理器（这一过程发生在DPC IRQL下，但其实并非通过常规内核DPC进行的）。设备驱动程序可使用DPC处理中断并执行更高IRQL下不可用的操作。为了向硬件中断提供及时的服务，Windows会在设备驱动程序的配合下尝试保持该IRQL低于设备的IRQL级别。实现这一目标的方法之一是让设备驱动程序ISR仅执行确认设备所需的最少量必要工作，保存可变的中断状态，并将数据传输工作或对时间要求不敏感的中断处理工作延迟到在DPC/Dispatch IRQL下通过DPC来执行（有关I/O系统的详细信息请参阅本书卷1第6章）。

如果IRQL为被动模式或处于APC级别，DPC将立即执行并阻止所有其他非硬件相关的处理任务，因此该机制通常也用于强制立即执行高优先级的系统代码。借此，DPC为操作系统提供了生成中断并在内核模式下执行系统函数的能力。例如，当一个线程无法继续执行时（也许因为该线程已终止或自愿进入等待状态），内核会直接调用调度程序来立即执行上下文切换。然而，有时候内核会检测到自己深陷于多层代码中，进而需要重新调度。此时内核会请求进行调度，但会延迟调度操作的发生，直到自己完成当前操作。DPC软件中断是实现这种延迟处理目标的一种便利方法。

当内核需要同步访问与调度有关的内核结构时，会始终将处理器的IRQL提升至DPC/ Dispatch级别或更高级别。这会同时禁用其他的软件中断和线程调度。当内核检测到需要进行调度时，会请求一个DPC/Dispatch级别的中断，但由于IRQL已处于或高于该级别，处理器会将该中断置于检查状态。内核完成当前活动后，发现自己需要将IRQL降低至DPC/Dispatch级别以下，并需要检查是否有挂起的调度中断。如果有，则IRQL会降低至DPC/Dispatch级别并开始处理调度中断。使用软件中断激活线程调度程序，是一种在所需条件满足之前进行延迟调度的方法。DPC由DPC对象表示，这是一种对用户模式程序不可见，但对设备驱动程序和其他系统代码可见的内核控制对象。内核在处理DPC中断时所调用系统函数的地址是DPC对象中包含的最重要信息。等待执行的DPC例程会保存在内核管理的队列中，该队列名为DPC队列，每个处理器都有一个这样的队列。若要请求DPC，系统代码会调用内核初始化一个DPC对象，并将其保存在DPC队列中。

默认情况下，内核会将DPC对象放置在请求了该DPC的处理器（通常也是负责执行ISR的处理器）所属的两个DPC队列之一的末尾处。不过设备驱动程序可以重写此行为，为此只需要指定一个DPC优先级（低、中、中高、高，其中“中”为默认优先级）并为该DPC选择一个特定处理器作为目标。针对特定CPU的DPC也称定向DPC（Targeted DPC）。如果DPC优先级为“高”，则内核会将该DPC对象插入队列前方；如果为其他任何优先级，则会置于队列末尾。

当处理器的IRQL即将从DPC/Dispatch级别或更高级别降至更低级别（APC或被动级别）时，内核将开始处理DPC。Windows会保证IRQL依然处于DPC/Dispatch级别，并从当前处理器的队列中持续“取出”DPC对象，直到队列为空（也就是说，内核开始“排空”队列），并会按顺序调用每个DPC函数。只有在队列为空后，内核才会让IRQL降至低于DPC/Dispatch的级别，并让常规线程继续执行。图8-17展示了DPC的处理过程。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1537.png)

图8-17　DPC的处理过程

DPC优先级还会以其他方式对系统行为产生影响。内核通常会使用DPC/Dispatch级别的中断发起DPC队列排空操作。但只有在DPC被当前处理器（执行ISR的处理器）控制且DPC的优先级高于“低”优先级时，内核才会生成此类中断。如果DPC的优先级为“低”，那么只有在该处理器尚未解决的DPC请求数量（存储在KPRCB的DpcQueueDepth字段中）超过某一阈值（在KPRCB中该阈值被称为MaximumDpcQueueDepth）之后，或特定时间窗口内处理器所请求的DPC数量极低的情况下，内核才会请求这样的中断。

如果某个DPC的目标CPU不同于运行ISR的CPU，且该DPC的优先级为“高”或“中高”时，内核就会立即向目标CPU发送信号（发送“调度IPI”）以排空其DPC队列，但前提是目标处理器必须为空闲状态。如果优先级为“中”或“低”，那么目标处理器DPC队列中的请求数量（依然是DpcQueueDepth）必须超过内核触发DPC/Dispatch中断的阈值（MaximumDpcQueueDepth）。系统闲置线程也可以排空它所运行的处理器的DPC队列。虽然DPC目标和优先级机制非常灵活，但设备驱动程序很少需要更改自己DPC对象的默认行为。表8-7总结了可以发起DPC队列排空的各种情况。从生成规则的角度来看，“中高”和“高”优先级其实是等同的，它们之间的差异在于插入队列的位置，“高”优先级中断会被插入头部，“中高”优先级中断会被插入尾部。

表8-7　DPC中断生成规则

| DPC优先级 | 以ISR的处理器为目标的DPC                        | 以其他处理器为目标的DPC               |
| ------ | -------------------------------------- | --------------------------- |
| 低      | DPC队列长度超过DPC队列最大长度，或DPC请求速率低于DPC请求最小速率 | DPC队列长度超过DPC队列最大长度，或系统为空闲状态 |
| 中      | 始终                                     | DPC队列长度超过DPC队列最大长度，或系统为空闲状态 |
| 中高     | 始终                                     | 目标处理器为空闲状态                  |
| 高      | 始终                                     | 目标处理器为空闲状态                  |

另外，表8-8描述了各种DPC中断生成变量及其默认值，以及该如何通过注册表修改这些值。除了注册表，我们也可以通过SystemDpcBehaviorInformation这个系统信息类来设置这些值。

表8-8　DPC中断生成变量及其默认值

| 变量                     | 定义                                                | 默认值 | 覆盖值                |
| ---------------------- | ------------------------------------------------- | --- | ------------------ |
| KiMaximumDpcQueueDepth | 发出中断前可加入队列的DPC数量（即便是“中”和更低优先级的DPC）                | 4   | DpcQueueDepth      |
| KiMinimumDpcRate       | “低”优先级DPC不导致生成本地中断的前提下，处理器每个时钟周期可处理的DPC数量         | 3   | MinimumDpcRate     |
| KiIdealDpcRate         | 如果DPC已挂起但未生成中断，在DPC队列深度最大值被减小前，处理器每个时钟周期可处理的DPC数量 | 20  | IdealDpcRate       |
| KiAdjustDpcThreshold   | 如果DPC未挂起，在DPC队列深度最大值被增大前，可处理的处理器时钟周期数量            | 20  | AdjustDpcThreshold |

由于用户模式线程以低IRQL执行，DPC在很多时候会中断常规用户线程的执行。DPC例程的执行并不考虑哪些线程正在运行，这意味着当DPC例程运行时，并不能假定当前已经映射了哪些进程地址空间。DPC例程可以调用内核函数，但无法调用系统服务，无法生成页面错误，也无法创建或等待调度程序对象（下文将详细介绍）。不过DPC例程可以访问未分页系统内存地址，因为无论当前进程是哪个，系统地址空间始终会被映射。

由于所有用户模式内存都可分页且DPC会在任意进程上下文中执行，DPC代码永远不能以任何方式访问用户模式内存。在支持管理模式访问保护（Supervisor Mode Access Protection，SMAP）或永无特权访问（Privileged Access Never，PAN）的系统中，Windows会在处理DPC队列（及执行例程）的过程中激活这些功能，保证访问用户模式内存的任何操作均会立即导致Bugcheck错误。

DPC中断线程执行带来的另一个副作用是最终会导致线程的运行时间被“窃取”。因为当调度器认为当前线程正在执行时，实际上执行的可能是DPC。卷1第4章中讨论过调度器会通过一些机制跟踪线程运行所消耗的CPU时钟周期准确数量，并在必要时扣除DPC和ISR时间，以此为线程失去的运行时间做出补偿。

虽然这保证了线程不会牺牲自己的量程作为代价，但仍意味着，从用户的角度来看，钟表时间（也就是现实世界中流逝的时间）依然用于处理其他事情了。假设用户正在通过在线音乐服务听自己喜欢的歌曲，如果DPC运行耗时2秒，在这2秒时间里，音乐可能会卡顿或重复播放一小段相同内容。在线视频流媒体甚至键盘鼠标的输入也可能会受到类似影响。因此，对于客户端系统或工作站工作负载来说，DPC已成为导致很多可察觉系统卡顿问题的主要原因，即使是最高优先级的线程，也可能被DPC的运行所打断。为了让某些包含需长时间运行DPC的驱动程序能够正确实现，Windows开始支持线程式DPC（Threaded DPC）。顾名思义，线程式DPC可以在实时优先级（优先级31）的线程上以被动模式执行DPC例程，这样DPC就可以抢占大部分用户模式线程（因为大部分应用程序线程并不在实时优先级的范围内运行），但同时又允许其他中断、非线程式DPC、APC以及其他优先级为31的线程能够抢占这种DPC例程的执行。

线程式DPC默认已经启用，我们可在注册表的HKEY_LOCAL_MACHINE\System\ CurrentControlSet\Control\Session Manager\Kernel键下添加一个名为ThreadDpcEnable的DWORD值，并将其数值设置为“0”，这样即可禁用线程式DPC。线程式DPC必须由开发者通过KeInitializeThreadedDpc API进行初始化，由此可将DPC的内部类型设置为ThreadedDpcObject。由于线程式DPC可以被禁用，所以使用该机制的驱动程序开发者必须按照与非线程式DPC例程相同的规则编写自己的例程，不能访问已分页内存、执行调度程序等待，或者假设执行所用的IRQL级别。此外，此类驱动程序的开发者也不应使用KeAcquire/ReleaseSpinLockAtDpcLevel API，因为相关函数会假设CPU处于调度级别。实际上，线程式DPC必须使用KeAcquire/ReleaseSpinLockForDpc，借此在检查当前IRQL后执行相应操作。

虽然线程式DPC是一项出色的功能，可帮助驱动程序开发者尽可能地保护系统资源，但无论是从开发者还是系统管理员的角度，这都是一项选择性使用的功能。因此大部分DPC依然以非线程式的模式执行，并可能导致上述系统卡顿问题。Windows会使用大量性能跟踪机制诊断并协助解决与DPC有关的问题。第一个问题当然是通过性能计数器和更精确的ETW跟踪机制来跟踪DPC和ISR所消耗的时间。

实验：监视DPC活动

可以使用Process Explorer监视DPC活动，为此请打开“System Information”对话框并切换至CPU选项卡，这里列出了每一次Process Explorer刷新显示结果（默认为1秒）过程中所执行的中断和DPC数量。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1616.png) ![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1630.png)

也可以使用内核调试器查看KPRCB中名称以“Dpc”开头的各种字段，例如，DpcRequestRate、DpcLastCount、DpcTime以及DpcData（其中包含DpcQueueDepth和DpcCount，分别对应非线程式和线程式DPC）。此外，较新版本的Windows还包含IsrDpcStats字段，该字段是一个指向_ISRDPCSTATS结构的指针，这个结构已包含在公开发布的符号文件中。例如，下列命令可显示当前KPRCB中已加入队列的（线程式和非线程式）DPC总数，以及已执行过的DPC数量：

```
lkd> dx new { QueuedDpcCount = @$prcb->DpcData[0].DpcCount + @$prcb->DpcData[1].DpcCount, ExecutedDpcCount = ((nt!_ISRDPCSTATS*)@$prcb->IsrDpcStats)->DpcCount },d
    QueuedDpcCount   : 3370380 
    ExecutedDpcCount : 1766914 [Type: unsigned __int64] 
```

上述范例输出结果中的差异是正常的，驱动程序可能会将已位于队列中的DPC再次加入队列，而Windows可以安全地处理这种情况。此外，最开始DPC可能会被加入特定处理器的队列（但并不以任何具体处理器作为目标），在某些情况下，它可能在另一个处理器上执行，例如，当驱动程序使用KeSetTargetProcessorDpc（该API可以让驱动程序将特定处理器作为DPC目标）时。

Windows不仅可以帮助用户手动调查由DPC导致的延迟问题，还能通过一套内置的机制解决少数导致严重问题的常见场景。首先是DPC Watchdog和DPC Timeout机制，这些机制可通过注册表HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session Manager\Kernel键下的DPCTimeout、DpcWatchdogPeriod以及DpcWatchdogProfileOffset等值进行配置。

如果IRQL降低事件在很长时间里都未注册，DPC Watchdog将负责监视在DISPATCH_ LEVEL或更高级别执行的所有代码。另外，DPC Timeout负责监视特定DPC的执行时间。默认情况下，特定DPC会在大约20秒后超时，而所有DISPATCH_LEVEL（以及更高级别）的执行会在2分钟后超时。这两项限制都可以通过上文提到的注册表值进行配置（DPCTimeout控制了特定DPC的时间限制，而DpcWatchdogPeriod控制了在高IRQL下运行的所有代码的整体执行情况）。当达到这些阈值后，系统可能会发出DPC_WATCHDOG_ VIOLATION的Bugcheck错误（由此可判断到底是哪种情况），如果附加了内核调试器，则会发出一个可以继续运行的断言。

驱动程序开发者如果希望通过自己的工作避免出现这些情况，可以使用KeQueryDpcWatchdogInformation API查看这些注册表当前配置的值以及剩余时间。此外，KeShouldYieldProcessor API也可以将这些值（以及其他与系统状态有关的值）纳入考虑范围，进而为驱动程序返回相关提示信息，供驱动程序决定接下来是否继续处理自己的DPC工作，或是否在可行的情况下将IRQL重新降低至PASSIVE_LEVEL（主要适用于DPC并未执行但驱动程序持有了锁或是通过某种方式与DPC进行同步的情况）。

在最新版本的Windows 10中，每个PRCB还包含一个DPC运行时历史记录表（DpcRuntimeHistoryHashTable），其中保存了一个哈希（或散列）表桶，它由最近执行的特定DPC回调函数及其运行所消耗的CPU周期数量等痕迹信息所组成。在分析内存转储或远程系统时，可在无须借助UI工具的情况下通过这些信息研究延迟问题，但更重要的是，内核也可以使用这些信息。

驱动程序开发者通过KeInsertQueueDpc将DPC插入队列时，该API将枚举处理器的表并检查该DPC之前是否曾执行并耗费了相当长时间（默认为100毫秒，可通过注册表HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session Manager\Kernel下的LongDpcRuntimeThreshold值进行配置）。如果是这种情况，上文提到的DpcData结构中将被设置LongDpcPresent字段。

对于每个闲置线程（有关线程调度和闲置线程的详细信息请参阅本卷1第4章），现在的内核也可以创建DPC委派线程（DPC Delegate Thread）。这是一种具备高度唯一性的线程，隶属于System Idle Process（这一点与闲置线程，即Idle Thread一样）。这种线程永远不会被包含在调度器的默认线程选择算法中，而是在内核中专供内核自己使用。图8-18展示了一个具备16个逻辑处理器、16个闲置线程和16个DPC委派线程的系统。请注意，在这种情况下，这些线程有着真实的线程ID（TID），图中Processor列显示的信息即可视为其TID。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1709.png)

图8-18　具备16个CPU的系统中的DPC委派线程

内核调度DPC时，会检查DPC队列深度是否已超过这种长时间运行DPC的阈值（默认深度为2，可通过上文多次提到的注册表键进行配置）。如果已超出，则需要决定是否通过查看当前执行中线程的属性来缓解这种情况，而具体要研究的属性包括：该线程是否闲置、是否为实时线程，其相关性掩码是否决定了该线程通常需要在不同的处理器上运行。基于结果，内核可能决定调度DPC委派线程作为代替，从本质来看，这等于是将该DPC从运行时间所剩无几的线程切换至一个优先级尽可能高的专用线程中（但依然在DISPATCH_LEVEL级别下执行）。这样原本被抢占的线程（或位于待命列表中的任何其他线程）就有机会重新调度至其他CPU。

该机制与上文提到的线程式DPC类似，但也有些差异。委派线程依然运行在DISPATCH_LEVEL级别下。实际上，当委派线程在NT内核初始化（详见第12章）的阶段1创建并启动时，就会将自己的IRQL提升至DISPATCH级别，保存到自己内核线程数据结构的WaitIrql字段中，并自发地请求调度器对另一个待机或就绪线程进行上下文切换（通过KiSwapThread例程实现）。因此，委派DPC为系统提供了一种自动化均衡操作，并不需要由驱动程序开发者选择性地采用并慎重地应用在自己的代码中。

如果是具备该功能的新版本Windows 10系统，可在内核调试器中运行下列命令来查看对委派线程的需求到底有多频繁，这可以从系统引导后执行的上下文切换次数推断出来：

```
lkd> dx @$cursession.Processes[0].Threads.Where(t => t.KernelObject.ThreadName->
ToDisplayString().Contains("DPC Delegate Thread")).Select(t => t.KernelObject.Tcb.
ContextSwitches),d 
    [44]             : 2138 [Type: unsigned long] 
    [52]             : 4 [Type: unsigned long] 
    [60]             : 11 [Type: unsigned long] 
    [68]             : 6 [Type: unsigned long] 
    [76]             : 13 [Type: unsigned long] 
    [84]             : 3 [Type: unsigned long] 
    [92]             : 16 [Type: unsigned long] 
    [100]            : 19 [Type: unsigned long] 
    [108]            : 2 [Type: unsigned long] 
    [116]            : 1 [Type: unsigned long] 
    [124]            : 2 [Type: unsigned long] 
    [132]            : 2 [Type: unsigned long] 
    [140]            : 3 [Type: unsigned long] 
    [148]            : 2 [Type: unsigned long] 
    [156]            : 1 [Type: unsigned long] 
    [164]            : 1 [Type: unsigned long] 
```

#### 异步过程调用中断

异步过程调用（Asynchronous Procedure Call，APC）为用户程序和系统代码提供了一种在特定用户线程的上下文（进而在特定进程地址空间）中执行的方法。由于APC需要在特定用户线程的上下文中排队执行，因此也会受制于线程调度规则，无法在与DPC相同的环境中运行。也就是说，APC无法在DISPATCH_LEVEL下运行，可能会被更高优先级的线程抢占，可以执行阻塞等待，可以访问可分页的内存。

话虽如此，但由于APC依然是一种软件中断，因此必须以某种方式从线程的主执行路径“夺取”控制权，本节将会介绍这是通过在名为APC_LEVEL的IRQL上操作实现的。这意味着尽管APC的运行不像DPC那样会遇到相同限制，但开发者依然需要遵守某些规则，下文还将详细介绍这一点。

APC是由一个名为APC对象的内核控制对象描述的。待执行的APC在内核管理的两个APC队列中等待。这与DPC队列不同，DPC队列是每个处理器专用的（并会分为线程的和非线程的），而APC队列是每个线程的，每个线程有两个APC队列：一个适用于内核APC，另一个适用于用户APC。

在需要将APC加入队列时，内核会查看APC的模式（用户或线程），随后将APC加入执行该APC例程的线程所属的相应队列。在介绍该APC如何以及何时执行之前，我们先来看看两种模式之间的差异。当APC被加入线程队列时，该线程可能处于下列三种情况之一：

● 线程当前正在运行（甚至可能就是当前线程）。

● 线程当前正在等待。

● 线程正在执行其他操作（就绪、准备等）。

首先请回忆卷1第4章的内容，执行等待的线程具备一个可告警的状态。除非针对某个线程彻底禁用了APC，否则对于内核APC，该状态会被忽略，也就是说，APC总是会终止等待，而这一行为的结果会在下文进一步讨论。不过对于用户APC，只有在等待操作是可告警的并且代表某个用户模式组件进行了实例化，或者其他正在挂起的用户APC已经开始终止该等待（如果有大量处理器试图将APC加入同一个线程的队列，就会发生这种情况）的情况下，该线程才是可以中断的。

用户APC也永远不会中断正在用户模式下运行的线程，此时该线程需要执行可告警的等待，或者通过Ring级别转换或上下文切换重新访问用户APC队列。然而对于内核APC，在目标线程所在处理器上请求中断会将IRQL提升至APC_LEVEL级别，通知处理器必须查看当前运行中线程的内核APC队列。并且在这两种场景下，如果线程正在“做其他事情”，则需要通过某种转换让该线程进入运行中或等待中的状态。而这种操作实际上会导致线程被挂起，例如不再执行被加入自己队列中的APC。

除了上文介绍的有关可告警场景，我们曾提到线程的APC是可被禁用的。内核与驱动程序开发者可通过两种机制做到这一点，一种是在执行某些代码时直接将其IRQL提升至APC_LEVEL或更高级别。由于线程已经处于运行中的状态，因此通常会产生一个中断，但根据之前介绍过的IRQL规则，如果处理器已经处于APC_LEVEL（或更高）级别，中断将会被遮掩。因此，只有当IRQL被降低至PASSIVE_LEVEL，挂起的中断才会被交付，APC才能正常执行。

如果希望将APC重新交付给线程，强烈建议使用第二种机制，即使用内核API KeEnterGuardedRegion并配合使用KeLeaveGuardedRegion，这种方式可避免更改中断控制器状态。这些API是递归的，可通过嵌套的方式多次调用。只要依然在这样的区域中，就可以安全地通过上下文切换至其他线程，因为状态更新操作会应用于线程对象（KTHREAD）结构中的SpecialApcDisable字段，而不是每个处理器的状态。

类似地，上下文切换也可以发生在APC_LEVEL级别上，即使这是每个处理器的状态。调度程序会将IRQL保存在KTHREAD的WaitIrql字段中，随后将处理器IRQL设置为新传入线程的WaitIrql（该IRQL可能是PASSIVE_LEVEL）。这会导致一种非常有趣的情况：从技术上来说，PASSIVE_LEVEL级别的线程可抢占APC_LEVEL级别的线程。这种可能性很常见并且完全正常，并且这也证明了在线程执行方面，调度器本身的重要性远远超过任何IRQL。只有提升至DISPATCH_LEVEL级别，禁用线程抢占，才能让IRQL取代调度器。由于最终只有APC_LEVEL的IRQL存在这样的行为，因此这通常也被称为线程本地IRQL（Thread-local IRQL），虽然并不完全准确，但该机制已经足以描述此处提到的这种行为。

无论内核开发者如何禁用APC，有一条规则是始终适用的：代码不能以PASSIVE_ LEVEL之上的任何APC级别返回至用户模式，SpecialApcDisable也不能设置为“0”之外的其他任何值。实际出现这种情况会立即触发Bugcheck，通常这意味着某些驱动程序忘了释放锁，或者离开了自己的保护区域。

对于两种APC模式，每种模式也有两个类型的APC：常规APC与特殊APC，这取决于不同的模式，这两种APC的行为也存在差异。下面将分别讨论每种组合。

● **特殊内核APC**。这种组合产生的APC会始终被插入APC队列中其他所有现有特殊内核APC的尾部，但在任何常规内核APC之前的位置。内核例程会收到指向APC参数和常规例程的指针，并在APC_LEVEL级别上运行，这样就可以选择将新的常规APC加入队列。

● **常规内核APC**。此类APC始终会被插入APC队列的末尾，由此，特殊内核APC就可以将新的常规内核APC加入队列并稍后执行，上文的例子中描述了这样的情况。此类APC不仅可以通过上文提到的两种机制禁用，也可以通过一种名为KeEnterCriticalRegion的API（配合KeLeaveCriticalRegion）禁用，这会更新KTHREAD中的KernelApcDisable计数器，但不会更新SpecialApcDisable计数器。

● 这些APC首先会在APC_LEVEL级别下执行自己的内核例程，并向其发送参数和常规例程指针。如果常规例程尚未清除，则会将IRQL降低至PASSIVE_LEVEL并照常执行常规例程，只不过此时会通过值的形式来传递输入参数。一旦常规例程返回，IRQL将再次重新提升至APC_LEVEL。

● **常规用户APC**。这种组合会导致APC被插入APC队列的末尾，进而供内核例程按照上一段所描述的方法在APC_LEVEL级别下首次执行。随后如果常规例程依然存在，该APC将准备进行用户模式的交付（很明显，是在PASSIVE_LEVEL级别进行的），为此会创建一个陷阱帧和执行帧，并最终导致在返回用户模式后，将由Ntdll.dll中的用户模式APC调度程序接管控制权，还将调用所提供的用户指针。一旦用户模式APC返回，调度程序将使用NtContinue或NtContinueEx系统调用返回到原来的陷阱帧。

● 这里需要注意，如果内核例程最后清理了常规例程，那么已收到告警的线程将失去该状态；相反，如果没有收到告警，则会变为已告警状态并且用户APC挂起标记会被设置，这可能导致其他用户模式APC被尽快交付。这是由KeTestAlertThread API负责执行的，本质上，其行为依然类似于常规APC在用户模式下执行，尽管内核例程已经取消了该调度。

● **特殊用户APC**。这种组合产生的APC是较新版本的Windows 10中新增的，概括体现了一种为线程终止APC而做的特殊调度情况，其他开发者也可以使用这种组合。下文很快将会提到，终止远程（非当前）线程的操作需要使用APC，但该操作只有在所有内核模式代码均已执行完毕后才能进行。以用户APC的形式交付终止代码很适合这种情况，但这也意味着用户模式的开发者应避免通过执行不可告警的等待或使用其他用户APC填充队列的方式进行终止。

为了解决这种问题，长久以来，内核都会通过一种硬编码的检查来验证用户APC的内核例程是否使用了KiSchedulerApcTerminate。如果是，则用户APC会被视为“特殊”的，放置在队列的开头处。此外，线程的状态也会被忽略，并且始终设置为“用户APC正在挂起”的状态，这会迫使系统在下一次用户模式Ring级别转换或上下文切换到该线程时执行该APC。

该功能是专为终止代码路径保留的，这意味着开发者如果希望为用户APC的执行提供类似保证，无论可告警状态如何，都必须进一步使用更复杂的机制，如使用SetThreadContext手动更改线程上下文，但这种做法易出错。为了解决此问题，QueueUserAPC2 API应运而生，该API可通过QUEUE_USER_APC_FLAGS_SPECIAL_ USER_APC标记传递，也能以官方可支持的方式为开发者提供类似功能。此类APC在加入队列后始终位于其他任何用户模式APC之前（极为特殊的终止APC除外），并且对于等待中的线程，还会忽略可告警标记。此外，该 APC 首先会以一种非常特殊的内核APC形式插入，其内核例程几乎可以立即执行，并将APC重新注册为一个特殊用户APC。

表8-9总结了每一类APC的插入与交付行为。

表8-9　APC的插入与交付行为

| APC类型                                  | 插入行为                              | 交付行为                                                                                                                                                                                                                                 |
| -------------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 特殊（内核）                                 | 插入最后一个特殊APC之后（位于所有其他常规APC之前）      | IRQL降低时，内核例程将在APC级别交付，此时线程不在保护区域内。当插入APC时，它将收到特定参数的指针                                                                                                                                                                                |
| 常规（内核）                                 | 插入内核模式APC列表末尾                     | IRQL降低时，内核例程将在APC_LEVEL级别交付，此时线程不在关键（或保护）区域内。当插入APC时，它将收到特定参数的指针。如果存在常规例程，在相关内核例程执行完毕后，这些常规例程将在PASSIVE_LEVEL级别上执行，并会收到相关内核例程返回的参数（可能是执行插入或新建操作时使用的原始参数）                                                                              |
| 常规（用户）                                 | 插入用户模式APC列表末尾                     | IRQL降低时，内核例程将在APC_LEVEL级别交付，此时线程将被设置为“用户APC挂起中”标记（意味着APC已被加入队列，线程处于可告警的等待状态），当插入APC时，它将收到特定参数的指针<br>如果存在常规例程，当相关内核例程执行完毕后，这些常规例程将在PASSIVE_LEVEL级别上以用户模式执行，并会收到相关内核例程返回的参数（可能是执行插入或新建操作时使用的原始参数）。如果常规例程被内核例程清理，则它会针对该线程执行Test-alert操作 |
| 用户线程终止APC<br>（KiSchedulerApcTerminate） | 插入用户模式APC列表开头                     | 立即设置为“用户APC挂起中”标记并按照与上文所述类似的规则进行处理，但在返回用户模式时会在PASSIVE_ LEVEL级别上进行交付，并收到线程终止特殊APC所返回的参数                                                                                                                                               |
| 特殊（用户）                                 | 插入用户模式APC列表开头，但在线程终止APC（如果存在的话）之后 | 与上一种情况相同，但参数是通过QueueUserAPC2（NtQueueApcThreadEx2）的调用方控制的。内核例程是一种内部的KeSpecialUserApcKernelRoutine函数，该函数可重新插入APC，将其由最初的特殊内核APC转换为特殊用户APC                                                                                               |

执行体会使用内核模式APC来执行必须在特定线程地址空间（以及上下文）中执行的操作系统工作。例如，它可以使用特殊内核模式APC指示线程停止执行可中断的系统服务，或借此记录某个线程地址空间内的一次异步I/O操作结果。环境子系统会使用特殊内核模式APC让线程变得可挂起或终止自身运行，或借此让线程获取或设置自己的用户模式执行上下文。Windows Subsystem for Linux（WSL）会使用内核模式APC来模拟向UNIX应用程序进程子系统传递的UNIX信号。

内核模式APC的另一个重要用途与线程的挂起和终止有关。由于这些操作可从任意线程发起并以其他任意线程为目标，内核会使用APC来查询线程上下文以及终止线程。设备驱动程序通常会阻止APC，或通过进入关键/保护区域防止在自己持有了锁的情况下执行此类操作，否则锁可能将永远无法释放，进而导致系统宕机。

设备驱动程序也可以使用内核模式APC。举例来说，如果发起一个I/O操作并且有线程进入等待状态，此时可调度执行另一个进程中的其他线程。当设备数据传输操作完成后，I/O系统必须通过某种方式重新进入发起该I/O操作的线程的上下文，以便将I/O操作结果复制到包含该线程的进程的地址空间缓冲区中。I/O系统使用一种特殊的内核模式APC来执行该操作，除非应用程序使用了SetFileIoOverlappedRange API或I/O完成端口，在这种情况下，缓冲区可能是内存中的全局缓冲区，否则，只有在线程从端口拉取到完成结果之后才能进行复制（I/O系统对APC的使用已在卷1第6章进行过详细介绍）。

很多Windows API（如ReadFileEx、WriteFileEx以及QueueUserAPC）也会使用用户模式APC。例如ReadFileEx和WriteFileEx函数可允许调用方指定I/O操作结束后要调用的完成例程。I/O完成是通过查询发起I/O操作的线程所对应的APC实现的，然而对完成例程的回调并不一定发生在将APC加入队列的时候，因为用户模式APC只能交付给处于可告警等待状态的线程。为了进入等待状态，线程可以等待对象句柄并指定自己的等待是可告警的（使用Windows的WaitForMultipleObjectsEx函数），或者可以直接测试自己是否有正在挂起的APC（使用SleepEx）。在这两种情况下，如果有用户模式APC正处于挂起状态，内核会中断（告警）这个线程，将控制转交给APC例程，并在APC例程完成后恢复线程的执行。与在APC_LEVEL级别下执行的内核模式APC不同，用户模式APC会在PASSIVE_LEVEL级别下执行。

APC的交付会导致等待队列重新排序，此处的“等待队列”可以理解为一个列表，其中列出了哪个线程正在等待什么，以及它们等待的具体顺序（有关如何解决这些等待的详细信息，请参阅“低IRQL同步”一节）。如果在交付APC时线程处于等待状态，在APC例程完成后，将重新发起或重新执行该等待。如果等待依然未能解决，线程将返回至等待状态，但这一次它会处于对象等待列表的末尾。例如，由于APC可用于挂起线程的执行过程，如果线程正在等待任何对象，那么其等待状态将被移除，直到线程恢复执行，随后该线程将被放置在线程列表的末尾，继续等待访问自己所等待的对象。正在执行可告警的内核模式等待的线程还可在线程终止时被唤醒，借此该线程就可以检查自己的唤醒到底是因为终止还是其他什么原因造成的。

### 8.4.3　计时器处理

系统的时钟间隔计时器可能是Windows计算机上最重要的设备，因为它有着高IRQL值（CLOCK_LEVEL）并且起着至关重要的作用。如果不使用该中断，Windows将无法跟踪时间，导致无法准确计算正常的运行时间和时钟时间，更严重的是，还会导致计时器无法过期，线程将永远无法使用自己的量程。如果不使用该中断，Windows还将无法成为一种可抢占的操作系统（preemptive operating system），此时，除非当前运行中的线程释放了CPU，否则任何处理器上将永远无法运行关键的后台任务和调度。

#### 计时器的类型和间隔

传统上，Windows控制计算机的系统时钟在某个适当的间隔内激发，后来还允许驱动程序、应用程序以及管理员根据需要修改时钟间隔。因此，系统时钟可以按照固定的周期性间隔进行激发，而时钟本身则是由自PC/AT时代起每台计算机都配备的可编程中断计时器（Programmable Interrupt Timer，PIT）芯片或实时时钟（Real Time Clock，RTC）维护的。PIT运行所用的晶振被调谐为以NTSC彩色载波频率的1/3来运行（这是因为该晶振最初被首款CGA图形卡用于视频输出功能），HAL可在此基础上通过多种可行的复合机制实现毫秒级别的间隔，这些间隔始于1 ms，最长可达15 ms。而RTC运行在32.768 kHz频率下，由于该频率本身是2的幂次，因此很容易配置为以2的幂次为间隔的各种频率运行。在基于RTC的系统中，可由APIC多处理器HAL将RTC配置为每15.6 ms激发一次，这大约等于每秒激发64次。

PIT和RTC存在很多问题：它们速度很慢，是一种连接到遗留总线上的外部设备，能实现的时钟粒度太粗，迫使所有处理器必须以同步方式访问自己的硬件寄存器，难以模拟，在新的嵌入式硬件设备（如物联网和移动设备）上已经越来越罕见。因此，硬件供应商开发了各种新型计时器，例如ACPI计时器（有时也叫电源管理（Power Management，PM）计时器）和APIC计时器（直接集成在处理器内部）。ACPI计时器针对不同的硬件架构实现了一流的灵活性和可移植性，但延迟较大，且在实现方面会导致各类问题的很多瑕疵。APIC计时器虽然高效，但通常已被用于实现其他的平台需求，如性能分析，即Profiling（不过较新的处理器已开始提供专用Profiling计时器）。

为了解决该问题，微软与业内厂商联手创建了一种名为高性能事件计时器（High Performance Event Timer，HPET）的规范，借此对RTC进行了大量改进。在具备HPET的系统中，将使用HPET代替RTC或PIC，此外，ARM64系统也有自己的计时器架构，名为通用中断计时器（Generic Interrupt Timer，GIT）。针对所有这些不同的机制，HAL会维持一种复杂的层次结构，借此针对特定系统确定可以使用的最佳计时器。这一过程的具体顺序如下：

1）如果是在虚拟机内部运行，为避免进行任何类型的模拟，首先会尝试找到一种合成的虚拟机监控程序（Hypervisor）计时器。

2）在物理硬件上，会试图找到GIT，但该机制仅适用于ARM64系统。

3）如果可能，会试图找到一种每处理器的计时器，例如本地APIC计时器（如果尚未被使用）。

4）否则会寻找HPET，具体查找顺序为：兼容MSI的HPET，遗留的周期性HPET，任何其他类型的HPET。

5）如果未找到HPET，则会使用RTC。

6）如果未找到RTC，则会试图寻找某些其他类型的计时器，如PIT或SFI计时器，并在可能的情况下，会优先尝试寻找支持MSI中断的此类计时器。

7）如果依然未找到任何计时器，意味着系统实际并不包含兼容Windows的计时器，这种情况应该是不会出现的。

HPET和LAPIC计时器还提供了另一个优势：除了只支持上文提到的典型的周期性模式外，这些计时器还可配置为一种“一次激发”（one shot）模式。该功能使得较新版本的Windows可以使用一种动态时钟周期模型（dynamic tick model），下文还将详细介绍这种模型。

#### 计时器粒度

某些类型的Windows应用程序需要非常快的响应速度，例如多媒体应用程序。实际上，某些多媒体任务甚至需要低至1 ms的响应速度。因此，Windows从早期开始就实现了一系列API与机制，以此降低系统时钟中断的间隔，进而可以更频繁地产生时钟中断。这些API并不会调整特定计时器所指定的速率（后续版本Windows通过增加增强的计时器提供了这样的功能，具体介绍请参见下一节），而是会提高系统中所有计时器的精度，但这也有可能导致其他计时器更频繁地过期。

也就是说，Windows依然会尽可能将时钟计时器还原为初始值。当进程每次请求更改时钟间隔时，Windows会增加一个内部引用计数器，并将其关联给该进程。驱动程序（也能更改时钟速率）也可以通过类似的方式加入这个全局引用计数器中。在所有驱动程序还原了时钟，且所有修改过时钟的进程已退出或还原改动后，Windows会将时钟还原至其默认值（否则将时钟调整为被进程或驱动程序使用过的第二高的值）。

实验：识别高频计时器

由于高频计时器可能会导致一些问题，Windows会使用Windows事件跟踪（Event Tracing for Windows，ETW）机制跟踪所有请求更改系统时钟间隔的进程和驱动程序，并显示这种请求的产生时间和所请求的间隔。目前的间隔如下图所示，开发者和系统管理员可以通过这些数据判断那些在其他方面完全正常，但电池性能较低的系统的问题所在，并能借此降低大型系统的整体能耗。要获取这些数据，只需运行powercfg/energy指令，随后就可以得到一个名为energy-report.html的HTML文件，其内容类似下图所示。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1821.png)

向下拖动页面打开Platform Timer Resolution（平台计时器精度）小节，在这里可以看到所有曾经更改过计时器精度并且依然活跃的应用程序，以及导致相关调用的调用栈。计时器精度对应的数值以“百纳秒”为单位，因此数值为20000的时段对应了2 ms。在如上例子中，有两个应用程序（Microsoft Edge以及远程桌面服务器TightVNC）分别请求过更高的精度。

我们也可以通过调试器获取此类信息。对于每个进程，EPROCESS结构中都包含了下列字段，这有助于我们发现计时器精度的变化：

```
+0x4a8 TimerResolutionLink : _LIST_ENTRY [ 0xfffffa80'05218fd8 - 0xfffffa80'059cd508 ]
+0x4b8 RequestedTimerResolution : 0 
+0x4bc ActiveThreadsHighWatermark : 0x1d 
+0x4c0 SmallestTimerResolution : 0x2710 
+0x4c8 TimerResolutionStackRecord : 0xfffff8a0'0476ecd0 _PO_DIAG_STACK_RECORD 
```

请注意，调试器还会额外显示另一类信息：特定进程曾经请求过的最小计时器精度。本例中所示的进程属于PowerPoint 2010，在放映幻灯片过程中，该应用通常会请求较低的计时器精度；但在编辑幻灯片过程中通常不会这样做。上文所示代码中PowerPoint的EPROCESS字段内容也证明了这一点，而相应的栈可通过转储PO_DIAG_STACK_ RECORD结构来进行解析。

最后，TimerResolutionLink字段通过双向链表ExpTimerResolutionListHead连接了所有曾经更改过计时器精度的进程。如果powercfg命令不可用或需要查阅历史进程的信息，则可使用调试器数据模型解析该列表，由此得知系统中所有已经或曾经更改过计时器精度的进程。例如，由下列输出结果可知，Edge曾在不同的时间请求过1 ms的精度，此外，远程桌面客户端和Cortana也有过类似的操作。不过WinDbg Preview不仅曾请求过更改精度，并且它在运行该命令时依然在请求更改精度。

```
lkd> dx -g Debugger.Utility.Collections.FromListEntry(*(nt!_LIST_ENTRY*)&nt!ExpTimerReso 
lutionListHead, "nt!_EPROCESS", "TimerResolutionLink").Select(p => new { Name = ((char*) 
p.ImageFileName).ToDisplayString("sb"), Smallest = p.SmallestTimerResolution, Requested = p.RequestedTimerResolution}),d 
====================================================== 
=         = Name             = Smallest = Requested = 
====================================================== 
= [0]     - msedge.exe       - 10000    - 0         = 
= [1]     - msedge.exe       - 10000    - 0         = 
= [2]     - msedge.exe       - 10000    - 0         = 
= [3]     - msedge.exe       - 10000    - 0         = 
= [4]     - mstsc.exe        - 10000    - 0         = 
= [5]     - msedge.exe       - 10000    - 0         = 
= [6]     - msedge.exe       - 10000    - 0         = 
= [7]     - msedge.exe       - 10000    - 0         = 
= [8]     - DbgX.Shell.exe   - 10000    - 10000     = 
= [9]     - msedge.exe       - 10000    - 0         = 
= [10]    - msedge.exe       - 10000    - 0         = 
= [11]    - msedge.exe       - 10000    - 0         = 
= [12]    - msedge.exe       - 10000    - 0         = 
= [13]    - msedge.exe       - 10000    - 0         = 
= [14]    - msedge.exe       - 10000    - 0         = 
= [15]    - msedge.exe       - 10000    - 0         = 
= [16]    - msedge.exe       - 10000    - 0         = 
= [17]    - msedge.exe       - 10000    - 0         = 
= [18]    - msedge.exe       - 10000    - 0         = 
= [19]    - SearchApp.exe    - 40000    - 0         = 
======================================================
```

#### 计时器过期

上文曾经提到与时钟源生成的中断相关联的ISR，其主要任务之一是跟踪系统时间，这主要是通过KeUpdateSystemTime例程实现的。该ISR的另一个作用是跟踪逻辑运行时间，例如进程/线程执行时间以及系统时钟周期时间，诸如GetTickCount等API会使用这些底层数据，以供开发者在自己的应用程序中执行计时操作。这部分工作是由KeUpdateRunTime进行的。不过在执行任何此类工作前，KeUpdateRunTime会检查是否有计时器已过期。

Windows计时器可以是绝对计时器，这种计时器暗含了明确的未来过期时间；也可以是相对计时器，其中包含一个为负数的过期值，在插入计时器后，可通过该值从当前时间中进行扣减。从内部运作来看，所有计时器都会转换为绝对过期时间，不过系统会持续跟踪每个时间到底是“真正的”绝对时间还是转换后的相对时间。这个差异在某些情况下非常重要，例如在夏令时（甚至手动调整时钟）的情况下，如果用户将时钟从1:00 p.m.改为7:00 p.m.，此时绝对计时器依然可以在8:00 p.m.激发。但相对计时器（例如一个被设置为“两小时后过期”的计时器）将无法感知时钟的变化，因为两小时实际上还没有到。在遇到类似这种系统时间产生变化的情况下，内核会重编程与相对计时器有关联的绝对时间，以便匹配新的设置。

当时钟仅以周期模式激发的时候，由于时钟会以已知间隔的倍数过期，因此计时器可关联的系统时间的每个倍数，也可以叫作时钟指针（Hand），这是一种索引，存储在计时器对象的调度程序头部。Windows会通过这种方式，根据数组将所有驱动程序和应用程序的计时器整理为链表，表中的每一项对应了系统时间一种可能的倍数。由于现代版本Windows 10的运行不再必须依赖周期性的时钟周期（这归功于动态时钟周期功能），因此时钟指针也被重新定义为到期时间的上46位（以100 ns为单位）。这样每个时钟指针可以获得大约28 ms的“时间”。此外，因为在一个特定的时钟周期过程中（尤其是没有以固定的周期间隔激发时），可能会有多个时钟指针具备即将过期的计时器，Windows不能只检查当前时钟指针，而是需要使用一个位图来跟踪每个处理器的计时器表中的每个时钟指针。这些挂起的时钟指针都可通过该位图找到，并在每个时钟中断期间进行检查。

无论使用何种方法，这256个链表都会保存到一个名为计时器表（位于PRCB中）的表中，这样每个处理器就可以单独让自己的计时器过期，而不需要获取全局锁。该过程如图8-19所示。新版的Windows 10最多可使用两个计时器表，因此总共可产生512个链表。

稍后我们还将讨论如何决定计时器会被插入哪个逻辑处理器的计时器表。因为每个处理器都有自己的计时器表，每个处理器也都需要处理自己的计时器过期工作。当处理器被初始化时，该表中会被填入绝对计时器，为避免产生不连贯的状态，这些计时器的过期时间是无限的。因此，为确定某个时钟是否已过期，就只需要检查与当前时钟指针相关的对应链表中是否存在任何计时器即可。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1828.png)

图8-19　每处理器计时器列表范例

虽然更新计数器和检查链表操作的执行速度都很快，但对每个计时器执行该操作并使其过期，则可能会造成巨大的运行开销，毕竟目前所有这些工作都是在CLOCK_LEVEL级别（一种特别提升后的IRQL）上进行的。类似于驱动程序ISR通过将DPC加入队列来延迟自己工作的做法，时钟ISR也会请求DPC软件中断并在PRCB中设置标记，这样DPC排空机制就会知道哪些计时器需要过期。同理，在更新进程/线程运行时的时候，如果时钟ISR确定某个线程的量程已经过期，此时也会请求DPC软件中断并设置一个不同的标记。这些标记是针对每个PRCB专用的，因为每个处理器通常都会自行处理自己的运行时更新，而这是由于每个处理器都在运行不同的线程，并关联了不同的任务。表8-10列出了在计时器过期和处理过程中所涉及的各种字段。

表8-10　计时器处理所涉及的KPRCB字段

| KPRCB字段                  | 类型           | 描述                                                     |
| ------------------------ | ------------ | ------------------------------------------------------ |
| LastTimerHand            | 索引（最大265）    | 由该处理器处理的最后一个计时器时钟指针。在新版系统中已包含在TimerTable中，因为新系统已经有两个表了 |
| ClockOwner               | 布尔值          | 表示当前处理器是否为时钟的所有者                                       |
| TimerTable               | KTIMER_TABLE | 计时器表列表中的列表头数量（256个，新版系统为512个）                          |
| DpcNormalTimerExpiration | 位            | 表示为请求计时器到期，已发出了DISPATCH_LEVEL中断                        |

DPC主要供设备驱动程序使用，但内核也可以使用。内核主要会使用DPC处理量程的过期。在系统时钟的每次时钟周期过程中，会在时钟的IRQL级别上发出一个中断。时钟中断处理程序（运行于Clock IRQL级别下）会更新系统时间并减小一个计数器的值，该计数器用于跟踪当前线程的运行时长。当该计数器归零后，意味着线程的时间量程已过期，此时内核可能需要重新调度处理器，并在DPC/Dispatch IRQL级别上完成一个低优先级的任务。时钟中断处理程序会将DPC加入队列以发起线程分发操作，随后完成自己的工作并降低处理器的IRQL。由于DPC中断的优先级低于设备中断，因此，在时钟中断完成之前所产生的任何挂起的设备中断都会先于DPC中断进行处理。

当IRQL最终降低至DISPATCH_LEVEL之后，作为DPC处理工作的一部分，还会选中这两个标记。

卷 1 第 4 章曾介绍过与线程调度和量程过期有关的操作。这里我们简要介绍计时器过期的工作方式。由于计时器会通过时钟指针相互链接，过期代码（由 PRCB 在TimerExpirationDpc字段中关联的DPC执行，通常为KiTimerExpirationDpc）会从头到尾解析该列表（在插入时，将优先插入距离时钟间隔倍数最接近的计时器，其次会选择最接近下一个间隔但依然位于当前时钟指针范围的计时器）。要让计时器过期，主要涉及两个任务：

● 计时器会被视为一种调度程序同步对象（在超时或直接等待的过程中，线程会在计时器上等待）。计时器上还会运行Wait-testing（等待测试）和Wait-satisfaction（等待满足）算法，下文介绍同步的章节中还将详细介绍具体的工作方式。用户模式应用程序以及一些驱动程序就是通过这种方法使用计时器的。

● 计时器会被视为一种与DPC回调例程相关联的控制对象，计时器过期时将会执行该例程。该方法仅供驱动程序使用，可以针对计时器过期实现非常低延迟的响应（等待/调度程序方法则需要通过各种额外的逻辑来实现等待信号）。此外，因为计时器过期本身是在DISPATCH_LEVEL级别执行的，DPC也运行在该级别下，因此很适合充当计时器回调。

随着每个处理器被唤醒来处理时钟间隔计时器，借此执行系统时间和运行时间的处理工作，当一个轻微的延迟/拖延后导致IRQL从CLOCK_LEVEL降低至DISPATCH_LEVEL级别时，该过程中还会处理计时器的过期。图8-20展示了双处理器系统中的这一行为：其中实线箭头代表时钟中断的激发，而虚线箭头代表在处理器具备相关计时器的情况下，可能需要进行的计时器过期处理工作。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1940.png)

图8-20　计时器的过期

#### 处理器的选择

插入计时器时还要做出一个关键决定：选择适合的表，换句话说，就是选择最适合的处理器。首先，内核会检查计时器序列化是否被禁用。如果禁用，随后还会检查计时器的过期是否关联了DPC。如果DPC已被关联到某个目标处理器，那么此时就会选择该处理器的计时器表。如果该计时器没有与其关联的DPC，或如果DPC未绑定至某个处理器，则内核会扫描当前处理器组中所有尚未休止的处理器（有关内核休止的详细信息，请参阅卷1第4章）。如果当前处理器已休止，则会选择同一NUMA节点中尚未休止且距离最接近的处理器，否则会使用当前处理器。

这种行为的本意是为了改善Hyper-V服务器系统的性能与可伸缩性，但其实也有助于改善高负荷系统的性能。随着系统计时器的堆积（因为大部分驱动程序并不为自己的DPC设置关联性），CPU 0将变得越来越拥堵，有越来越多计时器过期代码需要执行，这会导致延迟增加，甚至导致DPC的处理产生极高延迟以及缺失。此外，计时器过期还可能导致与通常负责驱动程序（例如网络数据包代码）中断处理的DPC产生竞争，这会导致整个系统速度受到影响。Hyper-V还会让这种情况进一步加剧，此时CPU 0可能必须处理大量虚拟机所关联的计时器和相关DPC，而每个虚拟机都有自己的计时器和相关联的设备。

通过将计时器分散到多个处理器上（见图8-21），每个处理器的计时器过期负载即可完全由未休止的多个逻辑处理器来分摊。在32位系统中，计时器对象会将与自己关联的处理器的编号存储在调度程序头部；在64位系统中，则会存储在对象本身之内。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1947.png)

图8-21　计时器队列行为

这种行为虽然能让服务器系统大幅获益，但对客户端系统的影响通常并不会太大。此外，这会使得每个计时器的过期事件（例如时钟周期）变得更复杂，因为处理器可能已经闲置，但此时可能依然关联了计时器，这就意味着该处理器依然需要接收时钟周期，甚至可能还需要扫描其他每个处理器的表。另外，因为多个处理器可能会同时取消和插入计时器，这也意味着计时器的过期本质上属于一种异步行为，这可能并非始终是我们需要的。这种复杂性使得系统几乎无法实现新型待机[1] 所需要的“复原阶段”（resiliency phase），因为无法保证始终使用同一个处理器来管理时钟。因此在客户端系统中，如果可使用新型待机功能，计时器序列化将被启用，此时无论何种情况，内核始终将选择CPU 0。这也使得CPU 0在实际行为上成为默认的时钟所有者，该处理器将始终处于激活状态，以便随时选择时钟中断（具体请参见下文）。

[1]新型待机（Modern Standby）早期也叫Connected Standby，是Windows 8开始引入的一种全新节能模式，意在让计算机实现与手机等移动设备类似的“待机”和唤醒能力，并与手机一样在“待机”状态下维持网络连接，以接收各应用的推送通知。——译者注

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　该行为是由内核变量KiSerializeTimerExpiration控制的，这个变量会根据一个注册表设置进行初始化，服务器和客户端Windows系统中，该设置使用了不同的值。通过在注册表HKLM\SYSTEM\CurrentControlSet\Control\Session Manager\Kernel键下修改或创建一个名为SerializeTimerExpiration的值，并将其数值设置为“0”和“1”之外的其他任何内容，即可禁用计时器序列化功能，进而使得计时器可以平均分配到不同的处理器。删除该值，或将其设置为“0”，可以让内核根据新型待机功能的可用性自行决定是否使用计时器序列化。将其设置为“1”，可永久启用序列化，哪怕系统并不支持新型待机。 |
| ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |

实验：查看系统计时器

我们可以使用内核调试器转储系统中当前已注册的计时器，以及每个计时器所关联的DPC（如果有的话）信息。具体的输出结果类似下列范例所示：

```
0: kd> !timer 
Dump system timers 
　
Interrupt time: 250fdc0f 00000000 [12/21/2020 03:30:27.739] 
　
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 0 - High precision) 
List Timer             Interrupt Low/High Fire Time                  DPC/thread
　
PROCESSOR 0 (nt!_KTIMER_TABLE fffff8011bea6d80 - Type 1 - Standard) 
List Timer             Interrupt Low/High Fire Time                DPC/thread 
 1 ffffdb08d6b2f0b0   0807e1fb 80000000 [         NEVER         ] thread ffffdb08d748f480
 4 ffffdb08d7837a20   6810de65 00000008 [12/21/2020 04:29:36.127] 
 6 ffffdb08d2cfc6b0   4c18f0d1 00000000 [12/21/2020 03:31:33.230] netbt!TimerExpiry
                                                                 (DPC @ ffffdb08d2cfc670)
   fffff8011fd3d8a8 A fc19cdd1 00589a19 [ 1/ 1/2100 00:00:00.054] nt!ExpCenturyDpcRoutine
                                                                 (DPC @ fffff8011fd3d868)
 7 ffffdb08d8640440   3b22a3a3 00000000 [12/21/2020 03:31:04.772] thread ffffdb08d85f2080
   ffffdb08d0fef300   7723f6b5 00000001 [12/21/2020 03:39:54.941] 
                             FLTMGR!FltpIrpCtrlStackProfilerTimer (DPC @ ffffdb08d0fef340) 
11 fffff8011fcffe70   6c2d7643 00000000 [12/21/2020 03:32:27.052] nt!KdpTimeSlipDpcRoutine
                                                                  (DPC @ fffff8011fcffe30) 
   ffffdb08d75f0180   c42fec8e 00000000 [12/21/2020 03:34:54.707] thread ffffdb08d75f0080
14 fffff80123475420   283baec0 00000000 [12/21/2020 03:30:33.060] tcpip!IppTimeout
                                                                  (DPC @ fffff80123475460) 
. . . 
58 ffffdb08d863e280 P 3fec06d0 00000000 [12/21/2020 03:31:12.803] thread ffffdb08d8730080
   fffff8011fd3d948 A 90eb4dd1 00000887 [ 1/ 1/2021 00:00:00.054] nt!ExpNextYearDpcRoutine
                                                                  (DPC @ fffff8011fd3d908) 
. . . 
104 ffffdb08d27e6d78 P 25a25441 00000000 [12/21/2020 03:30:28.699] 
                                  tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6d38) 
    ffffdb08d27e6f10 P 25a25441 00000000 [12/21/2020 03:30:28.699] 
                                  tcpip!TcpPeriodicTimeoutHandler (DPC @ ffffdb08d27e6ed0) 
106 ffffdb08d29db048 P 251210d3 00000000 [12/21/2020 03:30:27.754] 
                             CLASSPNP!ClasspCleanupPacketTimerDpc (DPC @ ffffdb08d29db088) 
    fffff80122e9d110 258f6e00 00000000 [12/21/2020 03:30:28.575] 
                                   Ntfs!NtfsVolumeCheckpointDpc (DPC @ fffff80122e9d0d0) 
108 fffff8011c6e6560  19b1caef 00000002 [12/21/2020 03:44:27.661] 
                                 tm!TmpCheckForProgressDpcRoutine (DPC @ fffff8011c6e65a0) 
111 ffffdb08d27d5540 P  25920ab5 00000000 [12/21/2020 03:30:28.592] 
                               storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27d5580) 
    ffffdb08d27da540 P  25920ab5 00000000 [12/21/2020 03:30:28.592] 
                               storport!RaidUnitPendingDpcRoutine (DPC @ ffffdb08d27da580) 
. . . 
Total Timers: 221, Maximum List: 8 
Current Hand: 139 
```

在上述范例（为节省版面，有所省略）中，包含多个与驱动程序相关且很快即将过期的计时器，这些计时器分别关联至Netbt.sys和Tcpip.sys驱动程序（均与网络功能有关）以及Ntfs（存储控制器驱动程序）。此外还有一些在后台负责清理工作的计时器即将过期，例如与电源管理、ETW、注册表刷新、用户账户控制（UAC）虚拟化有关的计时器。另外，还有十几个计时器没有关联任何DPC，这些可能是等待调度的用户模式或内核模式计时器。我们可以针对线程指针运行!thread命令来验证这一点。

最后，Windows系统中还有三个始终存在的有趣计时器，这些计时器分别负责检查夏令时时区的变化、检查新年是否即将到来，以及检查新世纪是否即将到来。根据这些计时器过期时间的远近，除非在相关时间点即将到来时执行该实验，否则就可以很轻松地找出它们。

#### 计时器时钟周期的智能分配

从图8-20所示的负责处理时钟的ISR和过期计时器的处理器范例中可知，尽管并不存在相关联的过期计时器（虚线箭头），但处理器1依然会被唤醒多次（实线箭头）。虽然只要处理器1处于运行状态就会体现出这样的行为（这是为了更新线程/进程运行次数和调度状态），但如果处理器1处于空闲状态（且不包含过期计时器）呢？它是否依然需要处理时钟中断？上文曾经提到，此时唯一需要做的工作是更新整体系统时间/时钟周期，因此仅指定一个处理器作为时间维持处理器（本例中为处理器0）就已足够，这样其他处理器就可以继续处于睡眠状态；如果这些处理器被唤醒，任何与时间有关的调整工作均可通过与处理器0重新同步来实现。

实际上，Windows已经实现了这样的目标（在内部这称为计时器时钟周期的智能分配），图8-22展示了处于此场景下的处理器状态，其中处理器1正在睡眠（与上文情况不同，当时我们假定它正在运行代码）。图8-22中，处理器1只被唤醒了5次以处理自己的过期计时器，这就产生了更大的间隙（睡眠时段）。内核所使用的KiPendingTimerBitmaps变量中包含一个由相关性掩码结构（affinity mask structure）组成的数组，该数组决定了哪个逻辑处理器需要按照特定计时器时钟指针（时钟周期间隔）接收时钟间隔。随后即可据此对中断控制器进行恰当的编程，并确定将向哪些处理器发送IPI以发起计时器处理工作。

留出尽可能大的间隙，这一点非常重要，这是由电源管理功能在处理器上的工作方式决定的：当处理器检测到工作负载即将越来越少时，它便会降低自己的能耗（P状态），直到自己最终处于闲置状态。随后处理器可以选择性地将自身的部分电路关闭，逐渐进入更深度的闲置/睡眠状态，例如可能会关闭缓存。然而，处理器的再次唤醒需要耗费电力并花费一定时间，因此，仅在当处理器处于特定状态下，在时间和能耗方面获得的好处超过进入并退出该状态所需的时间和能耗的情况下，设计者才会冒险让处理器进入更深度的闲置/睡眠状态（C状态）。很明显，花费10 ms进入某种睡眠状态但该状态只维持了1 ms，这是一种很不合理的做法。通过防止时钟中断在（由于计时器的存在而显得）非必要的时候唤醒睡眠中的处理器，才能让处理器在更长时间内处于更深度的C状态。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1989.png)

图8-22　应用于处理器1的计时器时钟周期智能分配

#### 计时器合并

在没有计时器将要过期的时间里让睡眠中的处理器只产生最少量的时钟中断，这种做法虽然能大幅延长C状态的间隔，但在计时器粒度仅为15 ms的情况下，很多计时器很可能会在任意时钟指针范围内排队并频繁过期，哪怕处理器0也会遇到这种情况。减少软件计时器过期所产生的工作量，不仅有助于降低延迟（因为需要在DISPATCH_LEVEL级别上执行的工作更少），同时可以让其他处理器在睡眠状态下维持更长时间（因为我们可以确信处理器被唤醒只是为了处理即将过期的计时器，因此过期计时器的数量越少，在睡眠状态维持的时间就越长）。实际上，过期计时器的数量减少不仅会对睡眠状态（以及延迟）产生切实影响，还会对这些计时器过期的周期性产生影响：6个计时器在同一个时钟指针范围内同时过期，这总好过6个计时器在6个不同时钟指针范围内过期。因此，为了全面优化闲置时间的持续长度，内核需要通过一种合并（coalescing）算法将不同的计时器时钟指针合并为包含多个过期的同一个时钟指针。

计时器合并生效依赖的一个假设前提：对于大部分驱动程序和用户模式应用程序，它们并不非常关心自己计时器的确切激发时长（但某些多媒体应用程序除外）。随着原始计时器时长的增长，这种“不关心”的范围也会扩大：一个本应每30 s被唤醒一次的应用程序可能并不介意自己每31 s或每29 s被唤醒一次；而一个本应每1 s轮询一次的驱动程序，如果每1 s外加50 ms，或每1 s减去50 ms轮询一次，通常也不会造成太大的问题。大部分周期性计时器都依赖一个重要的保证：在某一特定范围内，自己的激发时长可以保持固定不变。举例来说，如果一个计时器被更改为每1 s外加50 ms，那么它依旧可以永远在该范围内进行激发，而不会有时候以每2 s，有时候以每0.5 s为间隔激发。然而，并非所有计时器可以合并为更粗粒度，因此Windows只会为标记为“可合并”的计时器启用该机制。计时器可通过KeSetCoalescableTimer这个内核API或用户模式对应的SetWaitableTimerEx添加该标记。

借助这些API，驱动程序和应用程序开发者可以自由地为内核提供自己的计时器所能容忍的最大宽容度（或可容忍延迟），这个最大宽容度可理解为一段时间长度的最大值，当发出请求并等待了这么长的时间后，计时器将依然能正确工作（在上文的例子中，那个1 s计时器的宽容度是50 ms）。推荐的最小宽容度为32 ms，这对应了15.6 ms时钟周期的两倍，任何比这个数字更小的值实际上都不会导致任何合并，因为即将过期的计时器甚至已经无法从一个时钟周期移动到另一个时钟周期。无论指定怎样的宽容度，Windows都会将计时器与四个首选合并间隔之一进行对齐，这四个首选合并间隔分别为1 s、250 ms、100 ms以及50 ms。

在为周期性计时器设置了可容忍的延迟后，Windows会使用一种名为Shifting（挪动）的过程让该计时器在不同周期之间漂移，直到它与特定宽容度相关的首选合并周期中最优化的周期间隔倍数保持对齐（随后该信息会被编码至调度程序的头文件中）。对于绝对计时器，则会扫描首选合并间隔列表，并根据距离调用方所指定的最大宽容度，在最接近的可接受合并间隔内生成一个首选的过期时间。这种行为意味着绝对计时器会始终尽可能远离自己的实际过期时间点，这样可以让计时器尽可能地分散，并为处理器提供更长的睡眠时间。

对于计时器的合并，我们可以参考图8-20并假设所有计时器都指定了宽容度，因此是可以合并的。但在一种情况下Windows会决定合并计时器，如图8-23所示。请注意，处理器1总共只收到了三个时钟中断，因此会导致闲置睡眠时间大幅延长，进而可以进入能耗更低的C状态。此外，处理器0上某些时钟中断需要执行的工作并不多，因此在每个时钟中断时，可能会消除降低至DISPATCH_LEVEL级别所需的延迟。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx1997.png)

图8-23　计时器合并

#### 增强计时器

首先，增强计时器的出现主要是为了解决原本的计时器系统经过多次改进后依然无法解决的大量需求问题。例如，虽然计时器合并有助于降低能耗，但也会导致计时器产生不一致的过期时间，哪怕是在根本无须降低能耗的情况下也是如此（换句话说，计时器合并是一种“全有或全无”的做法）。其次，Windows用于实现高精度计时器的唯一机制就是让应用程序和驱动程序以全局形式降低时钟的时钟周期，但这种方式会对系统产生巨大的负面影响。出乎意料的是，尽管此时这类计时器的精度可能已经提高，但实际上可能未必很精确，因为无论粒度精细到何种程度，常规的计时器过期操作依然可能会先于时钟的时钟周期而发生。

最后，还请回忆卷1第6章介绍过的新型待机功能，这个功能引入了诸如计时器虚拟化和桌面活动审查器（Desktop Activity Moderator，DAM）[2] 等功能，在新型待机的复原阶段，这些功能会主动延迟计时器的过期，借此模拟S3的睡眠状态。但是在该阶段，依然需要允许一些重要的系统计时器活动定期运行。

[2]桌面活动审查器是Windows 8客户端系统引入的一个全新组件，主要用于当系统进入新型待机状态后暂停所有桌面应用程序的运行，并限制第三方系统服务的运行。——译者注

这三个需求催生了增强计时器，这类计时器在内部称为Timer2对象，是由一些新增的系统调用（例如NtCreateTimer2和NtSetTimer2）或驱动程序API（例如ExAllocateTimer和ExSetTimer）创建的。增强计时器支持四种行为模式，其中某些模式是互斥的：

● No-wake：此类增强计时器是对计时器合并进行的改进，可以提供原本只能在睡眠时段中使用的可容忍延迟。

● High-resolution：此类增强计时器对应于高精度计时器，但具备专属的精确时钟速率。时钟速率只需要在接近计时器到期时间时才需要以此速率运行。

● Idle-resilient：此类增强计时器可以在深度睡眠状态（例如新型待机的复原阶段）下依然保持活跃状态。

● Finite：此类增强计时器不包含上文所介绍的任何一种特性。

High-resolution计时器也可以是Idle resilient计时器，反之亦然。但Finite计时器无法具备上述任何一种特性。那么，如果Finite类型的增强计时器不包含任何“特殊”行为，最初又为何创建这种类型的计时器？实际上，由于新增的Timer2基础架构是对自Windows内核最初开发时就已具备的老旧计时器逻辑的重写，因此，抛开这些特殊功能不谈，它们还提供了其他一些好处：

● 它使用了一种自平衡的红黑二叉树，而没有使用来自计时器表的链表。

● 它允许驱动程序明确启用或禁用回调，而无须手动创建DPC。

● 它为每个操作提供了全新并且更简洁的ETW跟踪项，这能对故障分析工作起到一定帮助。

● 它通过某些指针混淆技术和额外的断言提供了更深入的安全性，从而强化了针对单纯以数据为目标的攻击和破坏行为的防御能力。

因此，完全以Windows 8.1和后续版本系统为目标的驱动程序开发者，即使不需要这些额外的功能，也强烈建议使用全新的增强计时器基础架构。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　微软技术文档中提到的ExAllocateTimer API并不允许驱动程序创建Idle-resilient类型的计时器。实际上，这样的操作企图会导致系统崩溃。只有微软在系统中内置的驱动程序可以使用ExAllocateTimerInternal API创建此类计时器。不建议读者使用该API创建计时器，因为内核维持了一个静态的硬编码列表，其中列出了所有已知的合法调用方，并要求调用方必须提供唯一标识符对全过程进行跟踪，借此即可知道允许不同组件创建多少个此类计时器。任何违反该规则的操作都会导致系统崩溃（蓝屏死机）。 |
| ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

相比常规计时器，增强计时器的过期规则也更复杂，因为这类计时器最终会面临两个可能的截止时间。第一个叫作最小截止时间（minimum due time），决定了允许该计时器过期的最早系统时钟时间；第二个叫作最大截止时间（maximum due time），代表该计时器应该过期的最晚系统时钟时间。Windows可以保证计时器会在这两个时间点间的某一刻过期，这可以是常规的时钟周期每个间隔（例如15 ms）所导致的，或者是因为对计时器过期操作的临时检查（例如当一个中断唤醒了闲置线程时）所导致的。该间隔通过将开发者传入的预期过期时间按照所传入的“不唤醒容忍度”进行调整计算而来。如果指定了无限制的唤醒容忍度，那么计时器将不具备最大截止时间。

因此，一个Timer2对象最多可以驻留在红黑二叉树的两个节点中：用于检查最小截止时间的节点0，以及用于检查最大截止时间的节点1。No-wake和High-resolution计时器位于节点0内，而Finite和Idle-resilient计时器位于节点1内。

上文曾经提到这些属性有些是可以合并的，那么这又该如何与两个节点配合生效？很明显，一个红黑二叉树是不够的，系统无疑需要更多这种二叉树，这也叫作集合（Collection，详见公开的KTIMER2_COLLECTION_INDEX数据结构），上文提到的每一类增强计时器对应一个二叉树。随后，计时器可被插入节点0或节点1，或同时插入这两者，或哪一个也不插入，这主要取决于表8-11中列出的规则与组合。

表8-11　计时器类型和节点集合索引

| 计时器类型                            | 节点0集合索引       | 节点1集合索引                |
| -------------------------------- | ------------- | ---------------------- |
| No-wake                          | NoWake，如果有容忍度 | NoWake，如果有非无限的容忍度或无容忍度 |
| Finite                           | 从不会插入该节点      | Finite                 |
| High-resolution                  | 始终插入Hr        | Finite，如果有非无限的容忍度或无容忍度 |
| Idle-resilient                   | NoWake，如果有容忍度 | Ir，如果有非无限容忍度或无容忍度      |
| High resolution 和 Idle-resilient | 始终插入Hr        | Ir，如果有非无限容忍度或无容忍度      |

节点1可以看成对默认的旧版计时器行为创建的镜像：会在每个时钟周期里检查计时器是否即将过期。因此计时器只要位于节点1中，就注定会过期，这也暗示了其最小截止时间与其最大截止时间是相同的。然而，有着无限容忍度的计时器是不会被放入节点1的，因为从技术的角度来看，只要CPU永远保持睡眠状态，这样的计时器将永远不过期。

High-resolution计时器则完全相反，系统会始终在此类计时器即将过期的“正确”时间点上进行检查，永远不会提前，因此它们会被放入节点0。然而，如果它们准确的过期时间对于节点0中的检查而言“太早了”，那么也可能会放入节点1，此时它们会像常规（有限）计时器一样处理（也就是说，实际过期时间会比预期时间略晚一些）。如果调用方提供了容忍度，系统处于闲置状态，并且产生了合并计时器的机会，则也有可能发生这种情况。

类似地，对于Idle-resilient计时器，如果系统并不处于复原阶段，那么此类计时器并不同时属于High-resolution计时器（这是增强计时器的默认状态），将会位于NoWake集合中；其他情况下此类计时器将位于Hr集合中。然而，在需要检查节点1的时钟周期内，尽管系统可能处于深度睡眠状态，只有位于特殊的Ir集合内的计时器才能被识别成为需要执行的计时器。

这种情况最初可能会令人感到困惑，但这种状态使得所有以合法方式合并的计时器在系统时钟周期内（节点1，强制实施最大截止时间）进行检查时，或在计算出的下一个最接近的截止时间（节点0，强制实施最小截止时间）进行检查时，能够表现出正确的行为。

当每个计时器被插入相应集合（KTIMER2_COLLECTION）和相关联的一个或多个红黑树节点时，集合的下一个截止时间会被更新，变更为集合中任意一个计时器最早的截止时间，此时可通过一个全局变量（KiNextTimer2Due）体现任意集合中任意一个计时器最早的截止时间。

实验：列出增强的系统计时器

我们可以使用上文实验中曾经用到的内核调试器查看增强计时器（Timer2），它们会显示在输出结果的最末尾：

```
KTIMER2s: 
Address,          Due time,                            Exp. Type   Callback, Attributes, 
ffffa4840f6070b0  1825b8f1f4 [11/30/2020 20:50:16.089] (Interrupt) [None] NWF (1826ea1ef4 
                                                               [11/30/2020 20:50:18.089])
ffffa483ff903e48  1825c45674 [11/30/2020 20:50:16.164] (Interrupt) [None] NW P (27ef6380) 
ffffa483fd824960  1825dd19e8 [11/30/2020 20:50:16.326] (Interrupt) [None] NWF (1828d80a68 
                                                               [11/30/2020 20:50:21.326]) 
ffffa48410c07eb8  1825e2d9c6 [11/30/2020 20:50:16.364] (Interrupt) [None] NW P (27ef6380) 
ffffa483f75bde38  1825e6f8c4 [11/30/2020 20:50:16.391] (Interrupt) [None] NW P (27ef6380) 
ffffa48407108e60  1825ec5ae8 [11/30/2020 20:50:16.426] (Interrupt) [None] NWF (1828e74b68 
                                                               [11/30/2020 20:50:21.426]) 
ffffa483f7a194a0  1825fe1d10 [11/30/2020 20:50:16.543] (Interrupt) [None] NWF (18272f4a10 
                                                               [11/30/2020 20:50:18.543]) 
ffffa483fd29a8f8  18261691e3 [11/30/2020 20:50:16.703] (Interrupt) [None] NW P (11e1a300) 
ffffa483ffcc2660  18261707d3 [11/30/2020 20:50:16.706] (Interrupt) [None] NWF (18265bd903 
                                                               [11/30/2020 20:50:17.157]) 
ffffa483f7a19e30  182619f439 [11/30/2020 20:50:16.725] (Interrupt) [None] NWF (182914e4b9 
                                                               [11/30/2020 20:50:21.725]) 
ffffa483ff9cfe48  182745de01 [11/30/2020 20:50:18.691] (Interrupt) [None] NW P (11e1a300) 
ffffa483f3cfe740  18276567a9 [11/30/2020 20:50:18.897] (Interrupt) 
               Wdf01000!FxTimer::_FxTimerExtCallbackThunk (Context @ ffffa483f3db7360) NWF
                                       (1827fdfe29 [11/30/2020 20:50:19.897]) P (02faf080)
ffffa48404c02938  18276c5890 [11/30/2020 20:50:18.943] (Interrupt) [None] NW P (27ef6380)
ffffa483fde8e300  1827a0f6b5 [11/30/2020 20:50:19.288] (Interrupt) [None] NWF (183091c835
                                                               [11/30/2020 20:50:34.288])
ffffa483fde88580  1827d4fcb5 [11/30/2020 20:50:19.628] (Interrupt) [None] NWF (18290629b5
                                                               [11/30/2020 20:50:21.628])
```

在本例中，我们看到的主要是No-wake（NW）增强计时器，以及对应的最小截止时间。其中一些计时器是周期性的（P），会在过期时间里重新插入。此外，一些计时器还具备最大截止时间，这意味着它们被指定了容忍度，可显示自己将会过期的最晚时间。最后，还有一个增强计时器关联了回调，这个计时器归WDF（Windows Driver Foundation）框架所有（有关WDF驱动程序的更多信息请参阅卷1第6章）。

### 8.4.4　系统工作线程

在系统初始化过程中，Windows会在System进程中创建多个名为系统工作线程（system worker thread）的线程，这些线程的存在只是为了代表其他线程执行某些工作。很多情况下，运行于DPC/Dispatch级别的线程在执行函数时将只能在更低的IRQL级别下执行。例如DPC例程，就可以在DPC/Dispatch这个IRQL级别下通过任意线程上下文执行（因为DPC的执行可以抢占系统中的任意线程），该例程可能需要访问分页池，或等待使用调度程序对象将其与应用程序线程的执行保持同步。由于DPC例程无法降低IRQL，因此必须将此类处理工作传递给能在低于DPC/Dispatch的IRQL级别下执行的线程。

一些设备驱动程序和执行体组件会创建自己专用的线程，以便在被动级别上执行此类处理工作，然而，大部分情况下则会直接使用系统工作线程，这样可以避免系统中有额外线程时产生的非必要调度以及内存开销。执行体组件可调用执行体函数ExQueueWorkItem或IoQueueWorkItem来请求系统工作线程的服务。设备驱动程序只能使用后者（因为后者可将工作项（work item）关联给Device对象，进而实现更强的可追责能力，并能支持工作项处于活跃状态时驱动程序进行卸载操作这样的场景）。这些函数会将工作项放入线程工作所需的队列调度程序对象中（队列调度程序对象的详细信息请参阅卷1第6章“I/O完成端口”一节）。

IoQueueWorkItemEx、IoSizeofWorkItem、IoInitializeWorkItem以及IoUninitializeWorkItem这些API的行为也较为相似，但它们会与驱动程序的Driver对象或自己的某一个Device对象创建关联。

工作项包含一个指向例程的指针以及一个参数，线程在处理工作项时可将这些内容传递给例程。需要以被动级别执行的设备驱动程序或执行体组件可实现该例程。举例来说，必须等待调度程序对象的DPC例程可以初始化一个工作项，该工作项指向驱动程序中等待调度程序对象的例程。在某些阶段中，系统工作线程会从自己的队列中移除工作项并执行驱动程序的例程。当驱动程序的例程执行完毕后，系统工作线程会检查是否还有更多的工作项等待处理。如果没有更多等待处理的工作项，系统工作线程会被阻塞，直到新的工作项放入队列。在系统工作线程处理自己的工作项时，DPC例程可能已经执行完毕，但也可能尚未执行完毕。

系统工作线程可分为多种类型：

● 常规工作线程（normal worker thread），在优先级8级别下执行，其他方面的行为与延迟的工作线程类似。

● 后台工作线程（background worker thread），在优先级7级别下执行，会继承与常规工作线程相同的行为。

● 延迟的工作线程（delayed worker thread），在优先级12级别下执行，主要处理对时间要求不敏感的工作项。

● 关键工作线程（critical worker thread），在优先级13级别下执行，主要用于处理对时间要求敏感的工作项。

● 超关键工作线程（super-critical worker thread），在优先级14级别下执行，其他方面与关键工作线程类似。

● 极关键工作线程（hyper-critical worker thread），在优先级15级别下执行，其他方面与关键工作线程类似。

● 实时工作线程（real-time worker thread），在优先级18级别下执行，是唯一能在实时调度范围（详见卷1第4章）下运行的工作线程，这意味着此类工作线程不会受制于优先级提升或常规的时间切片。

由于所有这些工作队列的命名方式开始让人感到混淆，较新版本的Windows引入了自定义优先级工作线程，建议所有驱动程序开发者使用这种新的工作线程，因为它可以让驱动程序传入自己的优先级级别。

系统引导的早期阶段会调用一个特殊的内核函数ExpLegacyWorkerInitialization，该函数可以为延迟的工作队列线程和关键工作队列线程设置初始数值，具体数值则通过可选的注册表参数进行配置。大家可能在本书之前的版本中看到过相关的细节介绍，不过需要注意，这些变量的存在只是为了兼容外部编排工具，现代的Windows 10系统和后续版本系统的内核从未真正使用过它们。这是因为较新的内核实现了一种名为Priority queue（KPRIQUEUE）的全新内核调度程序对象，将其与数量完全动态的内核工作线程结合在一起，并进一步将原本的单一工作线程队列拆分为每个NUMA节点的工作线程。

在Windows 10和后续版本中，内核会根据需要动态创建额外的工作线程，默认的数量上限为4096个（可参阅ExpMaximumKernelWorkerThreads），但可通过修改注册表设置将上限增大至最多16384个线程，或减少至最少32个。我们可以在注册表HKLM\SYSTEM\ CurrentControlSet\Control\Session Manager\Executive键下通过MaximumKernelWorkerThreads值修改该设置。

我们在卷1第5章曾经介绍过，每个分区对象都包含一个执行体分区，该分区也是与执行体（主要是系统工作线程逻辑）有关的分区对象的一部分。其中包含的一个数据结构可用于跟踪分区中每个NUMA节点的工作队列管理器（队列管理器由死锁检测计时器、工作队列项收割器及指向实际执行管理工作的线程句柄组成）。此外，其中还包含一个指针数组，这些指针指向了8个可能的工作队列（EX_WORK_QUEUE）中的每一个。这些队列会关联一个单独的索引，并跟踪最小线程（可保证的）和最大线程的数量，以及截至目前已处理的工作项数量。

每个系统都包含两个默认工作队列：ExPool队列和IoPool队列。前者主要被用到了ExQueueWorkItem API的驱动程序和系统组件所使用，后者主要适用于IoAllocateWorkItem类型的API。最后，最多还可以为内部系统定义额外的6个队列，这些队列主要被内部（不可导出）的ExQueueWorkItemToPrivatePool API使用，使用了0～5的池标识符（因此对应的队列索引为2～7）。目前，仅内存管理器的存储管理器（详见卷1第5章）用到了这些功能。

执行体会尽可能尝试着将关键工作线程的数量与系统执行过程中不断变化的工作负载保持匹配。当工作项处理完毕或被加入队列后，会通过检查来判断是否需要新的工作线程。如果需要，则会发送一个事件信号，唤醒相关NUMA节点和分区的ExpWorkQueueManagerThread。当遇到下列任一情况时，还会额外创建一个工作线程：

● 队列中线程数量少于最小线程数量。

● 尚未达到最大线程数量，所有工作线程都在忙碌，但队列中依然有等待处理的工作项，或上一次尝试将工作项加入队列的操作企图失败了。

此外，对于每个工作队列管理器（即每个分区上的每个NUMA节点），ExpWorkQueueManagerThread会以每秒一次的频率确定是否已经发生了死锁。这个死锁的具体定义是：最后一个时间间隔内，排队的工作项数量增加，但所处理工作项的匹配数未增加。如果发生这种情况，系统将忽略任何线程数量的最大值限制，额外创建一个工作线程，以便尽可能地清除潜在的死锁。随后这项检测工作会被禁用，直到系统认为有必要再次进行检测（例如达到线程数量最大值时）。由于处理器拓扑可能因为热添加动态处理器而产生变化，因此这个额外创建的线程还负责更新处理器的相关性和数据结构，以便能继续跟踪新添加的处理器。

最后，每当经历了工作线程的超时值分钟数两倍的时间后（默认超时值10分钟，因此也就是每20分钟一次），该线程还会检查自己是否需要摧毁任何系统工作线程。我们可以通过WorkerThreadTimeoutInSeconds这个注册表值将默认超时值改为2～120分钟。这个过程也称收割（reaping），它保证了系统工作线程数量不会失控。如果系统工作线程等待了很长时间（具体时间由工作线程超时值定义），并且没有依然在等待处理的工作项，那么该线程就会被收割（这意味着当前线程数量会以一种及时的方式进行清理）。

实验：列出系统工作线程

不幸的是，由于系统工作线程每分区改组（reshuffling）功能的存在（已经不再像以前那样按NUMA节点进行，自然也就不再具备全局特性），内核调试器的!exqueue命令已经无法列出按照类型进行分类的系统工作线程列表，这样做将会直接出错。

由于EPARTITION、EX_PARTITION以及EX_WORK_QUEUE数据结构均已包含在公开的符号中，因此可以使用调试器数据模型查看队列及其管理器。例如，我们可以这样查看主（默认）系统分区NUMA节点0工作线程管理器：

```
lkd> dx ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueueManagers[0] 
((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueueManagers[0]     : 0xffffa483edea99d0 [Type: _EX_WORK_QUEUE_MANAGER *]
    [+0x000] Partition       : 0xffffa483ede51090 [Type: _EX_PARTITION *] 
    [+0x008] Node            : 0xfffff80467f24440 [Type: _ENODE *] 
    [+0x010] Event           [Type: _KEVENT] 
    [+0x028] DeadlockTimer   [Type: _KTIMER] 
    [+0x068] ReaperEvent     [Type: _KEVENT] 
    [+0x080] ReaperTimer     [Type: _KTIMER2] 
    [+0x108] ThreadHandle    : 0xffffffff80000008 [Type: void *] 
    [+0x110] ExitThread      : 0x0 [Type: unsigned long] 
    [+0x114] ThreadSeed      : 0x1 [Type: unsigned short] 
```

或者这样查看NUMA节点0的ExPool，目前其中包含15个线程，并已处理了将近400万个工作项！

```
lkd> dx ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][0],d 
((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][0],d       : 0xffffa483ede4dc70 [Type: _EX_WORK_QUEUE *] 
    [+0x000] WorkPriQueue    [Type: _KPRIQUEUE] 
    [+0x2b0] Partition       : 0xffffa483ede51090 [Type: _EX_PARTITION *] 
    [+0x2b8] Node            : 0xfffff80467f24440 [Type: _ENODE *] 
    [+0x2c0] WorkItemsProcessed : 3942949 [Type: unsigned long] 
    [+0x2c4] WorkItemsProcessedLastPass : 3931167 [Type: unsigned long] 
    [+0x2c8] ThreadCount      : 15 [Type: long] 
    [+0x2cc (30: 0)] MinThreads       : 0 [Type: long] 
    [+0x2cc (31:31)] TryFailed        : 0 [Type: unsigned long] 
    [+0x2d0] MaxThreads       : 4096 [Type: long] 
    [+0x2d4] QueueIndex       : ExPoolUntrusted (0) [Type: _EXQUEUEINDEX] 
    [+0x2d8] AllThreadsExitedEvent : 0x0 [Type: _KEVENT *] 
```

随后即可通过WorkPriQueue的ThreadList字段枚举该队列关联的所有工作线程：

```
lkd> dx -r0 @$queue = ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->
    ExPartition)->WorkQueues[0][0] 
@$queue = ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][0]            : 0xffffa483ede4dc70 [Type: _EX_WORK_QUEUE *] 
　
lkd> dx Debugger.Utility.Collections.FromListEntry(@$queue->WorkPriQueue.ThreadListHead,
    "nt!_KTHREAD", "QueueListEntry") 
Debugger.Utility.Collections.FromListEntry(@$queue->WorkPriQueue.ThreadListHead,
    "nt!_KTHREAD", "QueueListEntry") 
    [0x0]            [Type: _KTHREAD] 
    [0x1]            [Type: _KTHREAD] 
    [0x2]            [Type: _KTHREAD] 
    [0x3]            [Type: _KTHREAD] 
    [0x4]            [Type: _KTHREAD] 
    [0x5]            [Type: _KTHREAD] 
    [0x6]            [Type: _KTHREAD] 
    [0x7]            [Type: _KTHREAD] 
    [0x8]            [Type: _KTHREAD] 
    [0x9]            [Type: _KTHREAD] 
    [0xa]            [Type: _KTHREAD] 
    [0xb]            [Type: _KTHREAD] 
    [0xc]            [Type: _KTHREAD] 
    [0xd]            [Type: _KTHREAD] 
    [0xe]            [Type: _KTHREAD] 
    [0xf]            [Type: _KTHREAD] 
```

这只是ExPool的情况。别忘了系统中还有一个IoPool，它是这个NUMA节点（节点0）上的下一个索引（索引1）。我们可以继续通过实验查看其他私有池，例如存储管理器的池。

```
lkd> dx ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][1],d 
((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->
    WorkQueues[0][1],d        : 0xffffa483ede77c50 [Type: _EX_WORK_QUEUE *] 
    [+0x000] WorkPriQueue     [Type: _KPRIQUEUE] 
    [+0x2b0] Partition        : 0xffffa483ede51090 [Type: _EX_PARTITION *] 
    [+0x2b8] Node             : 0xfffff80467f24440 [Type: _ENODE *] 
    [+0x2c0] WorkItemsProcessed : 1844267 [Type: unsigned long] 
    [+0x2c4] WorkItemsProcessedLastPass : 1843485 [Type: unsigned long] 
    [+0x2c8] ThreadCount      : 5 [Type: long] 
    [+0x2cc (30: 0)] MinThreads       : 0 [Type: long] 
    [+0x2cc (31:31)] TryFailed        : 0 [Type: unsigned long] 
    [+0x2d0] MaxThreads       : 4096 [Type: long] 
    [+0x2d4] QueueIndex       : IoPoolUntrusted (1) [Type: _EXQUEUEINDEX] 
    [+0x2d8] AllThreadsExitedEvent : 0x0 [Type: _KEVENT *]
```

### 8.4.5　异常调度

相比可能在任意时间产生的中断，异常（exception）则是由运行中的程序的执行直接导致的某些状况。Windows使用了一种名为结构化异常处理（structured exception handling）的设施，可让应用程序控制异常的发生。随后，应用程序即可修复相应状况并返回异常发生时的状态，并解除堆栈（借此终止产生异常的子例程的执行）或向系统告知异常未被识别，系统应该继续搜索可能处理该异常的异常处理程序。本节会假设读者已经熟悉Windows结构化异常处理背后的基本概念。对于不熟悉的读者，建议首先阅读Windows SDK中有关Windows API参考文档的概述部分，或者阅读由Jeffrey Richter与Christophe Nasarre合作撰写的*Windows via C/C++*一书（Microsoft Press，2007年）第23～25章的内容。另外请注意，虽然异常处理可通过语言扩展（例如Microsoft Visual C++中的_ _try构造）来访问，但这实际上是一种系统机制，因此与具体语言是无关的。

在x86和x64处理器上，所有异常都有预定义的中断号，该中断号直接对应指向特定异常陷阱处理程序的IDT中的项。表8-12列出了x86定义的异常以及所分配的中断号。由于IDT中的第一个项已经被异常所使用，因此硬件中断会分配表中较为靠后的项，这一点在上文也有所提及。

表8-12　x86异常及其中断号

| 中断号 | 异常           | 助记符     |
| --- | ------------ | ------- |
| 0   | 除法错误         | #DE     |
| 1   | 调试（单步）       | #DB     |
| 2   | 不可遮蔽中断（NMI）  | —       |
| 3   | 断点           | #BP     |
| 4   | 溢出           | #OF     |
| 5   | 边界检查（范围已超出）  | #BR     |
| 6   | 无效操作码        | #UD     |
| 7   | NPX不可用       | #NM     |
| 8   | 双重错误         | #DF     |
| 9   | NPX段溢出       | —       |
| 10  | 无效任务状态段（TSS） | #TS     |
| 11  | 段不存在         | #NP     |
| 12  | 栈段错误         | #SS     |
| 13  | 常规保护         | #GP     |
| 14  | 页面错误         | #PF     |
| 15  | Intel保留      | —       |
| 16  | x87浮点        | #MF     |
| 17  | 对齐检查         | #AC     |
| 18  | 机器检查         | #MC     |
| 19  | SIMD浮点       | #XM或#XF |
| 20  | 虚拟化异常        | #VE     |
| 21  | 控制保护（CET）    | #CP     |

除了简单到可以被陷阱处理程序所处理的异常外，其他所有异常都是由一个名为异常调度程序（Exception Dispatcher）的内核模块提供服务的。异常调度程序的作用是查找可以处理异常的异常处理程序。内核定义了很多架构独立的异常，例如内存访问冲突、整数除以零、整数溢出、浮点异常、调试器断点等。要查看架构独立异常的完整清单，请参阅Windows SDK参考文档。

内核陷阱及其处理程序会使用对用户程序来说透明的方式来处理某些异常。例如，在执行被调试的程序时遇到断点便会产生一个异常，内核会调用调试器处理这样的异常。但内核在处理某些其他异常时也会直接向调用方返回不成功的状态代码。

少数异常可在未经改动的情况下通过过滤回到用户模式。例如，某些类型的非法内存访问或算术溢出会生成操作系统无法处理的异常。32位应用程序可建立基于帧的异常处理程序来应对这些异常。此处的“基于帧”是指异常处理程序是与特定过程（procedure）的激活关联在一起的。调用某个过程时，代表该过程激活的栈帧会被推送到栈上。一个栈帧可关联一个或多个异常处理程序，其中每个异常处理程序负责保护源程序中特定的代码块。当发生异常时，内核会搜索与当前栈帧关联的异常处理程序。如果没找到，则内核将搜索与上一个栈帧关联的异常处理程序，以此类推，直到找到基于帧的异常处理程序。如果依然未找到任何异常处理程序，则内核将调用自己的默认异常处理程序。

对于64位应用程序，结构化的异常处理并不使用基于帧的处理程序（基于帧的技术已被证明很容易受到恶意用户攻击）。相反，应用程序编译过程中，会在映像中放置一个表，其中包含每个函数的处理程序。内核会据此查找与每个函数关联的处理程序，这个过程使用了与上文介绍的32位代码处理方式相同的算法。

内核本身在内部大量使用了结构化异常处理，借此可以安全地确认来自用户模式的指针可以安全地执行读取或写入操作。驱动程序在处理运行I/O控制代码（IOCTL）时发送的指针也可以使用相同的技术。

另一种异常处理机制叫作矢量异常处理。仅用户模式应用程序可以使用该方法。有关该方法的详细信息可参阅 Windows SDK 或 Microsoft Docs：https://docs.microsoft.com/ windows/win32/debug/vectored-exception-handling。

当异常（无论是软件显式产生的异常还是硬件隐式产生的异常）发生时，会在内核中引发一系列连锁事件。CPU硬件会将控制权转交给内核陷阱处理程序，后者会（像出现中断时那样）创建一个陷阱帧。在异常解决后，陷阱帧使得系统能够从之前的位置恢复。陷阱处理程序还会创建异常记录，其中包含了出现异常的原因和其他相关信息。

如果异常出现在内核模式下，异常调度程序会直接调用例程来查找能处理该异常的、基于帧的异常处理程序。由于未处理的内核模式异常会被视为致命的操作系统错误，因此我们可以假设调度程序始终能找到异常处理程序。然而有些陷阱无法找到适合的异常处理程序，因为内核始终会假设此类错误是致命的，只有内核内部代码中非常严重的Bug或驱动程序代码中极严重的不一致问题（这只能通过故意修改底层系统代码导致，驱动程序不应对此负责）才会导致此类错误。此类致命错误会导致系统进行错误检查（Bug Check），并显示UNEXPECTED_KERNEL_MODE_TRAP错误代码。

如果异常出现在用户模式下，异常调度程序会以更精细的方式执行一些操作。Windows子系统有一个调试器端口（这实际上是一个调试器对象，下文很快将会介绍）和一个异常端口，可用在Windows进程中接收来自用户模式异常的通知（此处的“端口”是指ALPC端口对象，下文将详细介绍）。内核会使用这些端口进行默认的异常处理，如图8-24所示。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2258.png)

图8-24　异常的调度

调试器断点是异常的常见来源。因此，异常调度程序所执行的第一个操作就是查看引起异常的进程是否关联了调试器进程。如果是，异常调度程序则会向该进程关联的调试器对象发送一条调试器对象消息（在内部，系统会将其称为“端口”，这是为了兼容可能依赖Windows 2000中某些行为的程序，因为Windows 2000使用了LPC端口而非调试对象）。

如果进程未附加调试器进程，或者调试器无法处理该异常，那么异常调度程序会切换至用户模式，将该陷阱帧复制到格式为CONTEXT数据结构（详见Windows SDK）的用户栈中，并调用例程来查找结构化或矢量异常处理程序。如果未找到，或任何处理程序均无法处理该异常，异常分发程序则会重新切换回内核模式，并再次调用调试器，以便让用户进一步执行调试操作（这个过程也叫二次通知，即Second-chance notification）。

如果调试器未运行并且未找到用户模式的异常处理程序，内核会向与线程的进程相关联的异常端口发送一条消息。该异常端口（如果存在的话）是由控制该线程的环境子系统注册的。该异常端口使得（大概率正在侦听该端口的）环境子系统有机会将异常转换为与该环境相关的信号或异常。然而，如果内核在异常的处理过程中已经进行到这种程度，并且子系统并未处理异常，则内核会向Csrss（Client/Server Run-Time Subsystem，客户端/服务器运行时子系统）中用于Windows错误报告（WER，详见第10章）的系统范围内的错误端口发送一条消息，并执行默认异常处理程序，随后直接终止导致该异常的线程所属的进程。

#### 未经处理的异常

所有Windows线程都具备一个能处理“未经处理的异常”的异常处理程序。该异常处理程序是在Windows内部的Start-of-thread函数中声明的。当用户创建进程或任何额外的线程时，便会运行Start-of-thread函数。该函数可调用初始线程上下文结构中所指定的、由环境提供的线程启动例程，随后这个例程会进一步调用CreateThread所指定的、由用户提供的线程启动例程。

内部Start-of-thread函数的通用代码如下所示：

```
VOID RtlUserThreadStart(VOID) 
{ 
    LPVOID StartAddress = RCX; // Located in the initial thread context structure 
    LPVOID Argument = RDX; // Located in the initial thread context structure 
       LPVOID Win32StartAddr; 
    if (Kernel32ThreadInitThunkFunction != NULL) {
        Win32StartAddr = Kernel32ThreadInitThunkFunction; 
    } else {
        Win32StartAddr = StartAddress; 
    } 
    __try 
    { 
        DWORD ThreadExitCode = Win32StartAddr(Argument); 
        RtlExitUserThread(ThreadExitCode); 
    } 
    __except(RtlpGetExceptionFilter(GetExceptionInformation())) 
    { 
        NtTerminateProcess(NtCurrentProcess(), GetExceptionCode()); 
    } 
} 
```

请注意，如果线程包含自己无法处理的异常，则会调用Windows未经处理的异常过滤器。该函数的作用是提供系统定义的行为，以便当存在未经处理的异常时能够启动WerFault.exe进程。然而在默认配置下，第10章即将介绍的Windows Error Reporting（Windows错误报告）服务将处理该异常，因此这个未经处理的异常过滤器将永远不被执行。

实验：查看Windows线程的真实用户起始地址

每个Windows线程都在系统提供的函数（而非用户提供的函数）中开始执行，这个事实解释了为何系统中每个Windows进程的线程0（以及第二个线程）的起始地址都是相同的。我们可以使用Process Explorer或内核调试器查看用户提供的函数地址。

由于Windows进程中的大部分线程都始于系统提供的一个包装器函数（wrapper function），在显示进程中线程的起始地址时，Process Explorer会跳过代表该包装器函数的初始调用帧，并直接显示该栈中的第二个帧。例如，请注意下图所示的Notepad.exe进程的线程起始地址。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2270.png)

当显示调用栈时，Process Explorer并不会显示完整的调用层次结构。请注意在点击Stack按钮后显示的结果。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2277.png)

图中第20行是这个栈中的第一个帧，即内部线程包装器的起始位置。第二个帧（第19行）是环境子系统（本例中为Kernel32）的线程包装器，因为我们查看的是一个Windows子系统应用程序。第三个帧（第18行）则是Notepad.exe的主入口点。

要显示正确的函数名称，我们应该为Process Explorer配置合适的符号。为此首先需要安装调试工具，该工具已包含在Windows SDK或WDK中。随后应选择Options

菜单中的Configure Symbols菜单项。dbghelp.dll路径应指向调试器工具文件夹中的文件（通常为C:\Program Files\Windows Kits\10\Debuggers，但请注意，位于C:\Windows\ System32下的dbghelp.dll文件将无法工作），而Symbols路径也需要正确配置，以便从微软的符号存储库将符号下载到本地文件夹。具体配置如下图所示。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2285.png)

### 8.4.6　系统服务处理

如图8-24所示，内核的陷阱处理程序可以调度中断、异常和系统服务调用。8.4.5节已经介绍了中断和异常处理的工作过程，本节将介绍系统服务。系统服务的调度（见图8-25）是通过执行分配给系统服务调度机制的指令所触发的。Windows用于系统服务调度的指令取决于执行时使用的处理器，以及是否启用了虚拟机监控程序代码完整性（Hypervisor Code Integrity，HVCI），下文将介绍这些内容。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2292.png)

图8-25　系统服务调度

#### 架构性系统服务调度

在大部分x64系统中，Windows使用了Syscall指令，这会导致我们在本章中介绍的一些关键处理器状态产生变化，具体变化则取决于某些预编程的特殊模块寄存器（MSR）：

● 0xC0000081，也叫STAR（SYSCALL Target Address Register，SYSCALL目标地址寄存器）。

● 0xC0000082，也叫LSTAR（Long-Mode STAR，长模式STAR）。

● 0xC0000084，也叫SFMASK（SYSCALL Flags Mask，SYSCALL标记掩码）。

当遇到Syscall指令时，处理器会执行下列操作：

● 从STAR的第32位到第47位加载代码段（Code Segment，CS），Windows将其设置为0x0010（KGDT64_R0_CODE）。

● 从STAR的第32位到第47位加载栈段（Stack Segment，SS）并加上“8”，这就得到了0x0018（KGDT_R0_DATA）。

● 指令指针（Instruction Pointer，RIP）被保存到RCX中，并从LSTAR加载新值，如果未启用Meltdown（KVA影子）缓解措施，Windows会将其设置为KiSystemCall64，否则会设置为KiSystemCall64Shadow。（有关Meltdown漏洞的详情请参阅上文“硬件侧信道漏洞”一节。）

● 当前处理器标记（RFLAGS）被保存到R11中，随后使用SFMASK添加掩码，后者被Windows设置为0x4700（陷阱标记、方向标记、中断标记以及嵌套任务标记）。

● 栈指针（Stack Pointer，RSP）以及所有其他段（DS、ES、FS和GS）被保存在各自的当前用户空间值中。

因此，尽管指令的执行只占用极少量的处理器周期，但确实会让处理器处于不安全且不稳定的状态：因为用户模式的栈指针依然处于载入状态，GS依然指向了TEB，但Ring级别（也就是CPL）目前为“0”，从而会产生内核模式特权。Windows会快速做出反应，将处理器置于一致的操作环境中。除了可能在老式处理器上发生的与KVA影子有关的操作外，KiSystemCall64必须精确执行如下这些步骤：

● 通过使用swapgs指令，现在GS可以指向PCR，该过程在上文已进行了介绍。

● 当前栈指针（RSP）会被保存至PCR的UserRsp字段。由于现在GS已被正确加载，因此无须使用任何栈或寄存器即可完成该操作。

● 从PRCB（该结构会被保存为PCR的一部分）的RspBase字段加载新的栈指针。

至此内核栈已成功加载，该函数会使用上文介绍的格式构建一个陷阱帧。这个陷阱帧中包含了被设置为KGDT_R3_DATA（0x2B）的SegSs、来自PCR中UserRsp的Rsp、来自R11的Eflags、被设置为KGDT_R3_CODE（0x33）的SegCs，以及来自RCX的Rip。通常来说，处理器陷阱设置了这些字段，但是Windows必须根据syscall的运行模拟相关行为。

在从R10加载了RCX后，一般来说，会由x64 ABI要求将任意函数（包括Syscall）的第一个参数放置在RCX中，同时Syscall会要求使用调用方的指令指针重写RCX，这一点在上文中已经介绍了。Windows可以感知到这种行为，并会在发出syscall指令前将RCX复制到R10。很快我们将介绍这步操作会还原相关的值。

随后的操作与处理器的缓解措施有关，例如监管人模式访问保护（Supervisor Mode Access Prevention，SMAP）（此时会发出Stac指令）以及各种处理器侧信道缓解措施（比如会清空分支跟踪缓冲区（Branch Tracing Buffer，BTB）或返回存储缓冲区（Return Store Buffer，RSB））。此外，对于支持控制流强制技术（Control-flow Enforcement Technology，CET）的处理器，还必须正确地同步线程的影子栈（shadow stack）。除此之外，陷阱帧的其他元素也会被存储起来，例如各种非易失寄存器和调试寄存器，随后开始对系统调用进行非架构性的处理。下面将详细讨论这些内容。

然而，并非所有的处理器都是x64架构的，对于x86处理器还需要注意一些问题，例如，此时会使用一种名为Sysenter的指令。由于32位处理器已经越来越少见了，我们不准备花费太多篇幅详细介绍该指令，但值得一提的是，该指令的行为是较为类似的：从多种MSR中加载处理器的某些状态，随后由内核执行一些额外的工作，例如设置陷阱帧。更多信息可参阅Intel处理器的相关手册。类似地，ARM架构的处理器使用了Svc指令，该指令有着自己的行为和操作系统级别的处理方式，但目前这些处理器在Windows总装机量中都只占据了很小的比例。

Windows还必须处理另外一种情况：那些不具备基于模式的执行控制（Mode Base Execution Control，MBEC）功能的处理器，在启用虚拟机监控程序代码完整性（HVCI）的情况下，会由于设计上存在的问题导致无法做到HVCI所提供的承诺（第9章将介绍HVCI和MBEC）。具体来说，攻击者可以分配用户空间的可执行内存，而HVCI允许这种做法（通过将相应的SLAT项标记为可执行），进而导致PTE损坏（无法针对内核篡改提供保护），并让这块虚拟地址显示为内核页。在MMU看来，由于这个页面是内核页，监管人模式执行保护（Supervisor Mode Execution Prevention，SMEP）机制将无法禁止代码的执行，又因为该页面最初是以用户物理页的形式分配的，所以SLAT项也将无法禁止其执行。借此攻击者就可以随意执行任何内核模式代码，这违反了HVCI最基本的原则。

MBEC及其同类技术（受限用户模式，Restricted User Mode）通过在SLAT项的数据结构中引入不同的内核与用户可执行位解决了这个问题，以此可让虚拟机监控程序（或让安全内核通过VTL1特有的超调用）将用户页标记为“内核不可执行但用户可执行”。遗憾的是，在不具备该功能的处理器上，虚拟机监控程序别无选择，只能用陷阱捕获所有代码特权级别的变更，并在两组不同的SLAT项之间切换，其中一组会将所有用户物理页标记为不可执行，另一组会将其标记为可执行。虚拟机监控程序通过将IDT标记为空（借此可有效地将其限制设置为0）和以解码底层指令的方式来捕获CPL变更，这种操作的开销很大。然而，因为中断可以直接被虚拟机监控程序用陷阱捕获，从而避免了这些开销。因此，如果检测到启用HVCI的系统不具备MBEC这样的功能，那么用户空间中的系统调用调度代码往往会发出中断。共享用户数据（shared user data）结构中的SystemCall位（详见卷1第4章）将决定此时的具体处理。

因此，当SystemCall被设置为1时，x64 Windows会使用int 0x2e指令，这会产生一个陷阱，包括一个无须操作系统参与的、完整构建的陷阱帧。有趣的是，此时使用的指令与Pentium Pro（奔腾Pro）之前的早期x86处理器所用的指令完全相同。为了与有着三十年以上历史的旧软件（一些此类软件中已经通过硬编码方式写入了这样的指令）实现向后兼容性，x86系统依然支持这些指令。不过在x64系统中，只有在上述情况下可以使用0x2e，因为其他情况下内核并不会填充相关的IDT项。

无论最终使用哪种指令，用户模式系统调用调度代码时，始终会将系统调用索引存储在一个寄存器中（x86和x64为EAX，32位ARM为R12，ARM64为X8），我们将通过接下来介绍的非架构性系统调用处理代码进一步查看该索引。此外，为了进一步简化相关工作，标准函数调用处理器ABI（Application Binary Interface，应用程序二进制接口）是跨边界维护的，例如，x86系统中的参数会放置在栈上，而x64系统中的RCX（由于受Syscall行为的影响，从技术上来说其实应该是R10）、RDX、R8、R9会在该栈的基础上为这四者加上其他参数。

调度完成后，处理器该如何返回自己原先的状态呢？对于通过int 0x2e进行的基于陷阱的系统调用，将由iret指令根据栈上的硬件陷阱帧来还原处理器状态。不过对于Syscall和Sysenter，处理器分别通过名为Sysret和Sysexit的专用指令再次利用了我们之前在项上看到的MSR和硬编码寄存器。前者的具体行为如下所示：

● 从STAR的第48～63位加载栈段（Stack Segment，SS），Windows将其设置为0x0023（KGDT_R3_DATA）。

● 从STAR的第48～63位加载代码段（Code Segment，CS）并为其加上0x10，这就得到了0x0033（KGDT64_R3_CODE）。

● 从RCX加载指令指针（RIP）。

● 从R11加载处理器标记（RFLAGS）。

● 栈指针（RSP）和其他段（DS、ES、FS与GS）依然保持当前的内核空间值。

因此，与系统调用的进入一样，退出机制也必须清理一些处理器状态。也就是说，RSP会从我们之前分析过的进入代码恢复到保存在制造商硬件陷阱帧中的Rsp字段，其他所有保存的寄存器做法都是类似的。RCX寄存器将从保存的Rip加载，R11将从EFlags加载，swapgs指令会在发出sysret指令之前使用。由于DS、ES和FS从未被触及，因此它们依然可以维持各自最初的用户空间值。最后，EDX以及XMM0到XMM5会被归零，所有其他的非易失性寄存器会在执行sysret指令前从陷阱帧中还原。另外还会对Sysexit和ARM64的退出指令（eret）执行等效的操作。此外，如果启用了CET，那么与进入路径类似，在退出路径上，影子栈也必须执行正确的同步。

实验：定位系统服务调度程序

如上文所述，x64系统调用会基于一系列MSR进行，而这些MSR均可使用调试器命令rdmsr查看。首先请注意STAR，其中显示了KGDT_R0_CODE（0x0010）和KGDT64_R3_DATA（0x0023）。

```
lkd> rdmsr c0000081 
msr[c0000081] = 00230010`00000000 
```

随后即可开始调查LSTAR，接着就可以使用ln命令来查看它是否指向KiSystemCall64（对于不需要KVA影子的系统）或指向KiSystemCall64Shadow（对于需要KVA影子的系统）：

```
lkd> rdmsr c0000082 
msr[c0000082] = fffff804`7ebd3740 
　
lkd> ln fffff804`7ebd3740 
(fffff804`7ebd3740) nt!KiSystemCall64 
```

下面查看SFMASK，其中应包含我们之前介绍过的值：

```
lkd> rdmsr c0000084 
msr[c0000084] = 00000000`00004700 
```

x86系统调用通过Sysenter进行，并使用了一组不同的MSR，包括0x176，其中存储了32位系统调用处理程序：

```
lkd> rdmsr 176 
msr[176] = 00000000'8208c9c0 
　
lkd> ln 00000000'8208c9c0 
(8208c9c0)   nt!KiFastCallEntry 
```

在不具备MBEC但使用了HVCI的x86和x64系统中，可以使用调试器命令!idt 2e查看IDT中注册的int 0x2e处理程序：

```
lkd> !idt 2e 
　
Dumping IDT: fffff8047af03000 
2e:            fffff8047ebd3040 nt!KiSystemService 
```

还可以使用u命令反汇编KiSystemService或KiSystemCall64例程。对于中断处理程序，我们最终会注意到：

```
nt!KiSystemService+0x227: 
fffff804`7ebd3267 4883c408        add     rsp,8 
fffff804`7ebd326b 0faee8          lfence 
fffff804`7ebd326e 65c604255308000000 mov   byte ptr gs:[853h],0 
fffff804`7ebd3277 e904070000      jmp     nt!KiSystemServiceUser (fffff804`7ebd3980)
```

而MSR处理程序会落入下列内容：

```
nt!KiSystemCall64+0x227: 
fffff804`7ebd3970 4883c408        add     rsp,8 
fffff804`7ebd3974 0faee8          lfence 
fffff804`7ebd3977 65c604255308000000 mov   byte ptr gs:[853h],0 
nt!KiSystemServiceUser: 
fffff804`7ebd3980 c645ab02        mov      byte ptr [rbp-55h],2 
```

由此可以看到，最终所有代码路径都将抵达KiSystemServiceUser，并由它跨越所有处理器执行大部分通用操作，具体过程将在下一节详细介绍。

#### 非架构性系统服务调度

如图8-25所示，内核会使用系统调用编号，在系统服务调度表中定位系统服务信息。在x86系统中，该表类似于上文介绍过的中断调度表（interrupt dispatch table），只是其中的每一项都包含一个指向系统服务（而不是中断处理例程）的指针。在其他平台（包括32位的ARM和ARM64）上，该表的实现方式略有差异，并不是包含指向系统服务的指针，而是包含与表本身相关的偏移量。这种寻址机制更适合x64和ARM64应用程序二进制接口（ABI）和指令编码格式，也更符合ARM处理器的RISC本质特征。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　不同版本操作系统中的系统服务编号经常会发生变化，微软不仅会偶尔添加或删除系统服务，而且该表还经常会被随机化并乱序排列，这是为了让那些硬编码系统调用编号发起的攻击失效。 |
| ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |

无论什么架构，系统服务分发程序会在所有平台上执行一些通用操作：

● 将额外的寄存器（如调试寄存器或浮点寄存器）保存在陷阱帧中。

● 如果线程属于一个微进程（pico process），则将其转发给系统调用的Pico提供程序例程（有关Pico提供程序的详情，请参阅卷1第3章）。

● 如果线程是一个UMS调度的线程，则会调用KiUmsCallEntry以便与主线程（primary thread）同步（有关UMS的详细介绍请参阅卷1第1章）。对于UMS主线程，会在线程对象中设置UmsPerformingSyscall标记。

● 将系统调用的第一个参数存储到线程对象的FirstArgument字段，并将系统调用编号存储到SystemCallNumber。

● 调用共享的用户/内核系统调用处理程序（KiSystemServiceStart），由它将线程对象的TrapFrame字段设置为自己所存储的当前栈指针。

● 启用中断交付。

至此，该线程开始正式经历系统调用，其状态完全一致并且可以中断。接下来需要选择正确的系统调用表，并可能将线程升级为GUI线程，具体细节则取决于下一节将要介绍的线程对象中的GuiThread和RestrictedGuiThread字段。随后只要TEB的GdiBatchCount字段非零，就会对GUI线程执行GDI批处理操作。

系统调用调度程序必须将未通过寄存器（取决于CPU具体架构）传递的任何调用方参数从线程的用户模式栈复制到其内核模式栈。这是为了避免让每个系统调用手动复制参数（可能需要汇编代码和异常处理），并确保内核访问参数时用户无法更改这些参数。该操作在一个特殊的代码块中完成，异常处理程序可以识别该代码块，并将其与用户栈的复制关联在一起，这确保了在攻击者或存在Bug的程序扰乱用户栈后，内核依然不会崩溃。由于系统调用可以接受任意数量的参数（大部分情况下都是这样的），因此下一节将讨论内核如何知道要复制多少个参数。

这里需要注意，这些参数复制操作是浅层的：如果传递给系统服务的任何参数指向了用户空间中的缓冲区，则必须先探测是否能够安全地访问，随后内核模式的代码才能读取或写入该缓冲区。如果缓冲区被多次访问，则可能需要将其捕获或复制到本地的内核缓冲区中。该探测和捕获操作是由每个系统调用分别进行的，并非由处理程序负责。然而系统调用分发程序还必须执行一个关键操作：设置线程原本的模式（previous mode）。该模式的值可以是KernelMode或UserMode，当当前线程执行陷阱时，这个值必须实现同步，借此才可以识别传入异常、陷阱或系统调用的特权级别。因此，系统调用可以使用ExGetPreviousMode正确地处理用户和内核调用方。

调度程序的主体还要执行最后两步操作。首先，如果配置了DTrace并启用了系统调用跟踪，则会围绕系统调用来调用相应的进入/退出回调。或者如果启用了ETW跟踪但未启用DTrace，则会围绕系统调用记录相应的ETW事件。抑或DTrace或ETW均未启用，那么这个系统调用就不需要任何额外的逻辑。其次也是最后一步，还需要让PRCB中的KeSystemCalls变量递增，该变量是以性能计数器的形式展现的，我们可以通过性能和可靠性监视器监视该计数器。

至此，系统调用调度已完成，随后在系统调用退出过程中还将执行相反的步骤。这些步骤会酌情还原并复制用户模式的状态，按需处理用户模式APC的交付，处理与各种架构缓冲区有关的侧信道缓解措施，并最终根据具体平台返回相应的CPU指令。

#### 内核发出的系统调用调度

由于系统调用可通过用户模式代码和内核模式代码执行，因此，任何指针、处理程序以及行为均应该被视为来自用户模式，很明显这是不对的。

为了解决这个问题，内核会将这些调用导出为专用的Zw版本，也就是说，内核会导出为ZwCreateFile而非NtCreateFile。此外，由于Zw函数必须由内核手动导出，因此，只有微软希望供第三方使用的API才能导出。例如，ZwCreateUserProcess就无法按照名称导出，因为内核驱动程序不应该启动用户应用程序。这种导出的API实际上并非为相应的Nt版本简单创建的别名或包装器，它们是相应Nt系统调用的“蹦床”，并且使用了相同的系统调用调度机制。

与KiSystemCall64类似，它们也构建了一种假的硬件陷阱帧（在栈上推送CPU时收到来自内核模式的中断后生成的数据），并且与陷阱一样，它们也禁用了中断。例如在x64系统中，KGDT64_R0_CODE（0x0010）选择器会作为CS来推送，而当前内核栈会作为RSP来推送。每个这种“蹦床”会将系统调用编号放入相应的寄存器中（例如x86和x64系统中的EAX），再调用KiServiceInternal在陷阱帧中保存额外的数据，读取当前的“原本的模式”并将其保存在陷阱帧中，随后将“原本的模式”设置为KernelMode（这是一个重大的差异）。

#### 用户发出的系统调用调度

正如卷1第1章中介绍的那样，Windows执行体服务所用的系统服务调度指令位于系统库Ntdll.dll中。子系统DLL可调用Ntdll中的函数来实现自己的公开功能。但Windows USER和GDI函数（包括DirectX内核图形函数）属于例外，这些系统服务调度指令是在Win32u.dll中实现的，并未涉及Ntdll.dll。这两种情况如图8-26所示。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2426.png)

图8-26　系统服务调度

如图8-26所示，Kernel32.dll中的Windows WriteFile函数会导入并调用API-MS-Win- Core-File-L1-1-0.dll（这是一个MinWin重定向DLL，有关API重定向的详细信息请参阅卷1第3章）中的WriteFile函数，随后会调用KernelBase.dll（实际实现的位置）中的WriteFile函数。在检查与子系统有关的一些参数后，会调用Ntdll.dll中的NtWriteFile函数，该函数接下来执行相应指令来产生系统服务陷阱，并传递代表NtWriteFile函数的系统服务编号。

Ntoskrnl.exe中的系统服务调度程序（本例中为KiSystemService）会调用真正的NtWriteFile来处理I/O请求。对于Windows USER、GDI和DirectX内核图形函数，系统服务调度会在Windows子系统可加载的内核模式部分（Win32k.sys）调用该函数，随后可能会过滤系统调用或将其转发给相应的模块，例如桌面系统中的Win32kbase.sys或Win32kfull.sys，Windows 10X[3]系统中的Win32kmin.sys或DirectX调用中的Dxgkrnl.sys。

[3]Windows 10X是Windows 10时期，微软针对双屏幕设备（如双屏幕笔记本电脑，原本的键盘位置被另一块屏幕取代）开发的一种新操作系统。目前该项目已终止，但相关“遗产”已被融入Windows 11中（例如居中显示的开始菜单和任务栏按钮）。——译者注

#### 系统调用的安全性

由于内核中包含正确同步系统调用操作的“原本模式”所需的机制，因此每个系统调用服务都可以在处理过程中依赖这个值。上文曾经提到，这些函数必须首先探测指向任何类型用户模式缓冲区的任何参数。这里的“探测”是指：

1）确保该地址低于MmUserProbeAddress，即比最高的用户模式地址低64 KB（例如32位系统中的0x7FFF0000）。

2）确保该地址与调用方意图访问的数据边界对齐，例如Unicode字符为2字节，64位指针为8字节，以此类推。

3）如果缓冲区要用于输出，还需要确保当系统调用开始时，该缓冲区实际上是可写的。

请注意，输出缓冲区可能会在将来的任何时间点变为无效或只读，为了避免内核崩溃，系统调用必须始终使用本章上文介绍过的SEH访问输出缓冲区。出于类似原因，虽然系统不检查输入缓冲区的可读性（因为无论如何，输入缓冲区都可能被迫投入使用），但必须使用SEH来确保输入缓冲区可以被安全地读取。而SEH并不能防止无法对齐或野内核指针（wild kernel pointer）的情况，因此必须执行上文列出的前两个步骤。

很明显，对任何内核模式调用方进行上述第一项检查都会立即失败，而这也是“原本的模式”开始生效的第一个地方：对非用户模式的调用跳过探测操作，并假定所有缓冲区都是有效的、可读取的或根据需要可写入的。然而，这并非系统调用唯一需要执行的验证类型，因为可能还会出现其他比较危险的情况：

● 调用方可能提供了一个对象句柄。内核在引用对象时通常会绕过所有安全访问检查，并且内核还可以完整访问内核句柄（我们将在本章“对象管理器”一节详细介绍），但用户模式代码并不会这样做。“原本的模式”可用于通知对象管理器依然需要执行访问检查，因为该请求来自用户空间。

● 更复杂的情况下，驱动程序可使用诸如OBJ_FORCE_ACCESS_CHECK等标记来表明：尽管使用了Zw API（以此将原本的模式设置为KernelMode），但对象管理器依然需要像处理来自UserMode的请求那样对待该请求。

● 同理，调用方可能已经指定了一个文件名。在打开文件时，系统调用可能会使用IO_FORCE_ACCESS_CHECKING标记迫使安全引用监视器验证对文件系统的访问，这一点很重要，否则，诸如ZwCreateFile等调用有可能将“原本的模式”更改为KernelMode而绕过访问检查。如果驱动程序需要代表来自用户空间的IRP创建文件，同样也需要这样做。

● 对文件系统的访问也可能带来与符号链接或其他类型的重定向攻击有关的风险，此时高特权内核模式代码可能会错误地使用各种与特定进程有关或用户可访问的重分析点。

● 一般来说，对于使用Zw接口执行的任何会导致链式系统调用的操作都要注意，该操作会将“原本的模式”重置为KernelMode并酌情做出相应的响应。

#### 服务描述符表

上文曾经提到，在执行系统调用前，必须由用户模式或内核模式的“蹦床”首先将系统调用编号放入处理器寄存器（如RAX、R12或X8）中。从技术角度来看，该编号包含两个元素，如图8-27所示。第一个元素存储在低12位中，代表系统调用索引；第二个元素存储在接下来的2位（12～13）中，充当表标识符。很快我们将会介绍，借此内核即可实现最多四种不同类型的系统服务，每种服务都存储在一个表中，而每个表最多可容纳4096个系统调用。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2433.png)

图8-27　系统服务编号与系统服务之间的转换

内核会使用三个可能的数组跟踪系统服务表，这三个数组分别为KeServiceDescriptorTable、KeServiceDescriptorTableShadow以及KeServiceDescriptorTableFilter。每个数组最多包含两个项，其中存储了下列三类数据：

● 一个指向该服务表所实现的系统调用数组的指针。

● 该服务表中包含的系统调用数量，也称Limit（限制）。

● 一个指向该服务表中每个系统调用对应的参数字节数组的指针。

第一个数组中始终只有一项，指向了KiServiceTable和KiArgumentTable，其中可包含略多于450个系统调用（具体数量取决于Windows版本）。默认情况下，所有线程都会发出仅访问该表的系统调用。在x86系统中，这是由线程对象中的ServiceTable指针强制执行的，其他所有平台则会将符号KeServiceDescriptorTable硬编码到系统调用调度程序中。

当线程发出的系统调用首次超过限制时，内核会调用PsConvertToGuiThread，由此向Win32k.sys中的USER和GDI服务告知该线程的情况，并在成功返回后设置线程对象的GuiThread标记或RestrictedGuiThread标记。具体设置哪个标记取决于是否启用了EnableFilteredWin32kSystemCalls进程缓解选项（有关该选项的详细介绍请参阅卷1第7章）。在x86系统中，取决于具体设置了哪个标记，随后线程对象的ServiceTable指针将会指向KeServiceDescriptorTableShadow或KeServiceDescriptorTableFilter，其他平台上则在每个系统调用时选择一个硬编码的符号（虽然会对性能产生些许影响，但后一种方式可避免产生容易被恶意软件滥用的挂钩点）。

大家可能已经猜到，其他数组中包含了第二个项，该项代表了在Windows子系统Win32k.sys的内核模式部分所实现的Windows USER和GDI服务。在较新版本的Windows中，该项还代表了由Dxgkrnl.sys实现的DirectX内核子系统服务，不过最初这些服务是通过Win32k.sys传输的。第二项会分别指向W32pServiceTable或W32pServiceTableFilter，以及W32pArgumentTable或W32pArgumentTableFilter，这取决于Windows版本，可包含大约1250个或更多的系统调用。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　内核并不链接Win32k.sys，因此会导出一个KeAddSystemServiceTable函数，以便在尚未填写KeServiceDescriptorTableShadow和KeServiceDescriptorTableFilter表时能够向这些表中添加额外的项。如果Win32k.sys已经调用了这些API，该函数将会失效，并且一旦调用该函数，PatchGuard就会去保护数组，最终使其结构变为只读状态。 |
| ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

Filter项之间唯一的实质性区别在于，它们会使用诸如stub_UserGetThreadState这样的名称指向Win32k.sys中的系统调用，但实际的数组会指向NtUserGetThreadState。前者的存根（Stub）在部分情况下会根据已为进程加载的过滤器集来检查是否为该系统调用启用了Win32k.sys过滤。根据检查结果，如果过滤器集明确禁止，则调用会失败并返回STATUS_ INVALID_SYSTEM_SERVICE，或最终调用原始函数（例如NtUserGetThreadState），这种情况下如果启用了审核，则可能还会返回遥测结果。

另一方面，参数表可以帮助内核了解要将多少个栈字节从用户栈复制到内核栈，具体过程详见上文的“调度”一节。参数表中的每个项均对应具备该索引并且匹配的系统调用，其中还存储了要复制的字节数量（最多255字节）。然而，x86系统之外其他所有平台的内核还采用了一种名为系统调用表压缩（system call table compaction）的机制，该机制可将调用表中的系统调用指针与参数表中的字节数组合成一个值。该功能的工作原理如下。

1）获取系统调用函数指针，并从系统调用表本身开头处开始计算32位差值。由于该表是一个全局变量，位于包含了这些函数的同一个模块内，因此，±2 GB 的范围应该足够了。

2）从参数表中获取栈字节数并将其除以4，以此将其转换为参数数量（某些函数可能采用8字节参数，但从目的的角度考虑，它们将被直接视为两个“参数”）。

3）将第1步得到的32位差值左移4位，最终使其成为28位差值（再次提醒，这样做没问题，因为没有内核组件会大于256 MB）并执行按位或运算以添加第2步得到的参数数量。

4）使用第3步获得的值重写系统调用函数指针。

这种优化方式虽然乍看起来并不好，但实际上有很多优点：通过避免在系统调用过程中在两个不同数组中查找减少了缓存的使用，减少了指针取消引用操作的数量，可充当一个混淆层进而使得更难以针对系统调用表进行挂钩或修补操作，同时也让PatchGuard可以更容易地保护系统调用表。

实验：将系统调用编号映射为函数和参数

我们可以重现内核在处理系统调用ID时所进行的查找，以此了解哪个函数负责处理该过程以及总共需要多少个参数。在x86系统中，我们可以直接用调试器通过dps命令转储每个系统调用表（如KiServiceTable），“dps”代表dump pointer symbol（转储指针符号），该命令可以代替我们进行查找。此外，也可通过db（dump bytes，转储字节）命令转储KiArgumentTable（或Win32k.sys中的任何系统调用表）。

不过根据上文介绍过的编码方式，更有趣的练习是在ARM64或x64系统中转储这些数据。为此请执行如下操作。

1）只要撤销上文介绍过的压缩操作，即可转储特定的系统调用。获取基准表并将其添加至所需索引中存储的28位偏移量，如下所示，其中内核系统表中的系统调用3会显示为NtMapUserPhysicalPagesScatter：

```
lkd> ?? ((ULONG)(nt!KiServiceTable[3]) >> 4) + (int64)nt!KiServiceTable 
unsigned int64 0xfffff803`1213e030 
　
lkd> ln 0xfffff803`1213e030 
(fffff803`1213e030) nt!NtMapUserPhysicalPagesScatter 
```

2）通过获取4位的参数数量，即可看到该系统调用所接收的基于栈的4字节参数数量：

```
lkd> dx (((int*)&(nt!KiServiceTable))[3] & 0xF) 
(((int*)&(nt!KiServiceTable))[3] & 0xF) : 0 
```

3）请注意，这并不意味着该系统调用没有参数。因为这是x64系统，调用可以接受0～4之间任意数量的参数，而所有参数都位于寄存器（RCX、RDX、R8和R9）中。

4）我们还可通过调试器数据模型，使用投射创建LINQ谓词并转储整个表，因为KiServiceLimit变量对应了服务描述符表中相同的限制字段（正如影子描述符表中Win32k.sys的W32pServiceLimit项）。输出结果应类似如下所示：

```
lkd> dx @$table = &nt!KiServiceTable 
@$table = &nt!KiServiceTable : 0xfffff8047ee24800 [Type: void *] 
　
lkd> dx (((int(*)[90000])&(nt!KiServiceTable)))->Take(*(int*)&nt!KiServiceLimit)->
     Select(x => (x >> 4) + @$table) 
(((int(*)[90000])&(nt!KiServiceTable)))->Take(*(int*)&nt!KiServiceLimit)->Select
     (x => (x >> 4) + @$table)
   [0]              : 0xfffff8047eb081d0 [Type: void *] 
   [1]              : 0xfffff8047eb10940 [Type: void *] 
   [2]              : 0xfffff8047f0b7800 [Type: void *] 
   [3]              : 0xfffff8047f299f50 [Type: void *] 
   [4]              : 0xfffff8047f012450 [Type: void *] 
   [5]              : 0xfffff8047ebc5cc0 [Type: void *] 
   [6]              : 0xfffff8047f003b20 [Type: void *] 
```

5）我们还可以使用该命令更复杂的版本将指针转换为对应的符号形式，本质上，这等于重新实现了适用于x86 Windows的dps命令：

```
lkd> dx @$symPrint = (x => Debugger.Utility.Control.ExecuteCommand(".printf \"
    %y\\n\"," + 
    ((unsigned __int64)x).ToDisplayString("x")).First()) 
@$symPrint = (x => Debugger.Utility.Control.ExecuteCommand(".printf \"%y\\n\"," +
((unsigned __int64)x).ToDisplayString("x")).First()) 
　
lkd> dx (((int(*)[90000])&(nt!KiServiceTable)))->Take(*(int*)&nt!KiServiceLimit)->Select
    (x => @$symPrint((x >> 4) + @$table)) 
(((int(*)[90000])&(nt!KiServiceTable)))->Take(*(int*)&nt!KiServiceLimit)->Select(x =>
@$symPrint((x >> 4) + @$table)) 
    [0]              : nt!NtAccessCheck (fffff804`7eb081d0) 
    [1]              : nt!NtWorkerFactoryWorkerReady (fffff804`7eb10940) 
    [2]              : nt!NtAcceptConnectPort (fffff804`7f0b7800) 
    [3]              : nt!NtMapUserPhysicalPagesScatter (fffff804`7f299f50) 
    [4]              : nt!NtWaitForSingleObject (fffff804`7f012450) 
    [5]              : nt!NtCallbackReturn (fffff804`7ebc5cc0) 
```

6）如果只对内核的服务表感兴趣，但对Win32k.sys项不感兴趣，也可以使用调试器的!chksvctbl -v命令，让输出结果包含所有这些数据，同时以此检查可能被Rootkit附加的内联挂钩：

```
lkd> !chksvctbl -v 
#    ServiceTableEntry        DecodedEntryTarget(Address)               CompactedOffset
=======================================================================================
0    0xfffff8047ee24800                 nt!NtAccessCheck(0xfffff8047eb081d0) 0n-52191996
1    0xfffff8047ee24804    nt!NtWorkerFactoryWorkerReady(0xfffff8047eb10940) 0n-51637248
2    0xfffff8047ee24808           nt!NtAcceptConnectPort(0xfffff8047f0b7800) 0n43188226
3    0xfffff8047ee2480c nt!NtMapUserPhysicalPagesScatter(0xfffff8047f299f50) 0n74806528
4    0xfffff8047ee24810         nt!NtWaitForSingleObject(0xfffff8047f012450) 0n32359680
```

实验：查看系统服务活动

我们可以通过观察System对象的System Calls/Sec性能计数器来监视系统服务活动。打开性能监视器，点击“**监视工具**”下的“**性能监视器**”，随后点击“**添加**”按钮将计数器添加到图表即可。请选择System对象，选中System Calls/Sec计数器，随后点击“**添加**”按钮将其加入图表。

我们可能还要增大图表的最大值，因为系统中的常态是每秒进行数十万个调用，系统配备的处理器越多，调用数量就越多。下图显示了这些数据在本书作者的计算机上所呈现的样子。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2515.png)

## 8.5　WoW64（Windows-on-Windows）

WoW64（64位Windows中模拟的Win32环境）是指用于在64位平台（属于不同的CPU架构）上执行32位应用程序的软件。WoW64最初是一个研究项目，旨在让旧版Windows NT 3.51的Alpha和MIPS版本能够运行x86代码。自那时（1995年前后）起，该技术经历了巨大的变化。当微软公司于2001年发布64位Windows XP版本时，WoW64就已包含在该系统中，借此即可用新的64位操作系统运行旧的x86 32位应用程序。在现代Windows版本中，WoW64通过进一步扩展，已经可以支持在ARM64系统中运行ARM32和x86应用程序。

WoW64核心以一系列用户模式DLL的形式实现，并由内核提供部分支持，进而创建出通常只有64位原生数据结构才会包含的目标架构版本，例如处理器环境块（Process Environment Block，PEB）和线程环境块（Thread Environment Block，TEB）。内核还实现了通过Get/SetThreadContext更改WoW64上下文的功能。负责WoW64的核心用户模式DLL包括以下几方面：

● Wow64.dll：在用户模式下实现了WoW64核心。它所创建的精简的软件层可充当32位应用程序的一种中间内核，并可以此为基础进行仿真模拟。它还可处理CPU上下文状态更改以及由Ntoskrnl.exe导出的基础系统调用，并负责实现文件系统重定向和注册表重定向。

● Wow64win.dll：为Win32k.sys导出的GUI系统调用实现了形式转换（thunking）。Wow64win.dll和Wow64.dll均包含形式转换代码，可将与调用有关的约定从一种架构转换为另一种架构。

其他模块是特定架构专用的，主要用于对隶属于不同架构的机器代码进行转换。某些情况下（如ARM64），机器代码需要进行模拟或实时编译（jitting）。本书中我们将使用“jitting”这个词代表即时编译（just-in-time compilation）技术，该技术可在运行过程中编译一小块代码（名叫“编译单元”），而无须每次模拟并执行一条指令。

机器代码的转换、模拟或实时编译主要由下列DLL负责，随后这些代码即可在目标操作系统中运行：

● Wow64cpu.dll：实现了在AMD64操作系统中运行x86 32位代码的CPU模拟器，负责管理WoW64中每个运行中线程的32位CPU上下文，为从32位到64位（以及反向）的CPU模式切换提供处理器架构支持。

● Wowarmhw.dll：实现了在ARM64系统中运行ARM32（AArch32）应用程序的CPU模拟器，这实际上是与x86系统中Wow64cpu.dll等效的ARM64组件。

● Xtajit.dll：实现了在ARM64系统中运行x86 32位应用程序的CPU模拟器。其中包含一个完整的x86模拟器、一个实时编译器（负责编译代码），以及实时编译器与XTA缓存服务器之间的通信协议。实时编译器可创建编译块，其中包含从x86映像转换后的ARM64代码。这些编译块会存储在本地缓存中。

WoW64用户模式库以及其他核心WoW64组件之间的关系如图8-28所示。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2527.png)

图8-28　WoW64架构

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　针对安腾（Itanium）架构计算机设计的老版本Windows包含了一个集成在WoW64层中的完整x86模拟器，名为Wowia32x.dll。安腾处理器无法以高效的方式原生执行x86 32位指令，因此需要模拟器介入。安腾架构已于2019年1月正式退役。较新的Windows Insider版还支持在ARM64系统上执行64位x86代码，微软针对此行为还设计了一套全新的实时编译器。然而，在ARM系统中模拟AMD64代码并非通过WoW64进行的。AMD64模拟器架构的相关介绍已超出了本书的内容范围。 |
| ------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

### 8.5.1　WoW64核心

正如上一节所述，WoW64核心是独立于平台的：它创建了一个软件层，借此可管理32位代码在64位操作系统中的执行。实际的转换工作由特定于具体平台的另一个名为模拟器（simulator，也叫二进制转换器）的组件负责。本节将讨论WoW64核心的作用及其与模拟器互操作的方式。虽然WoW64的核心几乎完全在用户模式下实现（位于Wow64.dll库中），但其中也有一部分位于NT内核中。

#### NT内核中的WoW64核心

在系统启动（阶段1）过程中，I/O管理器会调用PsLocateSystemDlls例程，借此将系统可支持的所有系统DLL映射至System进程用户地址空间（并将其基址存储在一个全局数组中）。其中还包含WoW64版本的Ntdll，如表8-13所示。在进程管理器（PS）开始启动的阶段2期间，会解析内部内核变量中所存储的DLL的某些入口点。其中的一个导出项LdrSystemDllInitBlock用于将WoW64信息和函数指针传递给新的WoW64进程。

表8-13　不同的Ntdll版本列表

| 路径                            | 内部名称         | 描述                                         |
| ----------------------------- | ------------ | ------------------------------------------ |
| c:\windows\system32\ntdll.dll | Ntdll.dll    | 系统Ntdll会映射至每个用户进程（最小进程除外），这也是唯一标记为“必需”的版本  |
| c:\windows\SysWow64\ntdll.dll | Ntdll32.dll  | 32位x86 Ntdll会映射至64位x86主机系统中运行的WoW64进程      |
| c:\windows\SysArm32\ntdll.dll | Ntdll32.dll  | 32位ARM Ntdll会映射至64位ARM主机系统中运行的WoW64进程      |
| c:\windows\SyChpe32\ntdll.dll | Ntdllwow.dll | 32位x86 CHPE Ntdll会映射至64位ARM主机系统中运行的WoW64进程 |

当进程最初被创建时，内核会使用一种算法来决定该进程是否可以在WoW64下运行，该算法会分析主进程是否可执行PE映像，并检查系统中是否映射了正确版本的Ntdll。如果系统确定该进程是WoW64进程，当内核初始化其地址空间时，就会同时映射原生版本的Ntdll和正确的WoW64版本Ntdll。

正如卷1第3章所述，每个非最小进程都有一个可从用户模式访问的PEB数据结构。对于WoW64进程，内核也会分配32位版本的PEB，并将指向它的指针存储在一个小型数据结构（EWoW64PROCESS）中，该数据结构会链接到代表新进程的主EPROCESS结构。随后内核会填充由32位版本的LdrSystemDllInitBlock符号所描述的数据结构，包括由Wow64 Ntdll导出的指针。

在为进程分配线程时，内核会经历类似的过程：除了线程的初始用户栈（其初始大小可通过主映像的PE头指定），还要分配执行32位代码所需的另一个栈。这个新栈也叫线程的WoW64栈。在为线程构建TEB时，内核会分配足够容量的内存，以便同时存储64位TEB以及随后的32位TEB。

此外，在基础的64位栈之上还会分配一个小型数据结构（名为WoW64 CPU Area Information，WoW64 CPU区域信息）。后者包含目标映像机器标识符、一个与平台相关的32位CPU上下文（X86_NT5_CONTEXT或ARM_CONTEXT数据结构，具体取决于目标架构），以及一个指向每线程WoW64 CPU共享数据的指针，这些内容都可被模拟器使用。指向这个小型数据结构的指针还会存储在线程的TLS插槽1中，以供二进制转换器快速引用。图8-29展示了只包含一个初始单线程的WoW64进程的最终配置。

#### 用户模式WoW64核心

除了上一节描述的各种差异外，对于非WoW64进程，进程及其初始线程的诞生方式完全相同，但从主线程调用原生版本Ntdll中的加载器初始化函数LdrpInitialize并开始执行的那一刻起，情况开始发生变化。当检测到该线程是新进程的上下文中第一个开始执行的线程后，加载器会调用进程初始化例程LdrpInitializeProcess，并结合其他多个因素（详情请参阅卷1第3章“进程初始化的早期工作”一节）来判断该进程是否为WoW64进程，而具体依据为检查是否存在32位TEB（位于原生TEB之后，会与原生TEB链接在一起）中。如果检查发现存在32位TEB，那么原生Ntdll会将内部全局变量UseWoW64设置为1，进而构建WoW64核心库（wow64.dll）的路径，并将其映射至4 GB虚拟地址空间限制之上的位置（这样就不会干扰为该进程模拟的32位地址空间）。随后Ntdll会获取负责处理进程/线程挂起、APC与异常调度的WoW64函数的地址，并将该地址存储在某些内部变量中。

![](../res/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tx2631.png)

图8-29　只包含一个线程的WoW64进程的内部配置

当进程初始化例程结束后，Windows加载器会通过导出的Wow64LdrpInitialize例程将执行过程转换至WoW64核心，随后永远不会返回。至此，每个新线程都将通过该入口点启动（而无须使用传统的RtlUserThreadStart）。WoW64核心会在TLS插槽1处获得指向内核存储的CPU WoW64区域的指针。如果该线程是进程中的第一个线程，则会调用WoW64进程初始化例程，该例程会执行如下操作：

1）尝试加载WoW64 Thunk Logging DLL（wow64log.dll）。该DLL用于记录WoW64调用，但并未包含在商业版的Windows版本中，因此可直接跳过。

2）通过NT内核填充的LdrSystemDllInitBlock查找Ntdll32基址和函数指针。

3）初始化文件系统和注册表重定向。文件系统和注册表重定向是在WoW64核心的Syscall层实现的，可拦截32位注册表和文件系统的请求，转换其路径，随后再调用原生的系统调用。

4）初始化WoW64服务表，该表中包含指向NT内核与Win32k GUI子系统所属系统服务的指针（类似于标准内核系统服务），并包含Console与NLS服务（均为WoW64系统服务调用，本章下文将介绍重定向）。

5）填充NT内核为该进程分配的32位版本的PEB，并根据进程主映像架构加载正确的CPU模拟器。系统会查询HKLM\SOFTWARE\Microsoft\Wow64\<arch>键的“默认”注册表值（其中的<arch>可以是x86或arm，这取决于目标架构），该值包含模拟器的主DLL名称。随后将模拟器载入并映射至进程的地址空间。模拟器主DLL的部分导出函数经过解析会存储在一个名为BtFuncs的内部数组中。该数组是将与平台相关的二进制转换器及WoW64子系统链接在一起的关键：WoW64仅通过它调用模拟器的函数。例如，BtCpuProcessInit函数就代表了模拟器的进程初始化例程。

6）形式转换跨进程机制通过分配并映射一个16 KB的共享内存节来完成初始化。当一个WoW64进程调用一个以另一个32位进程为目标的API时（该操作会在不同进程之间传播形式转换操作），会产生一个合成的工作项。

7）WoW64层会（通过调用导出的BtCpuNotifyMapViewOfSection）通知模拟器主模块以及32位版本的Ntdll已被映射至地址空间。

8）WoW64核心会将指向32位系统调用调度程序的指针存储在32位版本Ntdll导出的Wow64Transition变量中，这样系统调用调度程序就可以正常工作了。

当进程初始化例程运行完毕时，线程就准备好开始进行CPU模拟了。线程会调用模拟器的线程初始化函数并准备一个全新的32位上下文，并转换最初由NT内核填充的64位上下文。最后，还会根据新的上下文准备32位栈，以便执行32位版本LdrInitializeThunk函数。模拟操作是通过模拟器的BTCpuSimulate导出函数启动的，该函数永远不会返回至调用方（除非模拟器中发生严重错误）。

### 8.5.2　文件系统重定向

为了维持兼容性，并减少从Win32向64位Windows移植应用程序的工作量，不同版本的系统目录名称是完全一致的。因此\Windows\System32文件夹中包含了原生的64位映像。WoW64在拦截所有系统调用时，会对与路径有关的所有API进行转换，并将多种系统路径替换为WoW64的等价路径（主要取决于目标进程的架构），具体如表8-14所示。该表还列出了通过使用系统环境变量进行重定向的路径（例如%PROGRAMFILES%变量会将32位应用程序设置为\Program Files (x86)，会将64位应用程序设置为\Program Files文件夹）。

表8-14　WoW64重定向的路径

| 路径                           | 架构                                                            | 重定向后的位置                         |
| ---------------------------- | ------------------------------------------------------------- | ------------------------------- |
| c:\windows\system32          | x86 on AMD64                                                  | C:\Windows\SysWow64             |
| x86 on ARM64                 | C:\Windows\SyChpe32（或C:\Windows\SysWow64，如果Sychep32中不存在目标文件夹） |                                 |
| ARM32                        | C:\Windows\SysArm32                                           |                                 |
| %ProgramFiles%               | Native                                                        | C:\Program Files                |
| x86                          | C:\Program Files (x86)                                        |                                 |
| ARM32                        | C:\Program Files (Arm)                                        |                                 |
| %CommonProgramFiles%         | Native                                                        | C:\Program Files\Common Files   |
| x86                          | C:\Program Files (x86)                                        |                                 |
| ARM32                        | C:\Program Files (Arm)\Common Files                           |                                 |
| C:\Windows\regedit.exe       | x86                                                           | C:\Windows\SysWow64\regedit.exe |
| ARM32                        | C:\Windows\SysArm32\regedit.exe                               |                                 |
| C:\Windows\LastGood\System32 | x86                                                           | C:\Windows\LastGood\SysWow64    |
| ARM32                        | C:\Windows\LastGood\SysArm32                                  |                                 |

出于兼容性和安全性方面的原因，\Windows\System32的几个子目录不受重定向的影响，这样32位应用程序对它们的访问实际上会直接访问这些子目录本身。这些不被重定向的子目录包括：

● %windir%\system32\catroot和%windir%\system32\catroot2

● %windir%\system32\driverstore

● %windir%\system32\drivers\etc

● %windir%\system32\hostdriverstore

● %windir%\system32\logfiles

● %windir%\system32\spool

最后，WoW64还提供了一种机制，借此可通过Wow64DisableWow64FsRedirection与Wow64RevertWow64FsRedirection函数，以每个线程为基础控制内置于WoW64中的文件重定向。该机制会在TLS的“索引8”处存储启用/禁用值，WoW64的内部RedirectPath函数会参考该值。不过该机制可能会让延迟加载的DLL产生一些问题（例如通过通用文件对话框打开文件甚至在软件的国际化方面），因为一旦禁用了重定向，系统在内部加载期间也将不再使用重定向，这会导致某些仅64位的文件面临无法找到的情况。对开发者而言，此时一种更安全的方法是使用%SystemRoot%\Sysnative路径，或者上文提及的那些始终保持一致的路径。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　由于某些32位应用程序可能确实需要能够感知并处理64位映像，此时可让源自32位应用程序的任何I/O访问虚拟目录\Windows\Sysnative，以避免被文件重定向。该目录实际上并不存在，这是一个虚拟路径，可供应用程序（即使是WoW64下运行的应用程序）访问真正的System32目录。 |
| ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |

### 8.5.3　注册表重定向

应用程序和组件会将自己的配置数据存储在注册表中。组件通常会在安装过程中的注册环节将自己的配置数据写入注册表。如果同一个组件分别被安装并注册为32位和64位二进制文件，那么后注册的组件将覆盖先注册的组件，因为它们会写入注册表的同一个位置。

为了在无须更改32位组件代码的情况下以透明的方式解决此问题，注册表被分为两部分：Native和WoW64。默认情况下，32位组件会访问注册表的32位视图，64位组件会访问注册表的64位视图。这就为32位和64位组件提供了一种安全的执行环境，并能将32位应用程序的状态与64位应用程序的状态（如果存在的话）分隔开。

正如下文“系统调用”中将要介绍的那样，WoW64系统调用层可拦截由32位进程发出的所有系统调用。当WoW64拦截可以打开或创建注册表键的注册表系统调用时，它会将键路径转换为指向注册表的WoW64视图（除非调用方明确要求访问64位视图）。借助多种树状数据结构，WoW64可跟踪重定向后的注册表键，这些树状数据结构中存储了共享的和拆分的注册表键与子键列表（锚点树节点定义了系统该从什么位置开始重定向）。WoW64会在下列这些位置重定向注册表：

● HKLM\SOFTWARE

● HKEY_CLASSES_ROOT

并非注册表的上述整个根配置单元（Hive）都是拆分的。属于这些根键的子键可以存储在注册表中私有的WoW64部分内（此时的子键就是一种拆分键）。否则子键可在32位和64位应用程序之间共享（此时的子键是一种共享键）。在锚节点所跟踪的每个拆分键下，WoW64会创建一个名为WoW6432Node（针对x86应用程序）或WowAA32Node（针对ARM32应用程序）的键。该键中存储了32位配置信息。注册表的所有其他部分（例如HKLM\SYSTEM）均是32位和64位应用程序共享的。

作为一种额外措施，如果x86 32位应用程序向注册表写入以数据“%ProgramFiles%”或“%CommonProgramFiles%”开头的REG_SZ或REG_EXPAND_SZ值，WoW64会将实际的值改为“%ProgramFiles(x86)%”和“%CommonProgramFiles(x86)%”，以便匹配文件系统重定向以及上文介绍的相关布局。但为了符合这种情况，32位应用程序必须严格写入上述这些字符串，其他任何数据都会被忽略并正常写入。

对于需要将注册表键明确指定为某种视图的应用程序，可以为RegOpenKeyEx、RegCreateKeyEx、RegOpenKeyTransacted、RegCreateKeyTransacted以及RegDeleteKeyEx函数设置下列标记：

● KEY_WoW64_64KEY：从32位或64位应用程序中明确打开64位键，禁用上文提到的REG_SZ或REG_EXPAND_SZ拦截措施。

● KEY_WoW64_32KEY：从32位或64位应用程序中明确打开32位键。

### 8.5.4　AMD64平台上的x86模拟

AMD64平台上的x86模拟器（Wow64cpu.dll）接口相当简单。模拟器进程初始化函数会根据是否存在软件MBEC（Mode Based Execute Control，基于模式的执行控制，详见第9章）而启用快速系统调用接口。当WoW64核心通过调用模拟器的接口BtCpuSimulate开始模拟时，模拟器会（根据WoW64核心提供的32位CPU上下文）构建WoW64栈帧，为快速系统调用的调度初始化Turbo形式转换数组，并准备FS段寄存器使其指向线程的32位TEB。最后，它还会设置一个以32位段（通常是0x20段）为目标的调用门（call gate），切换栈，并发起到最终32位入口点的远跳（首次执行时，入口点会设置为32位版本的LdrInitializeThunk加载器函数）。当CPU执行该远跳时，会检测到调用门的目标为一个32位段，因此会将CPU执行模式改为32位。只有在调度了中断或系统调用后，代码的执行才会退出32位模式。有关调用门的详细信息请参阅Intel与AMD的软件开发手册。

| ![](https://cdn.ptpress.cn/pubcloud/5B0A982E/ushu/UBda647ada3435/online/FBOLda82494f5f9f/Images/tu.png) | **注意**　当首次切换至32位模式时，模拟器会使用IRET操作码而不进行远调用（far call）。这是因为所有32位寄存器，包括易失性寄存器和EFLAGS都需要初始化。 |
| ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |

#### 系统调用

对于32位应用程序，WoW64层的行为与NT内核本身类似：特殊的32位版Ntdll.dll、User32.dll以及Gdi32.dll均位于\Windows\Syswow64文件夹中（这里还有其他负责进程间通信的DLL，例如Rpcrt4.dll）。当一个32位应用程序需要操作系统的协助时，会直接调用这些位于特殊的32位版操作系统库中的函数。与相应的64位版等价物类似，操作系统例程可以直接在用户模式下执行自己的任务，或者也可以请求NT内核的协助。在后一种情况下，需要通过存根（stub）函数（例如常规64位Ntdll中实现的函数）调用系统调用。存根会将系统调用索引放入一个寄存器中，但存根并不发出原生的32位系统调用指令，而是会（通过WoW64核心所编译的Wow64Transition变量）调用WoW64系统调用调度程序。

WoW64系统调用调度程序是在与特定平台相关的模拟器（wow64cpu.dll）中实现的。它会发出另一个远跳以便转换至原生64位执行模式，随后从模拟中退出。二进制转换器会将栈切换至64位模式并保存CPU原本的上下文。随后会捕获与系统调用相关的参数并对其进行转换。这种转换过程也叫“形式转换”（thunking），借此通过32位ABI执行的机器代码就可以与64位代码实现互操作。调用过程的相关约定（由ABI描述）定义了数据结构、指针和值在每个函数参数中传递的方法以及通过机器代码访问的方法。

模拟器中的形式转换主要通过两种策略执行。对于无须与客户端所提供的复杂数据结构进行交互操作（但需要处理简单的输入/输出值）的API，将由Turbo形式转换（模拟器中实现的一种小型转换例程）负责转换并直接调用原生64位API。其他复杂的API需要Wow64SystemServiceEx例程的协助，由该例程从系统调用索引中提取正确的WoW64系统调用编号，并调用正确的WoW64系统调用函数。WoW64系统调用是在WoW64核心库和Wow64win.dll中实现的，与原生系统调用同名，但名称包含“wh-”前缀（例如NtCreateFile这个WoW64 API可通过whNtCreateFile调用）。

正确完成转换后，模拟器会发出相应的原生64位系统调用。当原生系统调用返回后，WoW64会在必要时对任何输出参数进行转换或形式转换，将其从64位格式转换为32位格式，并重新启动模拟过程。

#### 异常调度

与WoW64系统调用类似，异常调度也会迫使CPU退出模拟。当发生异常时，NT内核会确定该异常是否由执行用户模式代码的线程所产生。如果是，NT内核会在活跃栈上构建一个扩展的异常帧，并通过返回到64位Ntdll中的用户模式KiUserExceptionDispatcher函数来调度该异常。

请注意，异常产生时，64位异常帧（其中包含捕获的CPU上下文）会被分配到当时处于活动状态的32位栈中。因此需要在调度到CPU模拟器之前对其进行转换。这正是Wow64PrepareForException函数（由WoW64核心库导出）所起的作用：在原生64位栈上分配空间，并将原生异常帧从32位栈复制到64位栈中。随后它会切换至64位栈，并将原生异常和上下文记录转换为相应的32位形式，将结果存储到32位栈中（取代64位异常帧）。至此，WoW64核心即可从32位版的KiUserExceptionDispatcher调度程序函数重启模拟，通过与原生32位Ntdll相同的方式调度异常。

32位用户模式APC交付也遵循了类似的实现方式。常规的用户模式APC可通过原生Ntdll的KiUserApcDispatcher进行交付。当64位内核即将向WoW64进程调度用户模式APC时，它会将32位APC地址映射至64位地址空间中更高的范围。随后64位Ntdll将调用WoW64核心库所导出的Wow64ApcRoutine例程，借此捕获用户模式的原生APC和上下文记录，并将其重新映射回32位栈。随后它会准备一个32位用户模式APC和上下文记录，并通过32位版的KiUserApcDispatcher函数重启CPU模拟，进而使用与原生32位Ntdll相同的方式调度APC。
